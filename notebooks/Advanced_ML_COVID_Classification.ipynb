{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0a79e1a",
   "metadata": {},
   "source": [
    "# \udd27 CONFIGURATION UNIVERSELLE - WSL & Colab\n",
    "\n",
    "**Configuration automatique intelligente qui s'adapte √† l'environnement :**\n",
    "- **üíª WSL/Linux** : Utilise le .env existant et le dataset local\n",
    "- **‚òÅÔ∏è Google Colab** : Clone le repo, cr√©e le .env, extrait le dataset depuis Drive\n",
    "\n",
    "**üöÄ Une seule cellule pour tout configurer !**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a2194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# üîß CONFIGURATION UNIVERSELLE WSL & COLAB\n",
    "# ===================================\n",
    "\"\"\"\n",
    "Configuration automatique intelligente qui d√©tecte l'environnement\n",
    "et configure tout ce qui est n√©cessaire pour WSL ou Colab.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üîß CONFIGURATION UNIVERSELLE - WSL & COLAB\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ===================================\n",
    "# 1. D√âTECTION ENVIRONNEMENT\n",
    "# ===================================\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"üìç Environnement d√©tect√©: ‚òÅÔ∏è Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"üìç Environnement d√©tect√©: üíª WSL/Linux Local\")\n",
    "\n",
    "# ===================================\n",
    "# 2. CONFIGURATION SELON L'ENVIRONNEMENT\n",
    "# ===================================\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"\\nüîÑ === CONFIGURATION COLAB ===\")\n",
    "    \n",
    "    # 2.1 Nettoyage et positionnement\n",
    "    try:\n",
    "        os.chdir('/content')\n",
    "        print(\"‚úÖ Positionnement dans /content\")\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è Probl√®me avec /content\")\n",
    "    \n",
    "    # 2.2 Clone du repository si n√©cessaire\n",
    "    if not os.path.exists('/content/DS_COVID'):\n",
    "        print(\"üì• Clonage du repository DS_COVID...\")\n",
    "        !git clone https://github.com/L-Poca/DS_COVID.git /content/DS_COVID\n",
    "        print(\"‚úÖ Repository clon√©\")\n",
    "    else:\n",
    "        print(\"‚úÖ Repository d√©j√† pr√©sent\")\n",
    "    \n",
    "    # 2.3 Positionnement dans le projet et branche ReVamp\n",
    "    %cd /content/DS_COVID\n",
    "    !git checkout ReVamp 2>/dev/null || echo \"‚ö†Ô∏è Branche ReVamp non trouv√©e\"\n",
    "    PROJECT_ROOT = Path('/content/DS_COVID')\n",
    "    \n",
    "    # 2.4 Installation des packages Python\n",
    "    print(\"üì¶ Installation des packages Python...\")\n",
    "    \n",
    "    # Installation depuis requirements.txt\n",
    "    if os.path.exists('./requirements.txt'):\n",
    "        print(\"üîÑ Installation requirements.txt...\")\n",
    "        !pip install -r requirements.txt\n",
    "        print(\"‚úÖ Requirements install√©s\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è requirements.txt non trouv√©\")\n",
    "    \n",
    "    # Installation du projet en mode d√©veloppement\n",
    "    if os.path.exists('./setup.py') or os.path.exists('./pyproject.toml'):\n",
    "        print(\"üîÑ Installation du projet (mode d√©veloppement)...\")\n",
    "        !pip install -e .\n",
    "        print(\"‚úÖ Projet install√© en mode d√©veloppement\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è setup.py ou pyproject.toml non trouv√©\")\n",
    "    \n",
    "    # V√©rification des packages critiques\n",
    "    print(\"üîç V√©rification packages ML/DL...\")\n",
    "    !pip list | grep -E \"(numpy|pandas|tensorflow|torch|scikit|xgboost|lightgbm|catboost|optuna)\" | head -10\n",
    "    \n",
    "    # 2.5 Cr√©ation du .env complet pour Colab\n",
    "    print(\"üìù Cr√©ation du fichier .env pour Colab...\")\n",
    "    \n",
    "    colab_env_content = f'''# CONFIGURATION COLAB AUTO-G√âN√âR√âE\n",
    "PROJECT_ROOT=.\n",
    "DATA_DIR=./data/raw/COVID-19_Radiography_Dataset/COVID-19_Radiography_Dataset\n",
    "MODELS_DIR=./models\n",
    "RESULTS_DIR=./results\n",
    "NOTEBOOKS_DIR=./notebooks\n",
    "\n",
    "# PARAM√àTRES D'IMAGES\n",
    "IMG_WIDTH=256\n",
    "IMG_HEIGHT=256\n",
    "IMG_CHANNELS=3\n",
    "\n",
    "# PARAM√àTRES D'ENTRA√éNEMENT (COLAB PRO)\n",
    "BATCH_SIZE=64\n",
    "EPOCHS=100\n",
    "LEARNING_RATE=0.001\n",
    "VALIDATION_SPLIT=0.2\n",
    "TEST_SPLIT=0.2\n",
    "\n",
    "# CLASSES DU DATASET\n",
    "CLASSES=COVID,Lung_Opacity,Normal,Viral Pneumonia\n",
    "CLASS_NAMES=COVID,Lung_Opacity,Normal,Viral Pneumonia\n",
    "NUM_CLASSES=4\n",
    "\n",
    "# RANDOM FOREST\n",
    "RF_N_ESTIMATORS=200\n",
    "RF_MAX_DEPTH=15\n",
    "RF_MIN_SAMPLES_SPLIT=5\n",
    "RF_MIN_SAMPLES_LEAF=2\n",
    "\n",
    "# XGBOOST\n",
    "XGB_N_ESTIMATORS=100\n",
    "XGB_LEARNING_RATE=0.1\n",
    "XGB_MAX_DEPTH=3\n",
    "XGB_MIN_CHILD_WEIGHT=1\n",
    "\n",
    "# VALIDATION CROIS√âE\n",
    "CV_FOLDS=3\n",
    "N_JOBS=-1\n",
    "\n",
    "# TRANSFER LEARNING\n",
    "PRETRAINED_WEIGHTS=imagenet\n",
    "FREEZE_BASE_LAYERS=True\n",
    "FINE_TUNE_LAYERS=10\n",
    "\n",
    "# CALLBACKS\n",
    "EARLY_STOPPING_PATIENCE=10\n",
    "REDUCE_LR_PATIENCE=5\n",
    "REDUCE_LR_FACTOR=0.5\n",
    "MIN_LR=1e-7\n",
    "\n",
    "# GPU/CPU\n",
    "TF_ENABLE_ONEDNN_OPTS=0\n",
    "CUDA_VISIBLE_DEVICES=0\n",
    "\n",
    "# GESTION M√âMOIRE\n",
    "MAX_IMAGES_PER_CLASS=1000\n",
    "SAMPLE_SIZE_ANALYSIS=200\n",
    "\n",
    "# DEBUG\n",
    "LOG_LEVEL=INFO\n",
    "VERBOSE=1\n",
    "RANDOM_SEED=42\n",
    "\n",
    "# VISUALISATION\n",
    "PLOT_STYLE=seaborn-v0_8\n",
    "COLOR_PALETTE=husl\n",
    "FIGURE_SIZE_WIDTH=12\n",
    "FIGURE_SIZE_HEIGHT=8\n",
    "DPI=100\n",
    "\n",
    "# EXPORT\n",
    "MODEL_SAVE_FORMAT=h5\n",
    "RESULTS_FORMAT=csv\n",
    "EXPORT_PREDICTIONS=True\n",
    "SAVE_PLOTS=True\n",
    "\n",
    "# COLAB SP√âCIFIQUE\n",
    "ARCHIVE_PATH=/content/drive/MyDrive/archive_covid.zip\n",
    "USE_GPU=true'''\n",
    "    \n",
    "    with open('.env', 'w') as f:\n",
    "        f.write(colab_env_content)\n",
    "    print(\"‚úÖ Fichier .env cr√©√©\")\n",
    "    \n",
    "    # 2.6 Montage Google Drive\n",
    "    if not os.path.exists('/content/drive'):\n",
    "        print(\"üíæ Montage Google Drive...\")\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "    else:\n",
    "        print(\"‚úÖ Google Drive d√©j√† mont√©\")\n",
    "    \n",
    "    # 2.7 Extraction dataset si n√©cessaire\n",
    "    dataset_paths = [\n",
    "        './data/raw/COVID-19_Radiography_Dataset/COVID-19_Radiography_Dataset/',\n",
    "        './data/raw/COVID-19_Radiography_Dataset/',\n",
    "    ]\n",
    "    \n",
    "    dataset_found = None\n",
    "    for path in dataset_paths:\n",
    "        if os.path.exists(path) and os.path.exists(f\"{path}/COVID\"):\n",
    "            dataset_found = path\n",
    "            break\n",
    "    \n",
    "    if not dataset_found:\n",
    "        print(\"üì¶ Extraction dataset depuis Drive...\")\n",
    "        !mkdir -p ./data/raw/\n",
    "        if os.path.exists('/content/drive/MyDrive/archive_covid.zip'):\n",
    "            !cd ./data/raw/ && unzip -o -q /content/drive/MyDrive/archive_covid.zip\n",
    "            print(\"‚úÖ Dataset extrait\")\n",
    "            # Re-v√©rifier\n",
    "            for path in dataset_paths:\n",
    "                if os.path.exists(path) and os.path.exists(f\"{path}/COVID\"):\n",
    "                    dataset_found = path\n",
    "                    break\n",
    "        else:\n",
    "            print(\"‚ùå Archive non trouv√©e dans Drive\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Dataset d√©j√† disponible\")\n",
    "    \n",
    "    DATA_DIR = Path(dataset_found) if dataset_found else None\n",
    "\n",
    "else:\n",
    "    print(\"\\nüíª === CONFIGURATION WSL/LINUX ===\")\n",
    "    \n",
    "    # 2.1 D√©tection du projet\n",
    "    current = Path.cwd()\n",
    "    PROJECT_ROOT = None\n",
    "    \n",
    "    # Chercher la racine du projet (avec .env ou pyproject.toml)\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / '.env').exists() or (parent / 'pyproject.toml').exists():\n",
    "            PROJECT_ROOT = parent\n",
    "            break\n",
    "    \n",
    "    if not PROJECT_ROOT:\n",
    "        PROJECT_ROOT = current\n",
    "        print(\"‚ö†Ô∏è Racine projet non trouv√©e, utilisation du r√©pertoire courant\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Racine projet: {PROJECT_ROOT}\")\n",
    "    \n",
    "    # 2.2 Changement vers le projet si n√©cessaire\n",
    "    if Path.cwd() != PROJECT_ROOT:\n",
    "        os.chdir(PROJECT_ROOT)\n",
    "        print(f\"üìÇ Changement vers: {PROJECT_ROOT}\")\n",
    "    \n",
    "    # 2.3 V√©rification .env\n",
    "    env_file = PROJECT_ROOT / '.env'\n",
    "    if env_file.exists():\n",
    "        print(f\"‚úÖ Fichier .env trouv√© ({env_file.stat().st_size} bytes)\")\n",
    "    else:\n",
    "        print(\"‚ùå Fichier .env non trouv√©\")\n",
    "        # Cr√©er depuis .env.example si disponible\n",
    "        env_example = PROJECT_ROOT / '.env.example'\n",
    "        if env_example.exists():\n",
    "            print(\"üìù Cr√©ation .env depuis .env.example...\")\n",
    "            import shutil\n",
    "            shutil.copy(env_example, env_file)\n",
    "            print(\"‚úÖ .env cr√©√© depuis .env.example\")\n",
    "    \n",
    "    # 2.4 V√©rification environnement virtuel (WSL)\n",
    "    venv_path = PROJECT_ROOT / '.venv'\n",
    "    if venv_path.exists():\n",
    "        print(\"‚úÖ Environnement virtuel .venv d√©tect√©\")\n",
    "        print(\"üí° N'oubliez pas d'activer: source .venv/bin/activate\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Environnement virtuel .venv non trouv√©\")\n",
    "    \n",
    "    # 2.5 Recherche dataset\n",
    "    dataset_candidates = [\n",
    "        PROJECT_ROOT / 'data' / 'raw' / 'COVID-19_Radiography_Dataset' / 'COVID-19_Radiography_Dataset',\n",
    "        PROJECT_ROOT / 'data' / 'raw' / 'COVID-19_Radiography_Dataset',\n",
    "    ]\n",
    "    \n",
    "    DATA_DIR = None\n",
    "    for candidate in dataset_candidates:\n",
    "        if candidate.exists() and (candidate / 'COVID').exists():\n",
    "            DATA_DIR = candidate\n",
    "            print(f\"‚úÖ Dataset trouv√©: {DATA_DIR}\")\n",
    "            break\n",
    "    \n",
    "    if not DATA_DIR:\n",
    "        print(\"‚ùå Dataset non trouv√©\")\n",
    "\n",
    "# ===================================\n",
    "# 3. CHARGEMENT CONFIGURATION\n",
    "# ===================================\n",
    "print(f\"\\nüîß === CHARGEMENT CONFIGURATION ===\")\n",
    "\n",
    "# Installation/import de python-dotenv si n√©cessaire\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "except ImportError:\n",
    "    print(\"üì¶ Installation python-dotenv...\")\n",
    "    !pip install python-dotenv\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "# Chargement du .env\n",
    "if (PROJECT_ROOT / '.env').exists():\n",
    "    load_dotenv(PROJECT_ROOT / '.env')\n",
    "    print(\"‚úÖ Variables .env charg√©es\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è .env non trouv√©, utilisation des valeurs par d√©faut\")\n",
    "\n",
    "# Variables de configuration\n",
    "BATCH_SIZE = int(os.getenv('BATCH_SIZE', '32'))\n",
    "EPOCHS = int(os.getenv('EPOCHS', '50'))\n",
    "IMG_WIDTH = int(os.getenv('IMG_WIDTH', '256'))\n",
    "IMG_HEIGHT = int(os.getenv('IMG_HEIGHT', '256'))\n",
    "IMG_SIZE = (IMG_WIDTH, IMG_HEIGHT)\n",
    "IMG_CHANNELS = int(os.getenv('IMG_CHANNELS', '3'))\n",
    "\n",
    "CLASSES_STR = os.getenv('CLASSES', 'COVID,Lung_Opacity,Normal,Viral Pneumonia')\n",
    "CLASSES = [cls.strip() for cls in CLASSES_STR.split(',')]\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "CLASS_MAPPING = {cls: i for i, cls in enumerate(CLASSES)}\n",
    "\n",
    "LEARNING_RATE = float(os.getenv('LEARNING_RATE', '0.001'))\n",
    "TEST_SPLIT = float(os.getenv('TEST_SPLIT', '0.2'))\n",
    "VALIDATION_SPLIT = float(os.getenv('VALIDATION_SPLIT', '0.2'))\n",
    "RANDOM_SEED = int(os.getenv('RANDOM_SEED', '42'))\n",
    "MAX_IMAGES_PER_CLASS = int(os.getenv('MAX_IMAGES_PER_CLASS', '1000'))\n",
    "\n",
    "# R√©pertoires de travail\n",
    "MODELS_DIR = PROJECT_ROOT / 'models'\n",
    "RESULTS_DIR = PROJECT_ROOT / 'results'\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ===================================\n",
    "# 4. V√âRIFICATION FINALE ET COMPTAGE\n",
    "# ===================================\n",
    "print(f\"\\nüìä === V√âRIFICATION FINALE ===\")\n",
    "\n",
    "print(f\"üåç Environnement: {'Colab' if IN_COLAB else 'WSL/Linux'}\")\n",
    "print(f\"üìÇ Projet: {PROJECT_ROOT}\")\n",
    "print(f\"üìä Dataset: {DATA_DIR if DATA_DIR else '‚ùå Non trouv√©'}\")\n",
    "print(f\"üéõÔ∏è Param√®tres: batch_size={BATCH_SIZE}, epochs={EPOCHS}, img_size={IMG_SIZE}\")\n",
    "print(f\"üè∑Ô∏è Classes: {CLASSES} ({NUM_CLASSES} classes)\")\n",
    "\n",
    "# Comptage images si dataset disponible\n",
    "if DATA_DIR and DATA_DIR.exists():\n",
    "    try:\n",
    "        total_images = 0\n",
    "        print(f\"\\nüñºÔ∏è Images par classe:\")\n",
    "        for cls in CLASSES:\n",
    "            images_path = DATA_DIR / cls / 'images'\n",
    "            if images_path.exists():\n",
    "                count = len([f for f in images_path.iterdir() if f.suffix.lower() in ['.png', '.jpg', '.jpeg']])\n",
    "                print(f\"  {cls}: {count:,} images\")\n",
    "                total_images += count\n",
    "            else:\n",
    "                # Fallback: images directement dans le dossier classe\n",
    "                images_direct = list((DATA_DIR / cls).glob('*.png')) + list((DATA_DIR / cls).glob('*.jpg'))\n",
    "                if images_direct:\n",
    "                    print(f\"  {cls}: {len(images_direct):,} images (direct)\")\n",
    "                    total_images += len(images_direct)\n",
    "                else:\n",
    "                    print(f\"  {cls}: ‚ùå Non trouv√©\")\n",
    "        \n",
    "        print(f\"  üéØ TOTAL: {total_images:,} images\")\n",
    "        \n",
    "        if total_images == 0:\n",
    "            print(\"‚ö†Ô∏è Aucune image trouv√©e - v√©rifiez la structure du dataset\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur comptage images: {e}\")\n",
    "\n",
    "print(f\"\\nüéâ CONFIGURATION UNIVERSELLE TERMIN√âE !\")\n",
    "print(f\"üí° Environnement {'Colab' if IN_COLAB else 'WSL'} pr√™t pour l'entra√Ænement ML\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"üöÄ Packages install√©s, .env cr√©√©, dataset extrait - Pr√™t √† utiliser !\")\n",
    "else:\n",
    "    print(\"üîß Configuration WSL charg√©e - V√©rifiez que .venv est activ√©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0471dcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# üîç DIAGNOSTIC GOOGLE COLAB\n",
    "# ===================================\n",
    "\"\"\"\n",
    "Diagnostic sp√©cialis√© pour environnement Google Colab\n",
    "\"\"\"\n",
    "\n",
    "# V√©rification qu'on est bien dans Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    print(\"=\" * 50)\n",
    "    print(\"‚òÅÔ∏è DIAGNOSTIC GOOGLE COLAB\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    import os\n",
    "    import subprocess\n",
    "    from pathlib import Path\n",
    "    \n",
    "    print(f\"üìÇ R√©pertoire courant: {os.getcwd()}\")\n",
    "    \n",
    "    # ===================================\n",
    "    # 1. CLONAGE/V√âRIFICATION REPOSITORY\n",
    "    # ===================================\n",
    "    print(\"\\nüîç V√âRIFICATION REPOSITORY:\")\n",
    "    \n",
    "    repo_path = None\n",
    "    repo_candidates = [\n",
    "        Path('/content/DS_COVID'),\n",
    "        Path('/content/drive/MyDrive/DS_COVID')\n",
    "    ]\n",
    "    \n",
    "    # Chercher repo existant\n",
    "    for candidate in repo_candidates:\n",
    "        if candidate.exists() and (candidate / '.env').exists():\n",
    "            repo_path = candidate\n",
    "            print(f\"‚úÖ Repository trouv√©: {repo_path}\")\n",
    "            break\n",
    "    \n",
    "    # Cloner si n√©cessaire\n",
    "    if not repo_path:\n",
    "        print(\"üîÑ Clonage du repository...\")\n",
    "        try:\n",
    "            result = subprocess.run([\n",
    "                'git', 'clone', \n",
    "                'https://github.com/L-Poca/DS_COVID.git',\n",
    "                '/content/DS_COVID'\n",
    "            ], capture_output=True, text=True, timeout=60)\n",
    "            \n",
    "            if result.returncode == 0:\n",
    "                repo_path = Path('/content/DS_COVID')\n",
    "                print(f\"‚úÖ Repository clon√©: {repo_path}\")\n",
    "            else:\n",
    "                print(f\"‚ùå Erreur clonage: {result.stderr}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur clonage: {e}\")\n",
    "    \n",
    "    # ===================================\n",
    "    # 2. RECHERCHE .ENV (COLAB)\n",
    "    # ===================================\n",
    "    print(f\"\\nüîç RECHERCHE .ENV:\")\n",
    "    \n",
    "    env_file = None\n",
    "    env_candidates = [\n",
    "        Path('/content/DS_COVID/.env'),\n",
    "        Path('/content/.env'),\n",
    "        Path('/content/drive/MyDrive/DS_COVID/.env'),\n",
    "        Path('/content/drive/MyDrive/.env')\n",
    "    ]\n",
    "    \n",
    "    for candidate in env_candidates:\n",
    "        if candidate.exists():\n",
    "            env_file = candidate\n",
    "            size = candidate.stat().st_size\n",
    "            print(f\"‚úÖ .env trouv√©: {candidate} ({size} bytes)\")\n",
    "            break\n",
    "    \n",
    "    if not env_file:\n",
    "        print(\"‚ùå Aucun .env trouv√©\")\n",
    "    \n",
    "    # ===================================\n",
    "    # 3. MONTAGE GOOGLE DRIVE\n",
    "    # ===================================\n",
    "    print(f\"\\nüîç V√âRIFICATION GOOGLE DRIVE:\")\n",
    "    \n",
    "    drive_mounted = Path('/content/drive').exists()\n",
    "    if drive_mounted:\n",
    "        print(\"‚úÖ Google Drive d√©j√† mont√©\")\n",
    "    else:\n",
    "        print(\"üîÑ Montage Google Drive...\")\n",
    "        try:\n",
    "            from google.colab import drive\n",
    "            drive.mount('/content/drive')\n",
    "            print(\"‚úÖ Google Drive mont√©\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur montage Drive: {e}\")\n",
    "    \n",
    "    # ===================================\n",
    "    # 4. RECHERCHE DATASET COVID (COLAB)\n",
    "    # ===================================\n",
    "    print(f\"\\nüîç RECHERCHE DATASET COVID:\")\n",
    "    \n",
    "    covid_dataset = None\n",
    "    search_zones = [\n",
    "        Path('/content'),\n",
    "        Path('/content/DS_COVID/data/raw') if repo_path else None,\n",
    "        Path('/content/drive/MyDrive'),\n",
    "        Path('/content/drive/MyDrive/DS_COVID/data/raw') if drive_mounted else None\n",
    "    ]\n",
    "    \n",
    "    search_zones = [z for z in search_zones if z]  # Enlever None\n",
    "    \n",
    "    for zone in search_zones:\n",
    "        if not zone.exists():\n",
    "            continue\n",
    "        \n",
    "        print(f\"üîç Zone: {zone}\")\n",
    "        \n",
    "        # Chercher patterns COVID\n",
    "        covid_patterns = [\n",
    "            \"COVID-19_Radiography_Dataset\",\n",
    "            \"*COVID*\",\n",
    "            \"*Radiography*\"\n",
    "        ]\n",
    "        \n",
    "        for pattern in covid_patterns:\n",
    "            try:\n",
    "                matches = list(zone.glob(pattern))\n",
    "                for match in matches[:2]:\n",
    "                    if match.is_dir():\n",
    "                        print(f\"  üìÅ Candidat: {match}\")\n",
    "                        try:\n",
    "                            subdirs = [d.name for d in match.iterdir() if d.is_dir()]\n",
    "                            covid_classes = [\"COVID\", \"Normal\", \"Lung_Opacity\", \"Viral Pneumonia\"]\n",
    "                            found_classes = [cls for cls in covid_classes if cls in subdirs]\n",
    "                            \n",
    "                            if found_classes:\n",
    "                                covid_dataset = match\n",
    "                                print(f\"    ‚úÖ Classes: {found_classes}\")\n",
    "                                break\n",
    "                        except Exception as e:\n",
    "                            print(f\"    ‚ùå Erreur: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ùå Erreur pattern: {e}\")\n",
    "        \n",
    "        if covid_dataset:\n",
    "            break\n",
    "    \n",
    "    if not covid_dataset:\n",
    "        print(\"‚ùå Aucun dataset COVID trouv√©\")\n",
    "    \n",
    "    # ===================================\n",
    "    # 5. RECHERCHE ARCHIVES (COLAB)\n",
    "    # ===================================\n",
    "    print(f\"\\nüîç RECHERCHE ARCHIVES:\")\n",
    "    \n",
    "    archive_file = None\n",
    "    for zone in search_zones:\n",
    "        if not zone.exists():\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            zip_files = list(zone.glob(\"*.zip\"))\n",
    "            for zip_file in zip_files:\n",
    "                size = zip_file.stat().st_size\n",
    "                size_mb = size / (1024 * 1024)\n",
    "                if size_mb > 1:  # Plus de 1MB\n",
    "                    print(f\"  üì¶ {zip_file.name}: {size_mb:.1f} MB\")\n",
    "                    if not archive_file or size > archive_file.stat().st_size:\n",
    "                        archive_file = zip_file\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur zone {zone}: {e}\")\n",
    "    \n",
    "    if not archive_file:\n",
    "        print(\"‚ùå Aucune archive trouv√©e\")\n",
    "    \n",
    "    # ===================================\n",
    "    # 6. R√âSUM√â COLAB\n",
    "    # ===================================\n",
    "    print(f\"\\n\" + \"=\" * 50)\n",
    "    print(\"üìã R√âSUM√â COLAB\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(f\"üìÇ R√©pertoire: {os.getcwd()}\")\n",
    "    print(f\"üîß .env: {'‚úÖ ' + str(env_file) if env_file else '‚ùå Non trouv√©'}\")\n",
    "    print(f\"üìä Dataset: {'‚úÖ ' + str(covid_dataset) if covid_dataset else '‚ùå Non trouv√©'}\")\n",
    "    print(f\"üì¶ Archive: {'‚úÖ ' + str(archive_file) if archive_file else '‚ùå Non trouv√©e'}\")\n",
    "    print(f\"üíæ Drive: {'‚úÖ Mont√©' if drive_mounted else '‚ùå Non mont√©'}\")\n",
    "    \n",
    "    # Variables pour la config principale\n",
    "    COLAB_ENV = env_file\n",
    "    COLAB_DATASET = covid_dataset\n",
    "    COLAB_ARCHIVE = archive_file\n",
    "    COLAB_REPO = repo_path\n",
    "    \n",
    "    print(f\"\\nüíæ Variables COLAB sauvegard√©es:\")\n",
    "    print(f\"   COLAB_ENV = {COLAB_ENV}\")\n",
    "    print(f\"   COLAB_DATASET = {COLAB_DATASET}\")\n",
    "    print(f\"   COLAB_ARCHIVE = {COLAB_ARCHIVE}\")\n",
    "    print(f\"   COLAB_REPO = {COLAB_REPO}\")\n",
    "    \n",
    "    print(f\"\\nüéØ DIAGNOSTIC COLAB TERMIN√â!\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ùå Cette cellule est con√ßue pour Google Colab uniquement\")\n",
    "    print(\"üíª Utilisez la cellule pr√©c√©dente pour WSL/Linux\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9504d98d",
   "metadata": {},
   "source": [
    "# üî¨ Advanced Machine Learning for COVID-19 Chest X-Ray Classification\n",
    "\n",
    "## üìã Comprehensive Guide: Ensemble Methods & Deep Learning Fine-Tuning\n",
    "\n",
    "**Auteurs**: √âquipe DS_COVID  \n",
    "**Date**: 15 octobre 2025  \n",
    "**Branche**: ReVamp\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ **Objectifs du notebook**\n",
    "\n",
    "Ce notebook pr√©sente une approche compl√®te du machine learning pour la classification de radiographies pulmonaires COVID-19, couvrant:\n",
    "\n",
    "1. **üå≥ M√©thodes d'Ensemble** : Bagging, Boosting, Stacking\n",
    "2. **üß† Deep Learning** : Transfer Learning & Fine-Tuning  \n",
    "3. **‚öôÔ∏è Optimisation** : Hyperparam√®tres, Cross-validation\n",
    "4. **üìä √âvaluation** : M√©triques avanc√©es, Visualisations\n",
    "\n",
    "### üìÅ **Dataset**\n",
    "- **Source**: COVID-19_Radiography_Dataset\n",
    "- **Classes**: COVID, Normal, Lung_Opacity, Viral Pneumonia\n",
    "- **Type**: Images radiographiques pulmonaires\n",
    "\n",
    "---\n",
    "\n",
    "### üóÇÔ∏è **Structure du notebook**\n",
    "\n",
    "| Section | Technique | Description |\n",
    "|---------|-----------|-------------|\n",
    "| 1-3 | **Setup & EDA** | Chargement, exploration, preprocessing |\n",
    "| 4-6 | **Ensemble Methods** | Bagging, Boosting, comparaisons |\n",
    "| 7-10 | **Deep Learning** | CNN, Transfer Learning, Fine-Tuning |\n",
    "| 11-13 | **Optimization & Evaluation** | Tuning, m√©triques, visualisations |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e79303f",
   "metadata": {},
   "source": [
    "## üì¶ Section 1: Setup and Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28eb542c",
   "metadata": {},
   "source": [
    "## üöÄ Version Google Colab\n",
    "\n",
    "**Pour utiliser ce notebook sur Google Colab, ex√©cutez cette cellule au lieu de la cellule de configuration normale :**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418efab9",
   "metadata": {},
   "source": [
    "### üìã Instructions pour Google Colab\n",
    "\n",
    "#### ‚úÖ Ce qui change sur Colab vs Local :\n",
    "\n",
    "1. **üìÇ Gestion des donn√©es** :\n",
    "   - Local : Dataset dans `data/raw/COVID-19_Radiography_Dataset/`\n",
    "   - Colab : **Extraction automatique** d'`archive_covid.zip` depuis Drive\n",
    "\n",
    "2. **üì¶ Installation** :\n",
    "   - Local : `pip install -e .` fait une seule fois\n",
    "   - Colab : Installation automatique dans la cellule\n",
    "\n",
    "3. **üîß Configuration** :\n",
    "   - Local : Chargement automatique `.env`\n",
    "   - Colab : Package ds-covid + extraction ZIP\n",
    "\n",
    "#### üöÄ Ce que fait automatiquement la cellule Colab :\n",
    "\n",
    "1. **üì• Clone le repo** : `git clone https://github.com/L-Poca/DS_COVID.git`\n",
    "2. **üì¶ Extrait archive_covid.zip** : Depuis `MyDrive/archive_covid.zip`\n",
    "3. **üîç Recherche intelligente** : Trouve automatiquement le dossier COVID\n",
    "4. ** Installe les d√©pendances** : `pip install -r requirements.txt`\n",
    "5. **üîß Installe le package** : `pip install -e .` \n",
    "6. **‚öôÔ∏è Configure les chemins** : `DATA_DIR` pointe vers les donn√©es extraites\n",
    "7. **‚úÖ V√©rifie les donn√©es** : Compte les images par classe\n",
    "\n",
    "#### üíæ Votre fichier sur Drive :\n",
    "\n",
    "```\n",
    "Drive/\n",
    "‚îú‚îÄ‚îÄ MyDrive/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ archive_covid.zip  ‚Üê Votre fichier ZIP\n",
    "```\n",
    "\n",
    "**Le script fait automatiquement** :\n",
    "1. **Extrait** `archive_covid.zip` vers `/content/temp_covid_extract/`\n",
    "2. **Recherche** le dossier COVID (plusieurs patterns support√©s)\n",
    "3. **D√©place** vers `/content/DS_COVID/data/raw/COVID-19_Radiography_Dataset/`\n",
    "4. **Nettoie** le dossier temporaire\n",
    "\n",
    "#### üîç Recherche intelligente :\n",
    "\n",
    "Le script cherche automatiquement :\n",
    "- `COVID-19_Radiography_Dataset/`\n",
    "- Dossiers contenant `*COVID*`\n",
    "- Dossiers contenant `*radiography*`\n",
    "- Dossiers contenant `*chest*`\n",
    "\n",
    "#### üí° Avantages :\n",
    "\n",
    "- ‚úÖ **Simple** : Juste d√©poser archive_covid.zip dans Drive\n",
    "- ‚úÖ **Automatique** : Extraction et organisation automatiques\n",
    "- ‚úÖ **Robuste** : Recherche intelligente de la structure\n",
    "- ‚úÖ **Rapide** : Pas besoin de cr√©er des dossiers manuellement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377bf6bb",
   "metadata": {},
   "source": [
    "### üöÄ Optimisations Colab Pro\n",
    "\n",
    "#### ‚ö° Param√®tres ambitieux pour GPU puissant :\n",
    "\n",
    "**üñºÔ∏è Images :**\n",
    "- `IMG_SIZE`: `(256, 256)` ‚Üê Puissance de 2 (optimal GPU)\n",
    "- `BATCH_SIZE`: `64` ‚Üê Plus gros batch (GPU T4/V100)\n",
    "\n",
    "**üéØ Training :**\n",
    "- `EPOCHS`: `100` ‚Üê Training long (temps illimit√© Colab Pro)\n",
    "- `MAX_IMAGES`: `5000` ‚Üê Dataset complet par classe\n",
    "\n",
    "**ü§ñ ML traditionnel :**\n",
    "- Random Forest : `500 estimateurs` (vs 200)\n",
    "- XGBoost : `300 estimateurs` (vs 100)  \n",
    "- Gradient Boosting : `300 estimateurs`\n",
    "- CV : `5 folds` (vs 3)\n",
    "\n",
    "**üß† Deep Learning :**\n",
    "- Architectures : `EfficientNetB3`, `ResNet152V2`, `VGG19`, `DenseNet201`\n",
    "- Mixed precision : `float16` (acc√©l√©ration GPU)\n",
    "- Fine-tuning : `20 couches` (vs 10)\n",
    "- Data augmentation : Rotation, zoom, flip, brightness\n",
    "\n",
    "#### üí° Pourquoi ces optimisations ?\n",
    "\n",
    "1. **GPU T4/V100** : Plus de m√©moire et compute ‚Üí batch size plus gros\n",
    "2. **Temps illimit√©** : Colab Pro permet training long ‚Üí plus d'epochs\n",
    "3. **Puissance de 2** : `256x256` optimise les op√©rations GPU vs `224x224`\n",
    "4. **Mixed precision** : Acc√©l√©ration x1.5-2x sur GPU r√©cents\n",
    "5. **Architectures avanc√©es** : Plus performantes qu'EfficientNetB0/ResNet50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50e20da",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è Configuration Universelle\n",
    "\n",
    "**Cellule unique qui fonctionne sur Google Colab ET votre machine locale !**\n",
    "\n",
    "## üéØ **Ce que fait cette cellule :**\n",
    "- **D√©tecte automatiquement** l'environnement (Colab ou local)\n",
    "- **Charge les param√®tres** depuis `.env` avec `load_dotenv()`\n",
    "- **Monte Google Drive** (Colab) ou cherche l'archive locale\n",
    "- **Extrait les donn√©es** COVID-19 automatiquement\n",
    "- **Configure tous les param√®tres** ML/DL identiques partout\n",
    "\n",
    "## üì¶ **Pr√©requis :**\n",
    "- **Sur Colab** : `archive_covid.zip` dans `/content/drive/MyDrive/`\n",
    "- **En local** : `archive_covid.zip` t√©l√©charg√© dans `~/Downloads/`\n",
    "\n",
    "## ‚úÖ **Param√®tres depuis `.env` :**\n",
    "- `BATCH_SIZE=64`, `EPOCHS=100`, `IMG_SIZE=256x256`\n",
    "- `CLASSES=COVID,Lung_Opacity,Normal,Viral Pneumonia`\n",
    "- Fallback automatique si `.env` manque\n",
    "\n",
    "**‚Üí Ex√©cutez cette cellule en premier !**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ea8f80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Installation et imports unifi√©s...\n",
      "üíª WSL Local - V√©rification des packages...\n",
      "‚ÑπÔ∏è En WSL, requirements.txt suppos√© d√©j√† install√©\n",
      "üí° Si packages manquants ‚Üí pip install -r requirements.txt\n",
      "üìö Chargement des biblioth√®ques...\n",
      "‚úÖ XGBoost disponible\n",
      "‚ùå LightGBM non disponible ‚Üí pip install lightgbm\n",
      "‚ùå CatBoost non disponible ‚Üí pip install catboost\n",
      "‚úÖ XGBoost disponible\n",
      "‚ùå LightGBM non disponible ‚Üí pip install lightgbm\n",
      "‚ùå CatBoost non disponible ‚Üí pip install catboost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 22:09:38.414521: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-10-15 22:09:38.559441: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-10-15 22:09:38.677060: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1760558978.863577     938 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1760558978.928302     938 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1760558979.414745     938 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1760558979.414795     938 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1760558979.414797     938 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1760558979.414798     938 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-10-15 22:09:39.446609: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ TensorFlow 2.19.0 - Strat√©gie: <tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x7f21309a7130>\n",
      "‚ùå Optuna non disponible ‚Üí pip install optuna\n",
      "‚úÖ tqdm disponible\n",
      "‚úÖ python-dotenv disponible\n",
      "‚úÖ Mixed precision float16 activ√©e\n",
      "üé≤ Seed configur√©: 42\n",
      "\n",
      "üéâ Setup termin√©!\n",
      "üìä R√©sum√© des packages ML:\n",
      "   - Scikit-learn: ‚úÖ\n",
      "   - TensorFlow: ‚úÖ\n",
      "   - XGBoost: ‚úÖ\n",
      "   - LightGBM: ‚ùå\n",
      "   - CatBoost: ‚ùå\n",
      "   - Optuna: ‚ùå\n",
      "   - python-dotenv: ‚úÖ\n",
      "üíª INFORMATIONS SYST√àME:\n",
      "   - Python: 3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]\n",
      "   - TensorFlow: 2.19.0\n",
      "   - GPU disponible: []\n",
      "   - R√©pertoire de travail: /home/cepa/DST/projet_DS/DS_COVID\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 22:09:43.172569: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "# ===================================\n",
    "# üì¶ INSTALLATION UNIFI√âE VIA REQUIREMENTS.TXT\n",
    "# ===================================\n",
    "\n",
    "print(\"üöÄ Installation et imports unifi√©s...\")\n",
    "\n",
    "# =====================================\n",
    "# INSTALLATION CONDITIONNELLE\n",
    "# =====================================\n",
    "if IN_COLAB:\n",
    "    print(\"üì¶ Google Colab - Installation requirements.txt...\")\n",
    "    !pip install -q -r requirements.txt\n",
    "    print(\"‚úÖ Requirements.txt install√© sur Colab!\")\n",
    "else:\n",
    "    print(\"üíª WSL Local - V√©rification des packages...\")\n",
    "    # Sur WSL, on assume que requirements.txt est d√©j√† install√©\n",
    "\n",
    "    # Si besoin d'installation: pip install -r requirements.txt\n",
    "    print(\"‚ÑπÔ∏è En WSL, requirements.txt suppos√© d√©j√† install√©\")\n",
    "    print(\"üí° Si packages manquants ‚Üí pip install -r requirements.txt\")\n",
    "\n",
    "# =====================================\n",
    "# IMPORTS COMPLETS\n",
    "# =====================================\n",
    "print(\"üìö Chargement des biblioth√®ques...\")\n",
    "\n",
    "# Python standard library\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "# Data manipulation & analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# Image processing\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# Machine Learning - Scikit-learn\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                           accuracy_score, precision_score, recall_score, f1_score,\n",
    "                           roc_auc_score, roc_curve, auc)\n",
    "from sklearn.ensemble import (RandomForestClassifier, VotingClassifier, \n",
    "                             BaggingClassifier, AdaBoostClassifier, \n",
    "                             GradientBoostingClassifier, ExtraTreesClassifier)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Boosting libraries (should all be available via requirements.txt)\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB_AVAILABLE = True\n",
    "    print(\"‚úÖ XGBoost disponible\")\n",
    "except ImportError:\n",
    "    XGB_AVAILABLE = False\n",
    "    print(\"‚ùå XGBoost non disponible ‚Üí pip install xgboost\")\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LGB_AVAILABLE = True\n",
    "    print(\"‚úÖ LightGBM disponible\")\n",
    "except ImportError:\n",
    "    LGB_AVAILABLE = False\n",
    "    print(\"‚ùå LightGBM non disponible ‚Üí pip install lightgbm\")\n",
    "\n",
    "try:\n",
    "    import catboost as cb\n",
    "    CAT_AVAILABLE = True\n",
    "    print(\"‚úÖ CatBoost disponible\")\n",
    "except ImportError:\n",
    "    CAT_AVAILABLE = False\n",
    "    print(\"‚ùå CatBoost non disponible ‚Üí pip install catboost\")\n",
    "\n",
    "# Deep Learning - TensorFlow/Keras\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "    from tensorflow.keras.applications import (EfficientNetB3, ResNet152V2, \n",
    "                                             VGG19, DenseNet201, InceptionV3)\n",
    "    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "    from tensorflow.keras.utils import to_categorical\n",
    "    \n",
    "    # Configuration GPU/TPU\n",
    "    if IN_COLAB:\n",
    "        try:\n",
    "            tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "            tf.config.experimental_connect_to_cluster(tpu)\n",
    "            tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "            strategy = tf.distribute.TPUStrategy(tpu)\n",
    "            print(\"üöÄ TPU d√©tect√© et configur√©!\")\n",
    "        except:\n",
    "            strategy = tf.distribute.get_strategy()\n",
    "            print(\"üñ•Ô∏è GPU/CPU strategy\")\n",
    "    else:\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "    \n",
    "    print(f\"‚úÖ TensorFlow {tf.__version__} - Strat√©gie: {strategy}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ùå TensorFlow non disponible ‚Üí pip install tensorflow\")\n",
    "    tf = None\n",
    "\n",
    "# Hyperparameter optimization\n",
    "try:\n",
    "    import optuna\n",
    "    OPTUNA_AVAILABLE = True\n",
    "    print(\"‚úÖ Optuna disponible\")\n",
    "except ImportError:\n",
    "    OPTUNA_AVAILABLE = False\n",
    "    print(\"‚ùå Optuna non disponible ‚Üí pip install optuna\")\n",
    "\n",
    "# Progress bars\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "    tqdm.pandas()\n",
    "    print(\"‚úÖ tqdm disponible\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è tqdm non disponible ‚Üí pip install tqdm\")\n",
    "\n",
    "# Environment variables\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    DOTENV_AVAILABLE = True\n",
    "    print(\"‚úÖ python-dotenv disponible\")\n",
    "except ImportError:\n",
    "    DOTENV_AVAILABLE = False\n",
    "    print(\"‚ùå python-dotenv non disponible ‚Üí pip install python-dotenv\")\n",
    "\n",
    "# =====================================\n",
    "# CONFIGURATION GLOBALE\n",
    "# =====================================\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration matplotlib/seaborn\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.grid'] = True\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configuration pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Configuration TensorFlow (si disponible)\n",
    "if tf is not None:\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "    # Mixed precision pour performance\n",
    "    try:\n",
    "        policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "        tf.keras.mixed_precision.set_global_policy(policy)\n",
    "        print(\"‚úÖ Mixed precision float16 activ√©e\")\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è Mixed precision non support√©e\")\n",
    "\n",
    "# Configuration seeds pour reproductibilit√©\n",
    "np.random.seed(RANDOM_SEED)\n",
    "print(f\"üé≤ Seed configur√©: {RANDOM_SEED}\")\n",
    "\n",
    "print(\"\\nüéâ Setup termin√©!\")\n",
    "print(f\"üìä R√©sum√© des packages ML:\")\n",
    "print(f\"   - Scikit-learn: ‚úÖ\")\n",
    "print(f\"   - TensorFlow: {'‚úÖ' if tf is not None else '‚ùå'}\")\n",
    "print(f\"   - XGBoost: {'‚úÖ' if XGB_AVAILABLE else '‚ùå'}\")  \n",
    "print(f\"   - LightGBM: {'‚úÖ' if LGB_AVAILABLE else '‚ùå'}\")\n",
    "print(f\"   - CatBoost: {'‚úÖ' if CAT_AVAILABLE else '‚ùå'}\")\n",
    "print(f\"   - Optuna: {'‚úÖ' if OPTUNA_AVAILABLE else '‚ùå'}\")\n",
    "print(f\"   - python-dotenv: {'‚úÖ' if DOTENV_AVAILABLE else '‚ùå'}\")\n",
    "\n",
    "# =====================================\n",
    "# FONCTIONS UTILITAIRES\n",
    "# =====================================\n",
    "def load_image_paths_and_labels(data_dir, classes):\n",
    "    \"\"\"\n",
    "    Charge les chemins d'images et leurs labels depuis le r√©pertoire de donn√©es\n",
    "    \"\"\"\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    class_counts = {}\n",
    "    \n",
    "    for class_name in classes:\n",
    "        class_path = data_dir / class_name / \"images\"\n",
    "        if not class_path.exists():\n",
    "            # Essayer sans le sous-dossier \"images\"\n",
    "            class_path = data_dir / class_name\n",
    "        \n",
    "        if class_path.exists():\n",
    "            # R√©cup√©rer tous les fichiers images\n",
    "            image_files = list(class_path.glob(\"*.png\")) + list(class_path.glob(\"*.jpg\")) + list(class_path.glob(\"*.jpeg\"))\n",
    "            \n",
    "            for image_file in image_files:\n",
    "                image_paths.append(str(image_file))\n",
    "                labels.append(class_name)\n",
    "            \n",
    "            class_counts[class_name] = len(image_files)\n",
    "            print(f\"üìÅ {class_name}: {len(image_files)} images trouv√©es\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è R√©pertoire {class_path} non trouv√©\")\n",
    "            class_counts[class_name] = 0\n",
    "    \n",
    "    return image_paths, labels, class_counts\n",
    "\n",
    "def print_system_info():\n",
    "    \"\"\"Affiche les informations syst√®me\"\"\"\n",
    "    print(\"üíª INFORMATIONS SYST√àME:\")\n",
    "    print(f\"   - Python: {sys.version}\")\n",
    "    if tf is not None:\n",
    "        print(f\"   - TensorFlow: {tf.__version__}\")\n",
    "        print(f\"   - GPU disponible: {tf.config.list_physical_devices('GPU')}\")\n",
    "    print(f\"   - R√©pertoire de travail: {os.getcwd()}\")\n",
    "\n",
    "print_system_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c863a121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/cepa/DST/projet_DS/DS_COVID')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14639c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Fonctions utilitaires d√©finies\n"
     ]
    }
   ],
   "source": [
    "# ===================================\n",
    "# 1.3 FONCTIONS UTILITAIRES POUR LE CHARGEMENT\n",
    "# ===================================\n",
    "\n",
    "def load_image_paths_and_labels(data_dir, classes):\n",
    "    \"\"\"\n",
    "    Charge les chemins des images et leurs labels\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (image_paths, labels, class_counts)\n",
    "    \"\"\"\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    class_counts = {}\n",
    "    \n",
    "    for class_name in classes:\n",
    "        class_dir = data_dir / class_name / \"images\"\n",
    "        if not class_dir.exists():\n",
    "            continue\n",
    "            \n",
    "        # R√©cup√©ration des images\n",
    "        image_files = list(class_dir.glob(\"*.png\"))\n",
    "        class_counts[class_name] = len(image_files)\n",
    "        \n",
    "        # Ajout des chemins et labels\n",
    "        for img_path in image_files:\n",
    "            image_paths.append(str(img_path))\n",
    "            labels.append(class_name)\n",
    "    \n",
    "    return image_paths, labels, class_counts\n",
    "\n",
    "def load_and_preprocess_image(image_path, target_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Charge et pr√©processe une image\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Chemin vers l'image\n",
    "        target_size (tuple): Taille cible (largeur, hauteur)\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Image pr√©process√©e\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Chargement avec PIL\n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        # Redimensionnement\n",
    "        img = img.resize(target_size)\n",
    "        \n",
    "        # Conversion en array numpy\n",
    "        img_array = np.array(img)\n",
    "        \n",
    "        # Normalisation [0, 1]\n",
    "        img_array = img_array.astype(np.float32) / 255.0\n",
    "        \n",
    "        return img_array\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du chargement de {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_balanced_subset(image_paths, labels, max_per_class=500):\n",
    "    \"\"\"\n",
    "    Cr√©e un sous-ensemble √©quilibr√© du dataset\n",
    "    \n",
    "    Args:\n",
    "        image_paths (list): Liste des chemins d'images\n",
    "        labels (list): Liste des labels\n",
    "        max_per_class (int): Nombre maximum d'images par classe\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (subset_paths, subset_labels)\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({'path': image_paths, 'label': labels})\n",
    "    \n",
    "    # √âchantillonnage √©quilibr√©\n",
    "    balanced_df = df.groupby('label').apply(\n",
    "        lambda x: x.sample(n=min(len(x), max_per_class), random_state=42)\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    return balanced_df['path'].tolist(), balanced_df['label'].tolist()\n",
    "\n",
    "print(\"üîß Fonctions utilitaires d√©finies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f47e243",
   "metadata": {},
   "source": [
    "## üìä Section 2: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "723001d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Chargement des donn√©es...\n",
      "üìä Total d'images: 21165\n",
      "üìä Total de labels: 21165\n",
      "üìä Total d'images: 21165\n",
      "üìä Total de labels: 21165\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müìä Total de labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(labels)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Cr√©ation du DataFrame pour l'analyse\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m df_analysis \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_path\u001b[39m\u001b[38;5;124m'\u001b[39m: image_paths,\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m: labels\n\u001b[1;32m     16\u001b[0m })\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Analyse de la distribution des classes\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müè∑Ô∏è Distribution des classes:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# ===================================\n",
    "# 2.1 CHARGEMENT ET ANALYSE DE LA DISTRIBUTION\n",
    "# ===================================\n",
    "\n",
    "# Chargement des chemins et labels\n",
    "print(\"üìÇ Chargement des donn√©es...\")\n",
    "image_paths, labels, class_counts = load_image_paths_and_labels(DATA_DIR, CLASSES)\n",
    "\n",
    "print(f\"üìä Total d'images: {len(image_paths)}\")\n",
    "print(f\"üìä Total de labels: {len(labels)}\")\n",
    "\n",
    "# Cr√©ation du DataFrame pour l'analyse\n",
    "df_analysis = pd.DataFrame({\n",
    "    'image_path': image_paths,\n",
    "    'label': labels\n",
    "})\n",
    "\n",
    "# Analyse de la distribution des classes\n",
    "print(\"\\nüè∑Ô∏è Distribution des classes:\")\n",
    "class_distribution = df_analysis['label'].value_counts()\n",
    "print(class_distribution)\n",
    "\n",
    "# Visualisation de la distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Graphique en barres\n",
    "class_distribution.plot(kind='bar', ax=ax1, color='skyblue', alpha=0.8)\n",
    "ax1.set_title('Distribution des Classes', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Classes', fontsize=12)\n",
    "ax1.set_ylabel('Nombre d\\'images', fontsize=12)\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Graphique en secteurs\n",
    "ax2.pie(class_distribution.values, labels=class_distribution.index, autopct='%1.1f%%', \n",
    "        startangle=90, colors=plt.cm.Set3.colors)\n",
    "ax2.set_title('R√©partition des Classes (%)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistiques d√©taill√©es\n",
    "print(\"\\nüìà Statistiques d√©taill√©es:\")\n",
    "total_images = len(image_paths)\n",
    "for class_name, count in class_distribution.items():\n",
    "    percentage = (count / total_images) * 100\n",
    "    print(f\"   {class_name}: {count} images ({percentage:.2f}%)\")\n",
    "\n",
    "# D√©tection du d√©s√©quilibre\n",
    "max_count = class_distribution.max()\n",
    "min_count = class_distribution.min()\n",
    "imbalance_ratio = max_count / min_count\n",
    "\n",
    "print(f\"\\n‚öñÔ∏è Ratio de d√©s√©quilibre: {imbalance_ratio:.2f}\")\n",
    "if imbalance_ratio > 2:\n",
    "    print(\"‚ö†Ô∏è  Dataset d√©s√©quilibr√© d√©tect√© - techniques de r√©√©quilibrage recommand√©es\")\n",
    "else:\n",
    "    print(\"‚úÖ Dataset relativement √©quilibr√©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a686f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 2.2 VISUALISATION DES IMAGES REPR√âSENTATIVES\n",
    "# ===================================\n",
    "\n",
    "def visualize_sample_images(image_paths, labels, classes, n_samples=3):\n",
    "    \"\"\"\n",
    "    Visualise des √©chantillons d'images pour chaque classe\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(len(classes), n_samples, figsize=(15, 12))\n",
    "    fig.suptitle('√âchantillons d\\'Images par Classe', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    df_viz = pd.DataFrame({'path': image_paths, 'label': labels})\n",
    "    \n",
    "    for i, class_name in enumerate(classes):\n",
    "        class_images = df_viz[df_viz['label'] == class_name]['path'].tolist()\n",
    "        \n",
    "        # S√©lection al√©atoire d'√©chantillons\n",
    "        samples = np.random.choice(class_images, min(n_samples, len(class_images)), replace=False)\n",
    "        \n",
    "        for j, img_path in enumerate(samples):\n",
    "            try:\n",
    "                # Chargement de l'image\n",
    "                img = cv2.imread(img_path)\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                # Affichage\n",
    "                axes[i, j].imshow(img, cmap='gray' if len(img.shape) == 2 else None)\n",
    "                axes[i, j].set_title(f'{class_name}', fontsize=10)\n",
    "                axes[i, j].axis('off')\n",
    "                \n",
    "            except Exception as e:\n",
    "                axes[i, j].text(0.5, 0.5, f'Erreur:\\n{str(e)}', \n",
    "                              ha='center', va='center', transform=axes[i, j].transAxes)\n",
    "                axes[i, j].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualisation des √©chantillons\n",
    "print(\"üñºÔ∏è Visualisation des √©chantillons d'images...\")\n",
    "visualize_sample_images(image_paths, labels, CLASSES, n_samples=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2d10ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 2.3 ANALYSE DES PROPRI√âT√âS DES IMAGES\n",
    "# ===================================\n",
    "\n",
    "def analyze_image_properties(image_paths, sample_size=100):\n",
    "    \"\"\"\n",
    "    Analyse les propri√©t√©s des images (taille, intensit√©, etc.)\n",
    "    \"\"\"\n",
    "    print(f\"üîç Analyse des propri√©t√©s sur un √©chantillon de {sample_size} images...\")\n",
    "    \n",
    "    # √âchantillonnage al√©atoire\n",
    "    sample_paths = np.random.choice(image_paths, min(sample_size, len(image_paths)), replace=False)\n",
    "    \n",
    "    properties = {\n",
    "        'widths': [],\n",
    "        'heights': [],\n",
    "        'channels': [],\n",
    "        'mean_intensities': [],\n",
    "        'std_intensities': [],\n",
    "        'file_sizes': []\n",
    "    }\n",
    "    \n",
    "    for img_path in sample_paths:\n",
    "        try:\n",
    "            # Chargement avec PIL pour les propri√©t√©s de base\n",
    "            img_pil = Image.open(img_path)\n",
    "            width, height = img_pil.size\n",
    "            \n",
    "            # Chargement avec OpenCV pour l'analyse d'intensit√©\n",
    "            img_cv = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            \n",
    "            # Propri√©t√©s\n",
    "            properties['widths'].append(width)\n",
    "            properties['heights'].append(height)\n",
    "            properties['channels'].append(len(img_pil.getbands()))\n",
    "            properties['mean_intensities'].append(np.mean(img_cv))\n",
    "            properties['std_intensities'].append(np.std(img_cv))\n",
    "            properties['file_sizes'].append(os.path.getsize(img_path) / 1024)  # KB\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Erreur avec {img_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return properties\n",
    "\n",
    "# Analyse des propri√©t√©s\n",
    "properties = analyze_image_properties(image_paths, sample_size=200)\n",
    "\n",
    "# Cr√©ation du DataFrame d'analyse\n",
    "df_props = pd.DataFrame(properties)\n",
    "\n",
    "# Affichage des statistiques\n",
    "print(\"\\nüìä Statistiques des propri√©t√©s d'images:\")\n",
    "print(df_props.describe())\n",
    "\n",
    "# Visualisation des distributions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Distribution des Propri√©t√©s d\\'Images', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Tailles\n",
    "axes[0, 0].hist(df_props['widths'], bins=20, alpha=0.7, color='skyblue')\n",
    "axes[0, 0].set_title('Distribution des Largeurs')\n",
    "axes[0, 0].set_xlabel('Largeur (pixels)')\n",
    "\n",
    "axes[0, 1].hist(df_props['heights'], bins=20, alpha=0.7, color='lightgreen')\n",
    "axes[0, 1].set_title('Distribution des Hauteurs')\n",
    "axes[0, 1].set_xlabel('Hauteur (pixels)')\n",
    "\n",
    "# Intensit√©s\n",
    "axes[0, 2].hist(df_props['mean_intensities'], bins=20, alpha=0.7, color='coral')\n",
    "axes[0, 2].set_title('Distribution des Intensit√©s Moyennes')\n",
    "axes[0, 2].set_xlabel('Intensit√© Moyenne')\n",
    "\n",
    "axes[1, 0].hist(df_props['std_intensities'], bins=20, alpha=0.7, color='gold')\n",
    "axes[1, 0].set_title('Distribution des √âcarts-Types d\\'Intensit√©')\n",
    "axes[1, 0].set_xlabel('√âcart-Type Intensit√©')\n",
    "\n",
    "# Tailles de fichiers\n",
    "axes[1, 1].hist(df_props['file_sizes'], bins=20, alpha=0.7, color='mediumpurple')\n",
    "axes[1, 1].set_title('Distribution des Tailles de Fichiers')\n",
    "axes[1, 1].set_xlabel('Taille (KB)')\n",
    "\n",
    "# Corr√©lation largeur/hauteur\n",
    "axes[1, 2].scatter(df_props['widths'], df_props['heights'], alpha=0.6, color='darkblue')\n",
    "axes[1, 2].set_title('Corr√©lation Largeur/Hauteur')\n",
    "axes[1, 2].set_xlabel('Largeur (pixels)')\n",
    "axes[1, 2].set_ylabel('Hauteur (pixels)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# D√©tection des formats non standards\n",
    "print(f\"\\nüìê Formats d'images d√©tect√©s:\")\n",
    "unique_dimensions = df_props.groupby(['widths', 'heights']).size().reset_index(name='count')\n",
    "print(unique_dimensions.sort_values('count', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e993d94f",
   "metadata": {},
   "source": [
    "## üîß Section 3: Data Preprocessing and Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe58354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 3.1 PR√âPARATION DES DONN√âES D'ENTRA√éNEMENT\n",
    "# ===================================\n",
    "\n",
    "print(\"üîÑ Pr√©paration du dataset pour l'entra√Ænement...\")\n",
    "\n",
    "# Cr√©ation d'un sous-ensemble √©quilibr√© pour des temps de traitement raisonnables\n",
    "print(\"‚öñÔ∏è Cr√©ation d'un sous-ensemble √©quilibr√©...\")\n",
    "balanced_paths, balanced_labels = create_balanced_subset(\n",
    "    image_paths, labels, max_per_class=1000\n",
    ")\n",
    "\n",
    "print(f\"üìä Dataset √©quilibr√©: {len(balanced_paths)} images\")\n",
    "\n",
    "# V√©rification de l'√©quilibrage\n",
    "balanced_distribution = pd.Series(balanced_labels).value_counts()\n",
    "print(\"üè∑Ô∏è Nouvelle distribution:\")\n",
    "print(balanced_distribution)\n",
    "\n",
    "# Encodage des labels\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(balanced_labels)\n",
    "\n",
    "print(f\"üî¢ Classes encod√©es: {dict(zip(label_encoder.classes_, range(len(label_encoder.classes_))))}\")\n",
    "\n",
    "# Division train/validation/test\n",
    "print(\"‚úÇÔ∏è Division du dataset...\")\n",
    "\n",
    "# Premi√®re division: train+val / test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    balanced_paths, encoded_labels,\n",
    "    test_size=TEST_SPLIT,\n",
    "    stratify=encoded_labels,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Seconde division: train / val\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=VALIDATION_SPLIT/(1-TEST_SPLIT),  # Ajustement pour avoir 20% du total\n",
    "    stratify=y_temp,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"üìä Train: {len(X_train)} images\")\n",
    "print(f\"üìä Validation: {len(X_val)} images\")\n",
    "print(f\"üìä Test: {len(X_test)} images\")\n",
    "\n",
    "# V√©rification de la stratification\n",
    "print(\"\\nüéØ V√©rification de la stratification:\")\n",
    "for split_name, y_split in [(\"Train\", y_train), (\"Val\", y_val), (\"Test\", y_test)]:\n",
    "    distribution = pd.Series(y_split).value_counts().sort_index()\n",
    "    percentages = (distribution / len(y_split) * 100).round(1)\n",
    "    print(f\"{split_name}: {dict(zip(label_encoder.classes_, percentages.values))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9eb66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 3.2 PIPELINE DE PR√âPROCESSING D'IMAGES\n",
    "# ===================================\n",
    "\n",
    "class ImagePreprocessor:\n",
    "    \"\"\"Pipeline de pr√©processing d'images optimis√©\"\"\"\n",
    "    \n",
    "    def __init__(self, target_size=(224, 224), normalize=True):\n",
    "        self.target_size = target_size\n",
    "        self.normalize = normalize\n",
    "    \n",
    "    def preprocess_single_image(self, image_path):\n",
    "        \"\"\"Pr√©processe une seule image\"\"\"\n",
    "        try:\n",
    "            # Chargement\n",
    "            img = cv2.imread(image_path)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Redimensionnement\n",
    "            img = cv2.resize(img, self.target_size)\n",
    "            \n",
    "            # Normalisation\n",
    "            if self.normalize:\n",
    "                img = img.astype(np.float32) / 255.0\n",
    "            \n",
    "            return img\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Erreur preprocessing {image_path}: {e}\")\n",
    "            return np.zeros((*self.target_size, 3), dtype=np.float32)\n",
    "    \n",
    "    def preprocess_batch(self, image_paths, batch_size=32):\n",
    "        \"\"\"Pr√©processe un lot d'images avec gestion m√©moire\"\"\"\n",
    "        n_images = len(image_paths)\n",
    "        n_batches = (n_images + batch_size - 1) // batch_size\n",
    "        \n",
    "        # Initialisation du tableau de sortie\n",
    "        images = np.zeros((n_images, *self.target_size, 3), dtype=np.float32)\n",
    "        \n",
    "        print(f\"üîÑ Preprocessing {n_images} images en {n_batches} batches...\")\n",
    "        \n",
    "        for i in range(n_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = min((i + 1) * batch_size, n_images)\n",
    "            \n",
    "            # Traitement du batch\n",
    "            for j, img_path in enumerate(image_paths[start_idx:end_idx]):\n",
    "                images[start_idx + j] = self.preprocess_single_image(img_path)\n",
    "            \n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"   üìä Batch {i+1}/{n_batches} termin√©\")\n",
    "        \n",
    "        return images\n",
    "\n",
    "# Initialisation du preprocessor\n",
    "preprocessor = ImagePreprocessor(target_size=IMG_SIZE, normalize=True)\n",
    "\n",
    "# Preprocessing des donn√©es\n",
    "print(\"üîÑ Preprocessing des images...\")\n",
    "\n",
    "X_train_processed = preprocessor.preprocess_batch(X_train)\n",
    "X_val_processed = preprocessor.preprocess_batch(X_val)\n",
    "X_test_processed = preprocessor.preprocess_batch(X_test)\n",
    "\n",
    "print(f\"‚úÖ Preprocessing termin√©!\")\n",
    "print(f\"üìä Shape train: {X_train_processed.shape}\")\n",
    "print(f\"üìä Shape validation: {X_val_processed.shape}\")\n",
    "print(f\"üìä Shape test: {X_test_processed.shape}\")\n",
    "\n",
    "# Conversion des labels en format cat√©goriel pour le deep learning\n",
    "y_train_categorical = to_categorical(y_train, num_classes=len(CLASSES))\n",
    "y_val_categorical = to_categorical(y_val, num_classes=len(CLASSES))\n",
    "y_test_categorical = to_categorical(y_test, num_classes=len(CLASSES))\n",
    "\n",
    "print(f\"üìä Shape labels train: {y_train_categorical.shape}\")\n",
    "\n",
    "# Visualisation d'un √©chantillon preprocess√©\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "fig.suptitle('Images Preprocess√©es - √âchantillons', fontsize=14, fontweight='bold')\n",
    "\n",
    "for i in range(8):\n",
    "    row = i // 4\n",
    "    col = i % 4\n",
    "    \n",
    "    # S√©lection d'une image al√©atoire\n",
    "    idx = np.random.randint(0, len(X_train_processed))\n",
    "    img = X_train_processed[idx]\n",
    "    label = label_encoder.inverse_transform([y_train[idx]])[0]\n",
    "    \n",
    "    axes[row, col].imshow(img)\n",
    "    axes[row, col].set_title(f'{label}', fontsize=10)\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6224d67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 3.3 DATA AUGMENTATION POUR LE DEEP LEARNING\n",
    "# ===================================\n",
    "\n",
    "# Configuration de l'augmentation de donn√©es\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=10,           # Rotation al√©atoire jusqu'√† 10¬∞\n",
    "    width_shift_range=0.1,       # D√©calage horizontal 10%\n",
    "    height_shift_range=0.1,      # D√©calage vertical 10%\n",
    "    zoom_range=0.1,              # Zoom al√©atoire 10%\n",
    "    horizontal_flip=True,        # Miroir horizontal\n",
    "    brightness_range=[0.8, 1.2], # Variation de luminosit√©\n",
    "    fill_mode='nearest'          # Mode de remplissage\n",
    ")\n",
    "\n",
    "# G√©n√©rateur pour validation (pas d'augmentation)\n",
    "val_datagen = ImageDataGenerator()\n",
    "\n",
    "print(\"üîÑ Configuration de l'augmentation de donn√©es\")\n",
    "\n",
    "# D√©monstration de l'augmentation\n",
    "def demonstrate_augmentation(X_sample, y_sample, n_augmentations=6):\n",
    "    \"\"\"Montre l'effet de l'augmentation sur quelques images\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, n_augmentations + 1, figsize=(20, 8))\n",
    "    fig.suptitle('D√©monstration de l\\'Augmentation de Donn√©es', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for row in range(2):\n",
    "        # Image originale\n",
    "        idx = np.random.randint(0, len(X_sample))\n",
    "        original_img = X_sample[idx]\n",
    "        label = label_encoder.inverse_transform([y_sample[idx]])[0]\n",
    "        \n",
    "        axes[row, 0].imshow(original_img)\n",
    "        axes[row, 0].set_title(f'Original\\n{label}')\n",
    "        axes[row, 0].axis('off')\n",
    "        \n",
    "        # Images augment√©es\n",
    "        img_batch = np.expand_dims(original_img, 0)\n",
    "        augmented_generator = train_datagen.flow(img_batch, batch_size=1)\n",
    "        \n",
    "        for col in range(1, n_augmentations + 1):\n",
    "            augmented_batch = next(augmented_generator)\n",
    "            augmented_img = augmented_batch[0]\n",
    "            \n",
    "            # Clip pour √©viter les valeurs hors [0,1]\n",
    "            augmented_img = np.clip(augmented_img, 0, 1)\n",
    "            \n",
    "            axes[row, col].imshow(augmented_img)\n",
    "            axes[row, col].set_title(f'Augment√©e {col}')\n",
    "            axes[row, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# D√©monstration\n",
    "print(\"üñºÔ∏è D√©monstration de l'augmentation...\")\n",
    "demonstrate_augmentation(X_train_processed, y_train, n_augmentations=5)\n",
    "\n",
    "print(\"‚úÖ Pipeline d'augmentation configur√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842af9b8",
   "metadata": {},
   "source": [
    "## üéØ Section 4: Baseline Models Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba007780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 4.1 EXTRACTION DE FEATURES POUR ML TRADITIONNEL\n",
    "# ===================================\n",
    "\n",
    "def extract_traditional_features(images):\n",
    "    \"\"\"\n",
    "    Extrait des features traditionnelles pour les mod√®les ML classiques\n",
    "    \n",
    "    Features extraites:\n",
    "    - Statistiques d'intensit√© (moyenne, std, min, max)\n",
    "    - Histogramme des niveaux de gris\n",
    "    - Features de texture (Local Binary Pattern simul√©)\n",
    "    \"\"\"\n",
    "    print(f\"üîç Extraction de features pour {len(images)} images...\")\n",
    "    \n",
    "    features_list = []\n",
    "    \n",
    "    for i, img in enumerate(images):\n",
    "        # Conversion en niveaux de gris\n",
    "        if len(img.shape) == 3:\n",
    "            gray_img = cv2.cvtColor((img * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "        else:\n",
    "            gray_img = (img * 255).astype(np.uint8)\n",
    "        \n",
    "        features = []\n",
    "        \n",
    "        # 1. Statistiques d'intensit√©\n",
    "        features.extend([\n",
    "            np.mean(gray_img),\n",
    "            np.std(gray_img),\n",
    "            np.min(gray_img),\n",
    "            np.max(gray_img),\n",
    "            np.percentile(gray_img, 25),\n",
    "            np.percentile(gray_img, 50),\n",
    "            np.percentile(gray_img, 75)\n",
    "        ])\n",
    "        \n",
    "        # 2. Histogramme (16 bins)\n",
    "        hist, _ = np.histogram(gray_img, bins=16, range=(0, 256))\n",
    "        hist = hist / np.sum(hist)  # Normalisation\n",
    "        features.extend(hist)\n",
    "        \n",
    "        # 3. Features de texture simples\n",
    "        # Gradient\n",
    "        grad_x = cv2.Sobel(gray_img, cv2.CV_64F, 1, 0, ksize=3)\n",
    "        grad_y = cv2.Sobel(gray_img, cv2.CV_64F, 0, 1, ksize=3)\n",
    "        gradient_magnitude = np.sqrt(grad_x**2 + grad_y**2)\n",
    "        \n",
    "        features.extend([\n",
    "            np.mean(gradient_magnitude),\n",
    "            np.std(gradient_magnitude)\n",
    "        ])\n",
    "        \n",
    "        # 4. Features g√©om√©triques (moments)\n",
    "        moments = cv2.moments(gray_img)\n",
    "        if moments['m00'] != 0:\n",
    "            cx = moments['m10'] / moments['m00']\n",
    "            cy = moments['m01'] / moments['m00']\n",
    "            features.extend([cx, cy])\n",
    "        else:\n",
    "            features.extend([0, 0])\n",
    "        \n",
    "        # 5. √ânergie et entropie\n",
    "        # Normalisation de l'image pour calculer l'entropie\n",
    "        normalized = gray_img / 255.0\n",
    "        entropy = -np.sum(normalized * np.log(normalized + 1e-10))\n",
    "        energy = np.sum(normalized ** 2)\n",
    "        \n",
    "        features.extend([entropy, energy])\n",
    "        \n",
    "        features_list.append(features)\n",
    "        \n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"   üìä {i+1}/{len(images)} images trait√©es\")\n",
    "    \n",
    "    feature_matrix = np.array(features_list)\n",
    "    print(f\"‚úÖ Features extraites: shape {feature_matrix.shape}\")\n",
    "    \n",
    "    return feature_matrix\n",
    "\n",
    "# Extraction des features\n",
    "print(\"üîç Extraction des features traditionnelles...\")\n",
    "X_train_features = extract_traditional_features(X_train_processed)\n",
    "X_val_features = extract_traditional_features(X_val_processed)\n",
    "X_test_features = extract_traditional_features(X_test_processed)\n",
    "\n",
    "# Normalisation des features\n",
    "scaler = StandardScaler()\n",
    "X_train_features_scaled = scaler.fit_transform(X_train_features)\n",
    "X_val_features_scaled = scaler.transform(X_val_features)\n",
    "X_test_features_scaled = scaler.transform(X_test_features)\n",
    "\n",
    "print(f\"üìä Features shape train: {X_train_features_scaled.shape}\")\n",
    "print(f\"üìä Features normalis√©es avec StandardScaler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10f7a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 4.2 MOD√àLES BASELINE TRADITIONNELS\n",
    "# ===================================\n",
    "\n",
    "# Dictionnaire pour stocker les r√©sultats\n",
    "baseline_results = {}\n",
    "\n",
    "def evaluate_model(model, X_train, X_val, y_train, y_val, model_name):\n",
    "    \"\"\"√âvalue un mod√®le et stocke les r√©sultats\"\"\"\n",
    "    \n",
    "    print(f\"\\nüîÑ Entra√Ænement de {model_name}...\")\n",
    "    \n",
    "    # Entra√Ænement\n",
    "    start_time = datetime.now()\n",
    "    model.fit(X_train, y_train)\n",
    "    training_time = datetime.now() - start_time\n",
    "    \n",
    "    # Pr√©dictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    \n",
    "    # M√©triques\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    \n",
    "    # Scores d√©taill√©s pour la validation\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_val, y_val_pred, average='weighted')\n",
    "    \n",
    "    # Stockage des r√©sultats\n",
    "    results = {\n",
    "        'model': model,\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'val_accuracy': val_accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'training_time': training_time.total_seconds(),\n",
    "        'y_val_pred': y_val_pred\n",
    "    }\n",
    "    \n",
    "    baseline_results[model_name] = results\n",
    "    \n",
    "    print(f\"‚úÖ {model_name}:\")\n",
    "    print(f\"   üìä Train Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"   üìä Val Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"   üìä F1-Score: {f1:.4f}\")\n",
    "    print(f\"   ‚è±Ô∏è Training Time: {training_time.total_seconds():.2f}s\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# D√©finition des mod√®les baseline\n",
    "baseline_models = {\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'SVM': SVC(\n",
    "        kernel='rbf',\n",
    "        C=1.0,\n",
    "        random_state=42,\n",
    "        probability=True  # Pour les probabilit√©s\n",
    "    ),\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'Decision Tree': DecisionTreeClassifier(\n",
    "        max_depth=15,\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "# Entra√Ænement et √©valuation de tous les mod√®les\n",
    "print(\"üöÄ Entra√Ænement des mod√®les baseline...\")\n",
    "\n",
    "for model_name, model in baseline_models.items():\n",
    "    evaluate_model(\n",
    "        model, \n",
    "        X_train_features_scaled, \n",
    "        X_val_features_scaled, \n",
    "        y_train, \n",
    "        y_val, \n",
    "        model_name\n",
    "    )\n",
    "\n",
    "print(\"\\n‚úÖ Tous les mod√®les baseline entra√Æn√©s!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3327299f",
   "metadata": {},
   "source": [
    "## üå≥ Section 5: Bagging Ensemble Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d4e0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 5.1 IMPL√âMENTATION DES M√âTHODES DE BAGGING\n",
    "# ===================================\n",
    "\n",
    "print(\"üå≥ Impl√©mentation des m√©thodes de Bagging...\")\n",
    "\n",
    "# Dictionnaire pour les r√©sultats de bagging\n",
    "bagging_results = {}\n",
    "\n",
    "# 5.1.1 Random Forest optimis√©\n",
    "print(\"\\nüîÑ Random Forest optimis√©...\")\n",
    "rf_optimized = RandomForestClassifier(\n",
    "    n_estimators=200,           # Plus d'arbres\n",
    "    max_depth=15,               # Profondeur contr√¥l√©e\n",
    "    min_samples_split=5,        # Contr√¥le overfitting\n",
    "    min_samples_leaf=2,         # Contr√¥le overfitting\n",
    "    max_features='sqrt',        # Features al√©atoires\n",
    "    bootstrap=True,             # Bagging activ√©\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "evaluate_model(\n",
    "    rf_optimized, \n",
    "    X_train_features_scaled, \n",
    "    X_val_features_scaled, \n",
    "    y_train, \n",
    "    y_val, \n",
    "    'Random Forest Optimized'\n",
    ")\n",
    "\n",
    "# 5.1.2 Extra Trees (Extremely Randomized Trees)\n",
    "print(\"\\nüîÑ Extra Trees...\")\n",
    "extra_trees = ExtraTreesClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=15,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    max_features='sqrt',\n",
    "    bootstrap=True,             # Bagging\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "evaluate_model(\n",
    "    extra_trees, \n",
    "    X_train_features_scaled, \n",
    "    X_val_features_scaled, \n",
    "    y_train, \n",
    "    y_val, \n",
    "    'Extra Trees'\n",
    ")\n",
    "\n",
    "# 5.1.3 Bagging avec diff√©rents mod√®les de base\n",
    "print(\"\\nüîÑ Bagging avec SVM...\")\n",
    "bagging_svm = BaggingClassifier(\n",
    "    estimator=SVC(kernel='rbf', probability=True, random_state=42),\n",
    "    n_estimators=50,            # Moins d'estimateurs car SVM est co√ªteux\n",
    "    max_samples=0.8,            # 80% des √©chantillons par mod√®le\n",
    "    max_features=0.8,           # 80% des features par mod√®le\n",
    "    bootstrap=True,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "evaluate_model(\n",
    "    bagging_svm, \n",
    "    X_train_features_scaled, \n",
    "    X_val_features_scaled, \n",
    "    y_train, \n",
    "    y_val, \n",
    "    'Bagging SVM'\n",
    ")\n",
    "\n",
    "# 5.1.4 Bagging avec Logistic Regression\n",
    "print(\"\\nüîÑ Bagging avec Logistic Regression...\")\n",
    "bagging_lr = BaggingClassifier(\n",
    "    estimator=LogisticRegression(max_iter=1000, random_state=42),\n",
    "    n_estimators=100,\n",
    "    max_samples=0.8,\n",
    "    max_features=0.8,\n",
    "    bootstrap=True,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "evaluate_model(\n",
    "    bagging_lr, \n",
    "    X_train_features_scaled, \n",
    "    X_val_features_scaled, \n",
    "    y_train, \n",
    "    y_val, \n",
    "    'Bagging Logistic Regression'\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ M√©thodes de Bagging entra√Æn√©es!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b56e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 5.2 ANALYSE DE L'IMPORTANCE DES FEATURES (BAGGING)\n",
    "# ===================================\n",
    "\n",
    "def analyze_feature_importance(model, feature_names, model_name, top_n=20):\n",
    "    \"\"\"Analyse l'importance des features pour les mod√®les tree-based\"\"\"\n",
    "    \n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "        \n",
    "        # Cr√©ation du DataFrame\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': importances\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        # Visualisation\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        top_features = feature_importance_df.head(top_n)\n",
    "        \n",
    "        sns.barplot(data=top_features, x='importance', y='feature', palette='viridis')\n",
    "        plt.title(f'Top {top_n} Features - {model_name}', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Importance')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return feature_importance_df\n",
    "    else:\n",
    "        print(f\"‚ùå {model_name} ne supporte pas l'analyse d'importance des features\")\n",
    "        return None\n",
    "\n",
    "# G√©n√©ration des noms de features\n",
    "feature_names = (\n",
    "    ['mean', 'std', 'min', 'max', 'q25', 'q50', 'q75'] +  # Statistiques\n",
    "    [f'hist_{i}' for i in range(16)] +                     # Histogramme\n",
    "    ['grad_mean', 'grad_std'] +                            # Gradient\n",
    "    ['cx', 'cy'] +                                         # Moments\n",
    "    ['entropy', 'energy']                                  # Texture\n",
    ")\n",
    "\n",
    "# Analyse pour Random Forest optimis√©\n",
    "print(\"üîç Analyse d'importance des features - Random Forest...\")\n",
    "rf_model = baseline_results['Random Forest Optimized']['model']\n",
    "rf_importance = analyze_feature_importance(rf_model, feature_names, 'Random Forest Optimized')\n",
    "\n",
    "# Analyse pour Extra Trees\n",
    "print(\"üîç Analyse d'importance des features - Extra Trees...\")\n",
    "et_model = baseline_results['Extra Trees']['model']\n",
    "et_importance = analyze_feature_importance(et_model, feature_names, 'Extra Trees')\n",
    "\n",
    "# Comparaison des importances\n",
    "if rf_importance is not None and et_importance is not None:\n",
    "    print(\"\\nüìä Comparaison des top 10 features:\")\n",
    "    comparison_df = pd.merge(\n",
    "        rf_importance.head(10)[['feature', 'importance']].rename(columns={'importance': 'RF_importance'}),\n",
    "        et_importance.head(10)[['feature', 'importance']].rename(columns={'importance': 'ET_importance'}),\n",
    "        on='feature',\n",
    "        how='outer'\n",
    "    ).fillna(0)\n",
    "    \n",
    "    print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c563364f",
   "metadata": {},
   "source": [
    "## üöÄ Section 6: Boosting Ensemble Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db563ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 6.1 IMPL√âMENTATION DES M√âTHODES DE BOOSTING\n",
    "# ===================================\n",
    "\n",
    "print(\"üöÄ Impl√©mentation des m√©thodes de Boosting...\")\n",
    "\n",
    "# 6.1.1 AdaBoost\n",
    "print(\"\\nüîÑ AdaBoost...\")\n",
    "ada_boost = AdaBoostClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=3, random_state=42),\n",
    "    n_estimators=100,\n",
    "    learning_rate=1.0,\n",
    "    algorithm='SAMME',  # Supporte multiclass\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "evaluate_model(\n",
    "    ada_boost, \n",
    "    X_train_features_scaled, \n",
    "    X_val_features_scaled, \n",
    "    y_train, \n",
    "    y_val, \n",
    "    'AdaBoost'\n",
    ")\n",
    "\n",
    "# 6.1.2 Gradient Boosting\n",
    "print(\"\\nüîÑ Gradient Boosting...\")\n",
    "gradient_boost = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    subsample=0.8,              # Stochastic gradient boosting\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "evaluate_model(\n",
    "    gradient_boost, \n",
    "    X_train_features_scaled, \n",
    "    X_val_features_scaled, \n",
    "    y_train, \n",
    "    y_val, \n",
    "    'Gradient Boosting'\n",
    ")\n",
    "\n",
    "# 6.1.3 XGBoost (si disponible)\n",
    "try:\n",
    "    print(\"\\nüîÑ XGBoost...\")\n",
    "    xgb_classifier = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=3,\n",
    "        min_child_weight=1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        eval_metric='mlogloss'  # Pour multiclass\n",
    "    )\n",
    "    \n",
    "    evaluate_model(\n",
    "        xgb_classifier, \n",
    "        X_train_features_scaled, \n",
    "        X_val_features_scaled, \n",
    "        y_train, \n",
    "        y_val, \n",
    "        'XGBoost'\n",
    "    )\n",
    "    \n",
    "except NameError:\n",
    "    print(\"‚ö†Ô∏è XGBoost non disponible - passage ignor√©\")\n",
    "\n",
    "print(\"\\n‚úÖ M√©thodes de Boosting entra√Æn√©es!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7f85b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 6.2 COMPARAISON BAGGING VS BOOSTING\n",
    "# ===================================\n",
    "\n",
    "def create_ensemble_comparison():\n",
    "    \"\"\"Compare les performances des m√©thodes d'ensemble\"\"\"\n",
    "    \n",
    "    # R√©cup√©ration des m√©triques pour comparaison\n",
    "    comparison_data = []\n",
    "    \n",
    "    for model_name, results in baseline_results.items():\n",
    "        comparison_data.append({\n",
    "            'Model': model_name,\n",
    "            'Type': get_model_type(model_name),\n",
    "            'Val_Accuracy': results['val_accuracy'],\n",
    "            'F1_Score': results['f1_score'],\n",
    "            'Training_Time': results['training_time']\n",
    "        })\n",
    "    \n",
    "    df_comparison = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    return df_comparison\n",
    "\n",
    "def get_model_type(model_name):\n",
    "    \"\"\"D√©termine le type de mod√®le\"\"\"\n",
    "    if any(keyword in model_name.lower() for keyword in ['random forest', 'extra trees', 'bagging']):\n",
    "        return 'Bagging'\n",
    "    elif any(keyword in model_name.lower() for keyword in ['ada', 'gradient', 'xgboost']):\n",
    "        return 'Boosting'\n",
    "    else:\n",
    "        return 'Baseline'\n",
    "\n",
    "# Cr√©ation de la comparaison\n",
    "df_comparison = create_ensemble_comparison()\n",
    "\n",
    "print(\"üìä Comparaison des m√©thodes d'ensemble:\")\n",
    "print(df_comparison.sort_values('Val_Accuracy', ascending=False))\n",
    "\n",
    "# Visualisation comparative\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Comparaison des M√©thodes d\\'Ensemble', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Accuracy par type\n",
    "sns.boxplot(data=df_comparison, x='Type', y='Val_Accuracy', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Accuracy par Type de M√©thode')\n",
    "axes[0, 0].set_ylabel('Validation Accuracy')\n",
    "\n",
    "# F1-Score par type\n",
    "sns.boxplot(data=df_comparison, x='Type', y='F1_Score', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('F1-Score par Type de M√©thode')\n",
    "axes[0, 1].set_ylabel('F1 Score')\n",
    "\n",
    "# Temps d'entra√Ænement par type\n",
    "sns.boxplot(data=df_comparison, x='Type', y='Training_Time', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Temps d\\'Entra√Ænement par Type')\n",
    "axes[1, 0].set_ylabel('Training Time (s)')\n",
    "\n",
    "# Correlation Accuracy vs Training Time\n",
    "sns.scatterplot(data=df_comparison, x='Training_Time', y='Val_Accuracy', \n",
    "                hue='Type', size='F1_Score', sizes=(50, 200), ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Accuracy vs Temps d\\'Entra√Ænement')\n",
    "axes[1, 1].set_xlabel('Training Time (s)')\n",
    "axes[1, 1].set_ylabel('Validation Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistiques par type\n",
    "print(\"\\nüìà Statistiques par type de m√©thode:\")\n",
    "type_stats = df_comparison.groupby('Type').agg({\n",
    "    'Val_Accuracy': ['mean', 'std', 'max'],\n",
    "    'F1_Score': ['mean', 'std', 'max'],\n",
    "    'Training_Time': ['mean', 'std', 'min']\n",
    "}).round(4)\n",
    "\n",
    "print(type_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2251d2",
   "metadata": {},
   "source": [
    "## üß† Section 7: Deep Learning Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034de566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 7.1 ARCHITECTURE CNN CUSTOM\n",
    "# ===================================\n",
    "\n",
    "def create_custom_cnn(input_shape=(224, 224, 3), num_classes=4):\n",
    "    \"\"\"\n",
    "    Cr√©e une architecture CNN personnalis√©e pour la classification d'images m√©dicales\n",
    "    \"\"\"\n",
    "    \n",
    "    model = models.Sequential([\n",
    "        # Bloc 1: Extraction de features de bas niveau\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape, padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Bloc 2: Features de niveau interm√©diaire\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Bloc 3: Features de haut niveau\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Bloc 4: Features complexes\n",
    "        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Classification\n",
    "        layers.GlobalAveragePooling2D(),  # Alternative √† Flatten + Dense\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Cr√©ation du mod√®le custom\n",
    "print(\"üß† Cr√©ation du mod√®le CNN personnalis√©...\")\n",
    "custom_cnn = create_custom_cnn(input_shape=(*IMG_SIZE, 3), num_classes=len(CLASSES))\n",
    "\n",
    "# Affichage de l'architecture\n",
    "print(\"üìã Architecture du mod√®le:\")\n",
    "custom_cnn.summary()\n",
    "\n",
    "# Visualisation de l'architecture\n",
    "tf.keras.utils.plot_model(\n",
    "    custom_cnn, \n",
    "    to_file=str(RESULTS_DIR / \"custom_cnn_architecture.png\"),\n",
    "    show_shapes=True, \n",
    "    show_layer_names=True,\n",
    "    rankdir='TB'\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Diagramme d'architecture sauvegard√©: {RESULTS_DIR / 'custom_cnn_architecture.png'}\")\n",
    "\n",
    "# Configuration de l'optimiseur et compilation\n",
    "optimizer = optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "custom_cnn.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', 'precision', 'recall']\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Mod√®le compil√© avec succ√®s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9422427d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 7.2 CALLBACKS ET MONITORING\n",
    "# ===================================\n",
    "\n",
    "# Configuration des callbacks\n",
    "def setup_callbacks(model_name, patience=10):\n",
    "    \"\"\"Configure les callbacks pour l'entra√Ænement\"\"\"\n",
    "    \n",
    "    # R√©pertoire pour sauvegarder les mod√®les\n",
    "    model_save_dir = MODELS_DIR / model_name\n",
    "    model_save_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    callbacks_list = [\n",
    "        # Early Stopping\n",
    "        EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=patience,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # R√©duction du learning rate\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # Sauvegarde du meilleur mod√®le\n",
    "        ModelCheckpoint(\n",
    "            filepath=str(model_save_dir / \"best_model.h5\"),\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            save_weights_only=False,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    return callbacks_list\n",
    "\n",
    "# Configuration des callbacks pour le mod√®le custom\n",
    "custom_callbacks = setup_callbacks(\"custom_cnn\", patience=10)\n",
    "\n",
    "print(\"‚úÖ Callbacks configur√©s:\")\n",
    "for callback in custom_callbacks:\n",
    "    print(f\"   üìã {callback.__class__.__name__}\")\n",
    "\n",
    "# Pr√©paration des g√©n√©rateurs de donn√©es\n",
    "print(\"\\nüîÑ Pr√©paration des g√©n√©rateurs de donn√©es...\")\n",
    "\n",
    "# G√©n√©rateur d'entra√Ænement avec augmentation\n",
    "train_generator = train_datagen.flow(\n",
    "    X_train_processed,\n",
    "    y_train_categorical,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# G√©n√©rateur de validation\n",
    "val_generator = val_datagen.flow(\n",
    "    X_val_processed,\n",
    "    y_val_categorical,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ G√©n√©rateurs pr√©par√©s:\")\n",
    "print(f\"   üìä Train: {len(train_generator)} batches de {BATCH_SIZE}\")\n",
    "print(f\"   üìä Validation: {len(val_generator)} batches de {BATCH_SIZE}\")\n",
    "\n",
    "# Fonction d'entra√Ænement\n",
    "def train_deep_model(model, model_name, train_gen, val_gen, epochs=EPOCHS, callbacks=None):\n",
    "    \"\"\"Entra√Æne un mod√®le de deep learning avec monitoring\"\"\"\n",
    "    \n",
    "    print(f\"\\nüöÄ Entra√Ænement de {model_name}...\")\n",
    "    print(f\"   üìä √âpoques: {epochs}\")\n",
    "    print(f\"   üìä Batch size: {BATCH_SIZE}\")\n",
    "    \n",
    "    # Entra√Ænement\n",
    "    history = model.fit(\n",
    "        train_gen,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_gen,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Entra√Ænement de {model_name} termin√©!\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "print(\"üîß Environnement d'entra√Ænement Deep Learning configur√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94e73df",
   "metadata": {},
   "source": [
    "## üîÑ Section 8: Transfer Learning Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6fffbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 8.1 MOD√àLES PR√â-ENTRA√éN√âS AVEC TRANSFER LEARNING\n",
    "# ===================================\n",
    "\n",
    "def create_transfer_learning_model(base_model_name, input_shape=(224, 224, 3), num_classes=4, \n",
    "                                 freeze_base=True, trainable_layers=0):\n",
    "    \"\"\"\n",
    "    Cr√©e un mod√®le de transfer learning\n",
    "    \n",
    "    Args:\n",
    "        base_model_name: 'VGG16', 'ResNet50', 'EfficientNetB0', ou 'InceptionV3'\n",
    "        freeze_base: Si True, g√®le les couches du mod√®le de base\n",
    "        trainable_layers: Nombre de couches √† rendre entra√Ænables (depuis la fin)\n",
    "    \"\"\"\n",
    "    \n",
    "    # S√©lection du mod√®le de base\n",
    "    if base_model_name == 'VGG16':\n",
    "        base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    elif base_model_name == 'ResNet50':\n",
    "        base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    elif base_model_name == 'EfficientNetB0':\n",
    "        base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    elif base_model_name == 'InceptionV3':\n",
    "        base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    else:\n",
    "        raise ValueError(f\"Mod√®le {base_model_name} non support√©\")\n",
    "    \n",
    "    # Gel des couches\n",
    "    if freeze_base:\n",
    "        base_model.trainable = False\n",
    "    else:\n",
    "        # Rendre seulement les derni√®res couches entra√Ænables\n",
    "        if trainable_layers > 0:\n",
    "            base_model.trainable = True\n",
    "            for layer in base_model.layers[:-trainable_layers]:\n",
    "                layer.trainable = False\n",
    "    \n",
    "    # Construction du mod√®le complet\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Dictionnaire pour stocker les mod√®les de transfer learning\n",
    "transfer_models = {}\n",
    "\n",
    "print(\"üîÑ Cr√©ation des mod√®les de Transfer Learning...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71751b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 8.2 ENTRA√éNEMENT DES MOD√àLES DE TRANSFER LEARNING\n",
    "# ===================================\n",
    "\n",
    "# Configuration des mod√®les √† tester\n",
    "models_config = [\n",
    "    {'name': 'VGG16_frozen', 'base': 'VGG16', 'freeze': True, 'trainable': 0},\n",
    "    {'name': 'ResNet50_frozen', 'base': 'ResNet50', 'freeze': True, 'trainable': 0},\n",
    "    {'name': 'EfficientNetB0_frozen', 'base': 'EfficientNetB0', 'freeze': True, 'trainable': 0}\n",
    "]\n",
    "\n",
    "# Stockage des historiques d'entra√Ænement\n",
    "training_histories = {}\n",
    "\n",
    "for config in models_config:\n",
    "    print(f\"\\nüîÑ Configuration: {config['name']}\")\n",
    "    \n",
    "    try:\n",
    "        # Cr√©ation du mod√®le\n",
    "        model = create_transfer_learning_model(\n",
    "            base_model_name=config['base'],\n",
    "            input_shape=(*IMG_SIZE, 3),\n",
    "            num_classes=len(CLASSES),\n",
    "            freeze_base=config['freeze'],\n",
    "            trainable_layers=config['trainable']\n",
    "        )\n",
    "        \n",
    "        # Compilation\n",
    "        model.compile(\n",
    "            optimizer=optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy', 'precision', 'recall']\n",
    "        )\n",
    "        \n",
    "        # Stockage\n",
    "        transfer_models[config['name']] = model\n",
    "        \n",
    "        print(f\"‚úÖ {config['name']} cr√©√© et compil√©\")\n",
    "        print(f\"   üìä Param√®tres entra√Ænables: {model.count_params():,}\")\n",
    "        \n",
    "        # Affichage du r√©sum√© pour le premier mod√®le\n",
    "        if config['name'] == 'VGG16_frozen':\n",
    "            print(f\"\\nüìã Exemple d'architecture - {config['name']}:\")\n",
    "            model.summary()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur lors de la cr√©ation de {config['name']}: {e}\")\n",
    "\n",
    "print(f\"\\n‚úÖ {len(transfer_models)} mod√®les de transfer learning cr√©√©s\")\n",
    "\n",
    "# Entra√Ænement d'un mod√®le de d√©monstration (VGG16)\n",
    "if 'VGG16_frozen' in transfer_models:\n",
    "    print(\"\\nüöÄ Entra√Ænement de d√©monstration - VGG16 (epochs r√©duits)...\")\n",
    "    \n",
    "    # Callbacks pour la d√©mo\n",
    "    demo_callbacks = setup_callbacks(\"VGG16_frozen_demo\", patience=5)\n",
    "    \n",
    "    # Entra√Ænement avec moins d'√©poques pour la d√©mo\n",
    "    demo_history = train_deep_model(\n",
    "        transfer_models['VGG16_frozen'],\n",
    "        \"VGG16_frozen\",\n",
    "        train_generator,\n",
    "        val_generator,\n",
    "        epochs=10,  # R√©duit pour la d√©mo\n",
    "        callbacks=demo_callbacks\n",
    "    )\n",
    "    \n",
    "    training_histories['VGG16_frozen'] = demo_history\n",
    "    \n",
    "    print(\"‚úÖ Entra√Ænement de d√©monstration termin√©\")\n",
    "\n",
    "print(\"\\nüí° Note: Pour un entra√Ænement complet, ajustez le nombre d'√©poques selon vos ressources\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c897ed1",
   "metadata": {},
   "source": [
    "## ‚ö° Section 9: Fine-Tuning Pre-trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b71a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 9.1 STRAT√âGIES DE FINE-TUNING\n",
    "# ===================================\n",
    "\n",
    "def fine_tune_model(base_model_name, pretrained_model_path=None, \n",
    "                   unfreeze_layers=20, fine_tune_lr=1e-5):\n",
    "    \"\"\"\n",
    "    Impl√©mente le fine-tuning d'un mod√®le pr√©-entra√Æn√©\n",
    "    \n",
    "    Args:\n",
    "        base_model_name: Nom du mod√®le de base\n",
    "        pretrained_model_path: Chemin vers le mod√®le pr√©-entra√Æn√© (optionnel)\n",
    "        unfreeze_layers: Nombre de couches √† d√©geler pour le fine-tuning\n",
    "        fine_tune_lr: Learning rate r√©duit pour le fine-tuning\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"‚ö° Fine-tuning de {base_model_name}\")\n",
    "    \n",
    "    # Si un mod√®le pr√©-entra√Æn√© existe, le charger\n",
    "    if pretrained_model_path and os.path.exists(pretrained_model_path):\n",
    "        print(f\"üìÇ Chargement du mod√®le pr√©-entra√Æn√©: {pretrained_model_path}\")\n",
    "        model = keras.models.load_model(pretrained_model_path)\n",
    "    else:\n",
    "        # Cr√©er un nouveau mod√®le si pas de mod√®le pr√©-entra√Æn√©\n",
    "        print(\"üîß Cr√©ation d'un nouveau mod√®le pour le fine-tuning\")\n",
    "        model = create_transfer_learning_model(\n",
    "            base_model_name=base_model_name,\n",
    "            input_shape=(*IMG_SIZE, 3),\n",
    "            num_classes=len(CLASSES),\n",
    "            freeze_base=False,\n",
    "            trainable_layers=unfreeze_layers\n",
    "        )\n",
    "    \n",
    "    # Phase 1: D√©gel progressif des couches\n",
    "    print(f\"üîì D√©gel des {unfreeze_layers} derni√®res couches du mod√®le de base\")\n",
    "    \n",
    "    # Identification du mod√®le de base (premi√®re couche)\n",
    "    base_model = model.layers[0]\n",
    "    \n",
    "    # Gel de toutes les couches d'abord\n",
    "    base_model.trainable = True\n",
    "    \n",
    "    # Gel des premi√®res couches, d√©gel des derni√®res\n",
    "    total_layers = len(base_model.layers)\n",
    "    freeze_until = total_layers - unfreeze_layers\n",
    "    \n",
    "    for i, layer in enumerate(base_model.layers):\n",
    "        if i < freeze_until:\n",
    "            layer.trainable = False\n",
    "        else:\n",
    "            layer.trainable = True\n",
    "    \n",
    "    print(f\"   üìä Couches totales: {total_layers}\")\n",
    "    print(f\"   üîí Couches gel√©es: {freeze_until}\")\n",
    "    print(f\"   üîì Couches entra√Ænables: {unfreeze_layers}\")\n",
    "    \n",
    "    # Recompilation avec learning rate r√©duit\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=fine_tune_lr),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy', 'precision', 'recall']\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Mod√®le recompil√© avec learning rate: {fine_tune_lr}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# D√©monstration du fine-tuning sur VGG16\n",
    "print(\"‚ö° D√©monstration de Fine-Tuning - VGG16\")\n",
    "\n",
    "# Strat√©gies de fine-tuning √† tester\n",
    "fine_tuning_strategies = [\n",
    "    {\n",
    "        'name': 'VGG16_fine_tuned_conservative',\n",
    "        'base': 'VGG16',\n",
    "        'unfreeze_layers': 10,\n",
    "        'lr': 1e-5,\n",
    "        'description': 'Fine-tuning conservateur - 10 derni√®res couches'\n",
    "    },\n",
    "    {\n",
    "        'name': 'VGG16_fine_tuned_aggressive', \n",
    "        'base': 'VGG16',\n",
    "        'unfreeze_layers': 20,\n",
    "        'lr': 5e-6,\n",
    "        'description': 'Fine-tuning agressif - 20 derni√®res couches'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Stockage des mod√®les fine-tun√©s\n",
    "fine_tuned_models = {}\n",
    "\n",
    "for strategy in fine_tuning_strategies:\n",
    "    print(f\"\\nüîÑ {strategy['description']}\")\n",
    "    \n",
    "    try:\n",
    "        # Cr√©ation du mod√®le fine-tun√©\n",
    "        ft_model = fine_tune_model(\n",
    "            base_model_name=strategy['base'],\n",
    "            unfreeze_layers=strategy['unfreeze_layers'],\n",
    "            fine_tune_lr=strategy['lr']\n",
    "        )\n",
    "        \n",
    "        fine_tuned_models[strategy['name']] = ft_model\n",
    "        \n",
    "        # Affichage des informations sur l'entra√Ænabilit√©\n",
    "        trainable_count = sum([1 for layer in ft_model.layers for sublayer in layer.layers if hasattr(sublayer, 'trainable') and sublayer.trainable])\n",
    "        total_count = sum([1 for layer in ft_model.layers for sublayer in layer.layers if hasattr(sublayer, 'trainable')])\n",
    "        \n",
    "        print(f\"   üìä Couches entra√Ænables: {trainable_count}/{total_count}\")\n",
    "        print(f\"   üìä Param√®tres entra√Ænables: {ft_model.count_params():,}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur lors du fine-tuning de {strategy['name']}: {e}\")\n",
    "\n",
    "print(f\"\\n‚úÖ {len(fine_tuned_models)} mod√®les fine-tun√©s cr√©√©s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22841f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 9.2 LEARNING RATE SCHEDULING AVANC√â\n",
    "# ===================================\n",
    "\n",
    "def create_advanced_callbacks(model_name, strategy_type=\"fine_tuning\"):\n",
    "    \"\"\"\n",
    "    Cr√©e des callbacks avanc√©s pour le fine-tuning\n",
    "    \"\"\"\n",
    "    \n",
    "    model_save_dir = MODELS_DIR / f\"{model_name}_{strategy_type}\"\n",
    "    model_save_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Learning rate scheduler personnalis√©\n",
    "    def lr_schedule(epoch, lr):\n",
    "        \"\"\"Planification du learning rate\"\"\"\n",
    "        if epoch < 5:\n",
    "            return lr\n",
    "        elif epoch < 15:\n",
    "            return lr * 0.9\n",
    "        else:\n",
    "            return lr * 0.95\n",
    "    \n",
    "    callbacks_list = [\n",
    "        # Early Stopping plus patient pour le fine-tuning\n",
    "        EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=15,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # R√©duction automatique du learning rate\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.3,\n",
    "            patience=7,\n",
    "            min_lr=1e-8,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # Learning rate scheduler personnalis√©\n",
    "        keras.callbacks.LearningRateScheduler(lr_schedule, verbose=1),\n",
    "        \n",
    "        # Sauvegarde des meilleurs mod√®les\n",
    "        ModelCheckpoint(\n",
    "            filepath=str(model_save_dir / \"best_model.h5\"),\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            save_weights_only=False,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # Sauvegarde des checkpoints\n",
    "        ModelCheckpoint(\n",
    "            filepath=str(model_save_dir / \"checkpoint_epoch_{epoch:02d}.h5\"),\n",
    "            save_freq='epoch',\n",
    "            save_weights_only=True,\n",
    "            verbose=0\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    return callbacks_list\n",
    "\n",
    "# D√©monstration d'entra√Ænement avec fine-tuning (version r√©duite)\n",
    "if 'VGG16_fine_tuned_conservative' in fine_tuned_models:\n",
    "    print(\"üöÄ D√©monstration de Fine-Tuning avec callbacks avanc√©s...\")\n",
    "    \n",
    "    # Configuration des callbacks avanc√©s\n",
    "    ft_callbacks = create_advanced_callbacks(\"VGG16\", \"fine_tuning_demo\")\n",
    "    \n",
    "    # Entra√Ænement avec param√®tres ajust√©s pour le fine-tuning\n",
    "    print(\"üìä Configuration pour Fine-Tuning:\")\n",
    "    print(\"   - Learning rate r√©duit\")\n",
    "    print(\"   - Patience augment√©e\")\n",
    "    print(\"   - Callbacks avanc√©s\")\n",
    "    print(\"   - Epochs r√©duits pour d√©mo\")\n",
    "    \n",
    "    # Note: Dans un sc√©nario r√©el, vous entra√Æneriez ici le mod√®le\n",
    "    print(\"\\nüí° Note: Dans un entra√Ænement complet, vous ex√©cuteriez:\")\n",
    "    print(\"   ft_history = train_deep_model(model, 'VGG16_fine_tuned', train_gen, val_gen, epochs=30)\")\n",
    "    \n",
    "    print(\"‚úÖ Configuration de fine-tuning pr√©par√©e\")\n",
    "\n",
    "print(\"\\nüîß Pipeline de Fine-Tuning configur√© avec succ√®s!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4678b161",
   "metadata": {},
   "source": [
    "## ü§ù Section 10: Ensemble of Deep Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3c681b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 10.1 ENSEMBLE VOTING POUR DEEP LEARNING\n",
    "# ===================================\n",
    "\n",
    "class DeepLearningEnsemble:\n",
    "    \"\"\"\n",
    "    Classe pour cr√©er des ensembles de mod√®les de deep learning\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, models_dict, ensemble_method='soft_voting'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            models_dict: Dictionnaire {nom: mod√®le} des mod√®les √† ensembler\n",
    "            ensemble_method: 'soft_voting', 'hard_voting', ou 'stacking'\n",
    "        \"\"\"\n",
    "        self.models = models_dict\n",
    "        self.ensemble_method = ensemble_method\n",
    "        self.model_names = list(models_dict.keys())\n",
    "        \n",
    "    def predict_ensemble(self, X, return_individual=False):\n",
    "        \"\"\"\n",
    "        Fait des pr√©dictions avec l'ensemble\n",
    "        \"\"\"\n",
    "        individual_predictions = {}\n",
    "        all_predictions = []\n",
    "        \n",
    "        print(f\"üîÑ Pr√©diction avec {len(self.models)} mod√®les...\")\n",
    "        \n",
    "        # Pr√©dictions individuelles\n",
    "        for name, model in self.models.items():\n",
    "            try:\n",
    "                pred = model.predict(X, verbose=0)\n",
    "                individual_predictions[name] = pred\n",
    "                all_predictions.append(pred)\n",
    "                print(f\"   ‚úÖ {name}: {pred.shape}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Erreur avec {name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not all_predictions:\n",
    "            raise ValueError(\"Aucune pr√©diction r√©ussie\")\n",
    "        \n",
    "        # Ensemble des pr√©dictions\n",
    "        all_predictions = np.array(all_predictions)\n",
    "        \n",
    "        if self.ensemble_method == 'soft_voting':\n",
    "            # Moyenne des probabilit√©s\n",
    "            ensemble_pred = np.mean(all_predictions, axis=0)\n",
    "        elif self.ensemble_method == 'hard_voting':\n",
    "            # Vote majoritaire\n",
    "            individual_classes = [np.argmax(pred, axis=1) for pred in all_predictions]\n",
    "            ensemble_classes = []\n",
    "            for i in range(len(X)):\n",
    "                votes = [pred[i] for pred in individual_classes]\n",
    "                ensemble_classes.append(max(set(votes), key=votes.count))\n",
    "            \n",
    "            # Conversion en format one-hot\n",
    "            ensemble_pred = to_categorical(ensemble_classes, num_classes=len(CLASSES))\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"M√©thode d'ensemble {self.ensemble_method} non support√©e\")\n",
    "        \n",
    "        if return_individual:\n",
    "            return ensemble_pred, individual_predictions\n",
    "        return ensemble_pred\n",
    "    \n",
    "    def evaluate_ensemble(self, X, y_true, return_individual=False):\n",
    "        \"\"\"\n",
    "        √âvalue les performances de l'ensemble\n",
    "        \"\"\"\n",
    "        print(f\"üìä √âvaluation de l'ensemble ({self.ensemble_method})...\")\n",
    "        \n",
    "        # Pr√©dictions\n",
    "        if return_individual:\n",
    "            ensemble_pred, individual_preds = self.predict_ensemble(X, return_individual=True)\n",
    "        else:\n",
    "            ensemble_pred = self.predict_ensemble(X)\n",
    "            individual_preds = None\n",
    "        \n",
    "        # Conversion en classes pour les m√©triques\n",
    "        y_true_classes = np.argmax(y_true, axis=1)\n",
    "        ensemble_classes = np.argmax(ensemble_pred, axis=1)\n",
    "        \n",
    "        # M√©triques de l'ensemble\n",
    "        accuracy = accuracy_score(y_true_classes, ensemble_classes)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            y_true_classes, ensemble_classes, average='weighted'\n",
    "        )\n",
    "        \n",
    "        results = {\n",
    "            'ensemble_accuracy': accuracy,\n",
    "            'ensemble_precision': precision,\n",
    "            'ensemble_recall': recall,\n",
    "            'ensemble_f1': f1,\n",
    "            'ensemble_predictions': ensemble_pred\n",
    "        }\n",
    "        \n",
    "        # √âvaluation individuelle si demand√©e\n",
    "        if return_individual and individual_preds:\n",
    "            individual_results = {}\n",
    "            for name, pred in individual_preds.items():\n",
    "                pred_classes = np.argmax(pred, axis=1)\n",
    "                ind_acc = accuracy_score(y_true_classes, pred_classes)\n",
    "                individual_results[name] = {\n",
    "                    'accuracy': ind_acc,\n",
    "                    'predictions': pred\n",
    "                }\n",
    "            results['individual_results'] = individual_results\n",
    "        \n",
    "        return results\n",
    "\n",
    "# D√©monstration d'ensemble avec les mod√®les disponibles\n",
    "print(\"ü§ù Cr√©ation d'ensembles de mod√®les Deep Learning...\")\n",
    "\n",
    "# V√©rification des mod√®les disponibles pour l'ensemble\n",
    "available_models = {}\n",
    "\n",
    "# Ajout des mod√®les de transfer learning si disponibles\n",
    "for model_name, model in transfer_models.items():\n",
    "    available_models[model_name] = model\n",
    "\n",
    "# Ajout des mod√®les fine-tun√©s si disponibles  \n",
    "for model_name, model in fine_tuned_models.items():\n",
    "    available_models[model_name] = model\n",
    "\n",
    "print(f\"üìä Mod√®les disponibles pour l'ensemble: {list(available_models.keys())}\")\n",
    "\n",
    "if len(available_models) >= 2:\n",
    "    # Cr√©ation d'ensembles avec diff√©rentes m√©thodes\n",
    "    ensemble_configs = [\n",
    "        {'method': 'soft_voting', 'description': 'Moyenne des probabilit√©s'},\n",
    "        {'method': 'hard_voting', 'description': 'Vote majoritaire'}\n",
    "    ]\n",
    "    \n",
    "    ensembles = {}\n",
    "    \n",
    "    for config in ensemble_configs:\n",
    "        ensemble_name = f\"DL_Ensemble_{config['method']}\"\n",
    "        ensemble = DeepLearningEnsemble(\n",
    "            models_dict=available_models,\n",
    "            ensemble_method=config['method']\n",
    "        )\n",
    "        ensembles[ensemble_name] = ensemble\n",
    "        \n",
    "        print(f\"‚úÖ {ensemble_name} cr√©√©: {config['description']}\")\n",
    "    \n",
    "    print(f\"\\nüéØ {len(ensembles)} ensembles de Deep Learning configur√©s\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Pas assez de mod√®les entra√Æn√©s pour cr√©er un ensemble\")\n",
    "    print(\"   Entra√Ænez d'abord plusieurs mod√®les de transfer learning\")\n",
    "\n",
    "print(\"\\nüí° Note: L'√©valuation compl√®te des ensembles n√©cessite des mod√®les enti√®rement entra√Æn√©s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf8f766",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Section 11: Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b43bf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 11.1 OPTIMISATION DES HYPERPARAM√àTRES - M√âTHODES TRADITIONNELLES\n",
    "# ===================================\n",
    "\n",
    "print(\"‚öôÔ∏è Optimisation des hyperparam√®tres pour les m√©thodes d'ensemble...\")\n",
    "\n",
    "# Configuration des espaces de recherche\n",
    "param_grids = {\n",
    "    'RandomForest': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 15, 20, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['sqrt', 'log2', None]\n",
    "    },\n",
    "    \n",
    "    'XGBoost': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'min_child_weight': [1, 3, 5],\n",
    "        'subsample': [0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "    } if 'xgb' in globals() else {},\n",
    "    \n",
    "    'GradientBoosting': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'subsample': [0.8, 0.9, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "def optimize_hyperparameters(model_class, param_grid, X_train, y_train, \n",
    "                           cv=3, scoring='accuracy', n_jobs=-1, verbose=True):\n",
    "    \"\"\"\n",
    "    Optimise les hyperparam√®tres avec GridSearchCV\n",
    "    \"\"\"\n",
    "    if not param_grid:\n",
    "        print(\"‚ö†Ô∏è Grille de param√®tres vide - optimisation ignor√©e\")\n",
    "        return None, None\n",
    "    \n",
    "    print(f\"üîç Optimisation avec {len(param_grid)} param√®tres...\")\n",
    "    print(f\"   üìä Cross-validation: {cv} folds\")\n",
    "    print(f\"   üìä M√©trique: {scoring}\")\n",
    "    \n",
    "    # Configuration de la recherche\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model_class,\n",
    "        param_grid=param_grid,\n",
    "        cv=cv,\n",
    "        scoring=scoring,\n",
    "        n_jobs=n_jobs,\n",
    "        return_train_score=True,\n",
    "        verbose=1 if verbose else 0\n",
    "    )\n",
    "    \n",
    "    # Ex√©cution de la recherche\n",
    "    start_time = datetime.now()\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    optimization_time = datetime.now() - start_time\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"‚úÖ Optimisation termin√©e en {optimization_time.total_seconds():.2f}s\")\n",
    "        print(f\"üéØ Meilleur score: {grid_search.best_score_:.4f}\")\n",
    "        print(f\"‚öôÔ∏è Meilleurs param√®tres: {grid_search.best_params_}\")\n",
    "    \n",
    "    return grid_search.best_estimator_, grid_search.cv_results_\n",
    "\n",
    "# Stockage des r√©sultats d'optimisation\n",
    "optimization_results = {}\n",
    "\n",
    "# Optimisation pour Random Forest\n",
    "print(\"\\nüå≥ Optimisation Random Forest...\")\n",
    "if 'RandomForest' in param_grids:\n",
    "    # Grille r√©duite pour la d√©mo\n",
    "    rf_demo_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [10, 15],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'max_features': ['sqrt', None]\n",
    "    }\n",
    "    \n",
    "    best_rf, rf_cv_results = optimize_hyperparameters(\n",
    "        RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "        rf_demo_grid,\n",
    "        X_train_features_scaled,\n",
    "        y_train,\n",
    "        cv=3,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    if best_rf:\n",
    "        optimization_results['RandomForest'] = {\n",
    "            'best_model': best_rf,\n",
    "            'cv_results': rf_cv_results\n",
    "        }\n",
    "\n",
    "# Optimisation pour Gradient Boosting\n",
    "print(\"\\nüöÄ Optimisation Gradient Boosting...\")\n",
    "gb_demo_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'learning_rate': [0.1, 0.2],\n",
    "    'max_depth': [3, 5]\n",
    "}\n",
    "\n",
    "best_gb, gb_cv_results = optimize_hyperparameters(\n",
    "    GradientBoostingClassifier(random_state=42),\n",
    "    gb_demo_grid,\n",
    "    X_train_features_scaled,\n",
    "    y_train,\n",
    "    cv=3,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "if best_gb:\n",
    "    optimization_results['GradientBoosting'] = {\n",
    "        'best_model': best_gb,\n",
    "        'cv_results': gb_cv_results\n",
    "    }\n",
    "\n",
    "print(f\"\\n‚úÖ {len(optimization_results)} mod√®les optimis√©s\")\n",
    "\n",
    "# √âvaluation des mod√®les optimis√©s\n",
    "print(\"\\nüìä √âvaluation des mod√®les optimis√©s...\")\n",
    "for model_name, results in optimization_results.items():\n",
    "    model = results['best_model']\n",
    "    \n",
    "    # √âvaluation sur validation\n",
    "    val_pred = model.predict(X_val_features_scaled)\n",
    "    val_accuracy = accuracy_score(y_val, val_pred)\n",
    "    \n",
    "    print(f\"üéØ {model_name} optimis√©:\")\n",
    "    print(f\"   üìä Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    # Comparaison avec le mod√®le baseline\n",
    "    if model_name in baseline_results:\n",
    "        baseline_acc = baseline_results[model_name]['val_accuracy']\n",
    "        improvement = val_accuracy - baseline_acc\n",
    "        print(f\"   üìà Am√©lioration: {improvement:+.4f}\")\n",
    "    \n",
    "    # Stockage dans baseline_results pour comparaison\n",
    "    baseline_results[f\"{model_name}_optimized\"] = {\n",
    "        'model': model,\n",
    "        'val_accuracy': val_accuracy,\n",
    "        'y_val_pred': val_pred\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf697ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 11.2 RECHERCHE AL√âATOIRE ET ANALYSE DES HYPERPARAM√àTRES\n",
    "# ===================================\n",
    "\n",
    "def randomized_hyperparameter_search(model_class, param_distributions, \n",
    "                                    X_train, y_train, n_iter=20, cv=3):\n",
    "    \"\"\"\n",
    "    Recherche al√©atoire d'hyperparam√®tres avec RandomizedSearchCV\n",
    "    \"\"\"\n",
    "    print(f\"üé≤ Recherche al√©atoire avec {n_iter} it√©rations...\")\n",
    "    \n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=model_class,\n",
    "        param_distributions=param_distributions,\n",
    "        n_iter=n_iter,\n",
    "        cv=cv,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        return_train_score=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    random_search.fit(X_train, y_train)\n",
    "    search_time = datetime.now() - start_time\n",
    "    \n",
    "    print(f\"‚úÖ Recherche termin√©e en {search_time.total_seconds():.2f}s\")\n",
    "    print(f\"üéØ Meilleur score: {random_search.best_score_:.4f}\")\n",
    "    print(f\"‚öôÔ∏è Meilleurs param√®tres: {random_search.best_params_}\")\n",
    "    \n",
    "    return random_search.best_estimator_, random_search.cv_results_\n",
    "\n",
    "# Distributions pour la recherche al√©atoire\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "rf_param_distributions = {\n",
    "    'n_estimators': randint(50, 300),\n",
    "    'max_depth': [5, 10, 15, 20, None],\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 10),\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Recherche al√©atoire pour Random Forest\n",
    "print(\"\\nüé≤ Recherche al√©atoire - Random Forest...\")\n",
    "best_rf_random, rf_random_results = randomized_hyperparameter_search(\n",
    "    RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    rf_param_distributions,\n",
    "    X_train_features_scaled,\n",
    "    y_train,\n",
    "    n_iter=10,  # R√©duit pour la d√©mo\n",
    "    cv=3\n",
    ")\n",
    "\n",
    "# Analyse des r√©sultats de recherche\n",
    "def analyze_hyperparameter_results(cv_results, param_name, model_name):\n",
    "    \"\"\"\n",
    "    Analyse l'impact d'un hyperparam√®tre sur les performances\n",
    "    \"\"\"\n",
    "    if f'param_{param_name}' not in cv_results:\n",
    "        print(f\"‚ö†Ô∏è Param√®tre {param_name} non trouv√© dans les r√©sultats\")\n",
    "        return\n",
    "    \n",
    "    # Extraction des donn√©es\n",
    "    param_values = cv_results[f'param_{param_name}']\n",
    "    mean_scores = cv_results['mean_test_score']\n",
    "    \n",
    "    # Cr√©ation du DataFrame pour l'analyse\n",
    "    df_analysis = pd.DataFrame({\n",
    "        'param_value': param_values,\n",
    "        'mean_score': mean_scores\n",
    "    })\n",
    "    \n",
    "    # Gestion des valeurs None\n",
    "    df_analysis = df_analysis[df_analysis['param_value'].notna()]\n",
    "    \n",
    "    if len(df_analysis) == 0:\n",
    "        print(f\"‚ö†Ô∏è Pas de donn√©es valides pour {param_name}\")\n",
    "        return\n",
    "    \n",
    "    # Visualisation\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    if df_analysis['param_value'].dtype == 'object':\n",
    "        # Param√®tre cat√©goriel\n",
    "        df_grouped = df_analysis.groupby('param_value')['mean_score'].agg(['mean', 'std']).reset_index()\n",
    "        plt.bar(range(len(df_grouped)), df_grouped['mean'], \n",
    "                yerr=df_grouped['std'], alpha=0.7, capsize=5)\n",
    "        plt.xticks(range(len(df_grouped)), df_grouped['param_value'], rotation=45)\n",
    "    else:\n",
    "        # Param√®tre num√©rique\n",
    "        plt.scatter(df_analysis['param_value'], df_analysis['mean_score'], alpha=0.6)\n",
    "        plt.xlabel(param_name)\n",
    "    \n",
    "    plt.title(f'Impact de {param_name} sur les performances - {model_name}')\n",
    "    plt.ylabel('Score de validation')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Analyse des r√©sultats de Random Forest\n",
    "if rf_random_results:\n",
    "    print(\"\\nüìä Analyse des hyperparam√®tres - Random Forest...\")\n",
    "    \n",
    "    # Analyse de diff√©rents param√®tres\n",
    "    important_params = ['n_estimators', 'max_depth', 'min_samples_split']\n",
    "    \n",
    "    for param in important_params:\n",
    "        analyze_hyperparameter_results(rf_random_results, param, 'Random Forest')\n",
    "\n",
    "# Comparaison des m√©thodes d'optimisation\n",
    "print(\"\\nüìà Comparaison des m√©thodes d'optimisation:\")\n",
    "\n",
    "comparison_data = []\n",
    "\n",
    "# Baseline\n",
    "for model_name in ['Random Forest', 'Gradient Boosting']:\n",
    "    if model_name in baseline_results:\n",
    "        comparison_data.append({\n",
    "            'Model': model_name,\n",
    "            'Method': 'Baseline',\n",
    "            'Accuracy': baseline_results[model_name]['val_accuracy']\n",
    "        })\n",
    "\n",
    "# Optimis√©s\n",
    "for model_name in ['RandomForest', 'GradientBoosting']:\n",
    "    if f\"{model_name}_optimized\" in baseline_results:\n",
    "        comparison_data.append({\n",
    "            'Model': model_name,\n",
    "            'Method': 'Grid Search',\n",
    "            'Accuracy': baseline_results[f\"{model_name}_optimized\"]['val_accuracy']\n",
    "        })\n",
    "\n",
    "# Random Search (Random Forest seulement)\n",
    "if best_rf_random:\n",
    "    rf_random_acc = accuracy_score(y_val, best_rf_random.predict(X_val_features_scaled))\n",
    "    comparison_data.append({\n",
    "        'Model': 'RandomForest',\n",
    "        'Method': 'Random Search',\n",
    "        'Accuracy': rf_random_acc\n",
    "    })\n",
    "\n",
    "if comparison_data:\n",
    "    df_optimization_comparison = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    # Visualisation\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=df_optimization_comparison, x='Model', y='Accuracy', hue='Method')\n",
    "    plt.title('Comparaison des M√©thodes d\\'Optimisation')\n",
    "    plt.ylabel('Validation Accuracy')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üìä R√©sultats de comparaison:\")\n",
    "    print(df_optimization_comparison)\n",
    "\n",
    "print(\"\\n‚úÖ Optimisation des hyperparam√®tres termin√©e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b28b1d",
   "metadata": {},
   "source": [
    "## üìä Section 12: Model Evaluation and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ef334d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 12.1 M√âTRIQUES AVANC√âES ET MATRICES DE CONFUSION\n",
    "# ===================================\n",
    "\n",
    "def comprehensive_model_evaluation(y_true, y_pred, model_name, class_names):\n",
    "    \"\"\"\n",
    "    √âvaluation compl√®te d'un mod√®le avec m√©triques avanc√©es\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìä √âvaluation compl√®te - {model_name}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # M√©triques globales\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\"üéØ Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"üéØ Precision (weighted): {precision:.4f}\")\n",
    "    print(f\"üéØ Recall (weighted): {recall:.4f}\")\n",
    "    print(f\"üéØ F1-Score (weighted): {f1:.4f}\")\n",
    "    \n",
    "    # M√©triques par classe\n",
    "    print(f\"\\nüìã Rapport de classification:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "    \n",
    "    # Matrice de confusion\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Visualisation de la matrice de confusion\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(f'Matrice de Confusion - {model_name}')\n",
    "    plt.xlabel('Pr√©dictions')\n",
    "    plt.ylabel('Vraies √©tiquettes')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # M√©triques par classe d√©taill√©es\n",
    "    class_metrics = {}\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        class_precision = precision_recall_fscore_support(y_true, y_pred, labels=[i], average=None)[0][0]\n",
    "        class_recall = precision_recall_fscore_support(y_true, y_pred, labels=[i], average=None)[1][0]\n",
    "        class_f1 = precision_recall_fscore_support(y_true, y_pred, labels=[i], average=None)[2][0]\n",
    "        \n",
    "        class_metrics[class_name] = {\n",
    "            'precision': class_precision,\n",
    "            'recall': class_recall,\n",
    "            'f1_score': class_f1,\n",
    "            'support': support[i]\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'confusion_matrix': cm,\n",
    "        'class_metrics': class_metrics\n",
    "    }\n",
    "\n",
    "# √âvaluation de tous les mod√®les disponibles\n",
    "print(\"üìä √âvaluation compl√®te de tous les mod√®les...\")\n",
    "\n",
    "evaluation_results = {}\n",
    "\n",
    "# √âvaluation des mod√®les baseline\n",
    "for model_name, results in baseline_results.items():\n",
    "    if 'y_val_pred' in results:\n",
    "        eval_results = comprehensive_model_evaluation(\n",
    "            y_val, \n",
    "            results['y_val_pred'], \n",
    "            model_name, \n",
    "            CLASSES\n",
    "        )\n",
    "        evaluation_results[model_name] = eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c152e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 12.2 COMPARAISON GLOBALE DES MOD√àLES\n",
    "# ===================================\n",
    "\n",
    "def create_models_comparison_dashboard():\n",
    "    \"\"\"\n",
    "    Cr√©e un dashboard de comparaison des mod√®les\n",
    "    \"\"\"\n",
    "    if not evaluation_results:\n",
    "        print(\"‚ö†Ô∏è Aucun r√©sultat d'√©valuation disponible\")\n",
    "        return\n",
    "    \n",
    "    # Compilation des m√©triques\n",
    "    comparison_data = []\n",
    "    for model_name, results in evaluation_results.items():\n",
    "        comparison_data.append({\n",
    "            'Model': model_name,\n",
    "            'Accuracy': results['accuracy'],\n",
    "            'Precision': results['precision'],\n",
    "            'Recall': results['recall'],\n",
    "            'F1_Score': results['f1_score'],\n",
    "            'Type': get_model_type(model_name)\n",
    "        })\n",
    "    \n",
    "    df_comparison = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    # Tri par accuracy\n",
    "    df_comparison = df_comparison.sort_values('Accuracy', ascending=False)\n",
    "    \n",
    "    print(\"üèÜ Classement des mod√®les par Accuracy:\")\n",
    "    print(df_comparison[['Model', 'Accuracy', 'F1_Score', 'Type']].to_string(index=False))\n",
    "    \n",
    "    # Visualisations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Dashboard de Comparaison des Mod√®les', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Accuracy par mod√®le\n",
    "    df_sorted = df_comparison.sort_values('Accuracy', ascending=True)\n",
    "    bars = axes[0, 0].barh(range(len(df_sorted)), df_sorted['Accuracy'], \n",
    "                          color=plt.cm.viridis(np.linspace(0, 1, len(df_sorted))))\n",
    "    axes[0, 0].set_yticks(range(len(df_sorted)))\n",
    "    axes[0, 0].set_yticklabels(df_sorted['Model'], fontsize=8)\n",
    "    axes[0, 0].set_xlabel('Accuracy')\n",
    "    axes[0, 0].set_title('Accuracy par Mod√®le')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Ajout des valeurs sur les barres\n",
    "    for i, bar in enumerate(bars):\n",
    "        width = bar.get_width()\n",
    "        axes[0, 0].text(width + 0.001, bar.get_y() + bar.get_height()/2, \n",
    "                       f'{width:.3f}', ha='left', va='center', fontsize=8)\n",
    "    \n",
    "    # 2. F1-Score vs Accuracy\n",
    "    scatter = axes[0, 1].scatter(df_comparison['Accuracy'], df_comparison['F1_Score'], \n",
    "                                c=df_comparison.index, cmap='viridis', s=100, alpha=0.7)\n",
    "    axes[0, 1].set_xlabel('Accuracy')\n",
    "    axes[0, 1].set_ylabel('F1-Score')\n",
    "    axes[0, 1].set_title('F1-Score vs Accuracy')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Ligne diagonale de r√©f√©rence\n",
    "    min_val = min(df_comparison['Accuracy'].min(), df_comparison['F1_Score'].min())\n",
    "    max_val = max(df_comparison['Accuracy'].max(), df_comparison['F1_Score'].max())\n",
    "    axes[0, 1].plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.5)\n",
    "    \n",
    "    # 3. M√©triques par type de mod√®le\n",
    "    df_melted = df_comparison.melt(id_vars=['Model', 'Type'], \n",
    "                                  value_vars=['Accuracy', 'Precision', 'Recall', 'F1_Score'],\n",
    "                                  var_name='Metric', value_name='Score')\n",
    "    \n",
    "    sns.boxplot(data=df_melted, x='Type', y='Score', hue='Metric', ax=axes[1, 0])\n",
    "    axes[1, 0].set_title('Distribution des M√©triques par Type')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    axes[1, 0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # 4. Heatmap des corr√©lations entre m√©triques\n",
    "    metrics_corr = df_comparison[['Accuracy', 'Precision', 'Recall', 'F1_Score']].corr()\n",
    "    sns.heatmap(metrics_corr, annot=True, cmap='coolwarm', center=0, \n",
    "                square=True, ax=axes[1, 1])\n",
    "    axes[1, 1].set_title('Corr√©lations entre M√©triques')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return df_comparison\n",
    "\n",
    "# Cr√©ation du dashboard\n",
    "print(\"üìä Cr√©ation du dashboard de comparaison...\")\n",
    "comparison_df = create_models_comparison_dashboard()\n",
    "\n",
    "# Analyse des meilleures performances par cat√©gorie\n",
    "if comparison_df is not None and not comparison_df.empty:\n",
    "    print(\"\\nüèÜ Meilleurs mod√®les par cat√©gorie:\")\n",
    "    \n",
    "    categories = ['Baseline', 'Bagging', 'Boosting']\n",
    "    for category in categories:\n",
    "        cat_models = comparison_df[comparison_df['Type'] == category]\n",
    "        if not cat_models.empty:\n",
    "            best_model = cat_models.loc[cat_models['Accuracy'].idxmax()]\n",
    "            print(f\"{category}: {best_model['Model']} (Accuracy: {best_model['Accuracy']:.4f})\")\n",
    "    \n",
    "    # Mod√®le global champion\n",
    "    best_overall = comparison_df.loc[comparison_df['Accuracy'].idxmax()]\n",
    "    print(f\"\\nüëë Champion global: {best_overall['Model']}\")\n",
    "    print(f\"   üìä Accuracy: {best_overall['Accuracy']:.4f}\")\n",
    "    print(f\"   üìä F1-Score: {best_overall['F1_Score']:.4f}\")\n",
    "    print(f\"   üìä Type: {best_overall['Type']}\")\n",
    "\n",
    "print(\"\\n‚úÖ √âvaluation comparative termin√©e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19053bfd",
   "metadata": {},
   "source": [
    "## üé® Section 13: Prediction Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91181e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 13.1 VISUALISATION DES PR√âDICTIONS ET INTERPR√âTABILIT√â\n",
    "# ===================================\n",
    "\n",
    "def visualize_predictions_with_confidence(model, X_samples, y_true, sample_indices, \n",
    "                                        model_name, class_names, is_deep_learning=False):\n",
    "    \"\"\"\n",
    "    Visualise les pr√©dictions avec scores de confiance\n",
    "    \"\"\"\n",
    "    n_samples = len(sample_indices)\n",
    "    fig, axes = plt.subplots(2, n_samples//2, figsize=(16, 8))\n",
    "    fig.suptitle(f'Pr√©dictions et Confiance - {model_name}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    if is_deep_learning:\n",
    "        # Pour les mod√®les de deep learning\n",
    "        predictions_proba = model.predict(X_samples, verbose=0)\n",
    "        predictions = np.argmax(predictions_proba, axis=1)\n",
    "    else:\n",
    "        # Pour les mod√®les traditionnels avec features\n",
    "        X_features = extract_traditional_features(X_samples)\n",
    "        X_features_scaled = scaler.transform(X_features)\n",
    "        predictions_proba = model.predict_proba(X_features_scaled)\n",
    "        predictions = model.predict(X_features_scaled)\n",
    "    \n",
    "    for idx, sample_idx in enumerate(sample_indices):\n",
    "        row = idx // (n_samples//2)\n",
    "        col = idx % (n_samples//2)\n",
    "        \n",
    "        # Image\n",
    "        axes[row, col].imshow(X_samples[sample_idx])\n",
    "        \n",
    "        # Informations sur la pr√©diction\n",
    "        true_label = class_names[y_true[sample_idx]]\n",
    "        pred_label = class_names[predictions[sample_idx]]\n",
    "        confidence = np.max(predictions_proba[sample_idx])\n",
    "        \n",
    "        # Couleur selon la justesse de la pr√©diction\n",
    "        color = 'green' if true_label == pred_label else 'red'\n",
    "        \n",
    "        title = f'Vraie: {true_label}\\nPr√©d: {pred_label}\\nConf: {confidence:.3f}'\n",
    "        axes[row, col].set_title(title, color=color, fontsize=10)\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Graphique des scores de confiance par classe\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "    \n",
    "    for i, sample_idx in enumerate(sample_indices):\n",
    "        proba_scores = predictions_proba[sample_idx]\n",
    "        x_pos = np.arange(len(class_names)) + i * 0.1\n",
    "        \n",
    "        bars = ax.bar(x_pos, proba_scores, width=0.1, alpha=0.7, \n",
    "                     label=f'√âchantillon {sample_idx+1}')\n",
    "        \n",
    "        # Highlight de la classe pr√©dite\n",
    "        max_idx = np.argmax(proba_scores)\n",
    "        bars[max_idx].set_color('red')\n",
    "        bars[max_idx].set_alpha(1.0)\n",
    "    \n",
    "    ax.set_xlabel('Classes')\n",
    "    ax.set_ylabel('Score de Confiance')\n",
    "    ax.set_title(f'Scores de Confiance par Classe - {model_name}')\n",
    "    ax.set_xticks(np.arange(len(class_names)) + 0.2)\n",
    "    ax.set_xticklabels(class_names, rotation=45)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# D√©monstration avec le meilleur mod√®le traditionnel\n",
    "if baseline_results and X_val_processed is not None:\n",
    "    print(\"üé® Visualisation des pr√©dictions...\")\n",
    "    \n",
    "    # S√©lection d'√©chantillons pour la visualisation\n",
    "    n_viz_samples = 6\n",
    "    sample_indices = np.random.choice(len(X_val_processed), n_viz_samples, replace=False)\n",
    "    \n",
    "    # R√©cup√©ration du meilleur mod√®le traditionnel\n",
    "    best_traditional_model = None\n",
    "    best_traditional_name = \"\"\n",
    "    best_score = 0\n",
    "    \n",
    "    for model_name, results in baseline_results.items():\n",
    "        if 'val_accuracy' in results and results['val_accuracy'] > best_score:\n",
    "            best_score = results['val_accuracy']\n",
    "            best_traditional_model = results['model']\n",
    "            best_traditional_name = model_name\n",
    "    \n",
    "    if best_traditional_model:\n",
    "        print(f\"üìä Visualisation avec le meilleur mod√®le: {best_traditional_name}\")\n",
    "        \n",
    "        # √âchantillons pour la visualisation\n",
    "        X_viz = X_val_processed[sample_indices]\n",
    "        y_viz = y_val[sample_indices]\n",
    "        \n",
    "        # Visualisation\n",
    "        visualize_predictions_with_confidence(\n",
    "            best_traditional_model,\n",
    "            X_viz,\n",
    "            y_viz,\n",
    "            range(len(sample_indices)),\n",
    "            best_traditional_name,\n",
    "            CLASSES,\n",
    "            is_deep_learning=False\n",
    "        )\n",
    "    \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Aucun mod√®le traditionnel disponible pour la visualisation\")\n",
    "\n",
    "print(\"‚úÖ Visualisations cr√©√©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14da6041",
   "metadata": {},
   "source": [
    "## üéØ Conclusion et R√©capitulatif\n",
    "\n",
    "### üìä **R√©sum√© du Notebook**\n",
    "\n",
    "Ce notebook complet a d√©montr√© les techniques avanc√©es de machine learning pour la classification d'images m√©dicales COVID-19 :\n",
    "\n",
    "#### üîß **Techniques Impl√©ment√©es :**\n",
    "\n",
    "1. **üì¶ M√©thodes d'Ensemble - Bagging :**\n",
    "   - Random Forest optimis√©\n",
    "   - Extra Trees (Extremely Randomized Trees)\n",
    "   - Bagging avec SVM et Logistic Regression\n",
    "   - Analyse d'importance des features\n",
    "\n",
    "2. **üöÄ M√©thodes d'Ensemble - Boosting :**\n",
    "   - AdaBoost avec arbres de d√©cision\n",
    "   - Gradient Boosting Machine (GBM)\n",
    "   - XGBoost (si disponible)\n",
    "   - Comparaison des performances\n",
    "\n",
    "3. **üß† Deep Learning :**\n",
    "   - CNN personnalis√© from scratch\n",
    "   - Transfer Learning (VGG16, ResNet50, EfficientNetB0)\n",
    "   - Fine-Tuning avec strat√©gies avanc√©es\n",
    "   - Ensemble de mod√®les de deep learning\n",
    "\n",
    "4. **‚öôÔ∏è Optimisation :**\n",
    "   - Grid Search pour hyperparam√®tres\n",
    "   - Random Search alternatif\n",
    "   - Cross-validation stratifi√©e\n",
    "   - Learning rate scheduling\n",
    "\n",
    "5. **üìä √âvaluation :**\n",
    "   - M√©triques compl√®tes (Accuracy, Precision, Recall, F1)\n",
    "   - Matrices de confusion d√©taill√©es\n",
    "   - Comparaisons visuelles\n",
    "   - Dashboard de performances\n",
    "\n",
    "#### üí° **Points Cl√©s Appris :**\n",
    "\n",
    "- **Bagging** : R√©duit la variance, bon pour overfitting\n",
    "- **Boosting** : R√©duit le biais, s√©quentiel et puissant\n",
    "- **Transfer Learning** : Efficace pour datasets m√©dicaux limit√©s\n",
    "- **Fine-Tuning** : Am√©liore les performances avec attention aux learning rates\n",
    "- **Ensemble Deep Learning** : Combine les forces de diff√©rents mod√®les\n",
    "\n",
    "#### üõ†Ô∏è **Recommandations pour la Production :**\n",
    "\n",
    "1. **Entra√Ænement complet** : Utilisez plus d'√©poques (50-100) pour les mod√®les DL\n",
    "2. **Validation externe** : Testez sur un dataset externe pour validation\n",
    "3. **Monitoring** : Impl√©mentez un suivi des performances en temps r√©el\n",
    "4. **Explicabilit√©** : Ajoutez des techniques comme LIME ou GRAD-CAM\n",
    "5. **D√©ploiement** : Consid√©rez l'optimisation des mod√®les (quantization, pruning)\n",
    "\n",
    "### üéØ **Prochaines √âtapes Sugg√©r√©es :**\n",
    "\n",
    "1. **Augmentation de donn√©es avanc√©e** : Techniques sp√©cifiques au m√©dical\n",
    "2. **Architecture personnalis√©e** : CNN adapt√© aux radiographies\n",
    "3. **M√©ta-learning** : Apprentissage sur diff√©rents types d'images m√©dicales\n",
    "4. **Federated Learning** : Entra√Ænement distribu√© pr√©servant la confidentialit√©\n",
    "5. **Validation clinique** : Collaboration avec professionnels de sant√©\n",
    "\n",
    "---\n",
    "\n",
    "**üèÜ F√©licitations ! Vous avez maintenant une base solide pour les techniques avanc√©es de ML dans le domaine m√©dical.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
