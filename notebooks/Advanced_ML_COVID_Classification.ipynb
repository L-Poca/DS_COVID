{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9504d98d",
   "metadata": {},
   "source": [
    "# üî¨ Advanced Machine Learning for COVID-19 Chest X-Ray Classification\n",
    "\n",
    "## üìã Comprehensive Guide: Ensemble Methods & Deep Learning Fine-Tuning\n",
    "\n",
    "**Auteurs**: √âquipe DS_COVID  \n",
    "**Date**: 15 octobre 2025  \n",
    "**Branche**: ReVamp\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ **Objectifs du notebook**\n",
    "\n",
    "Ce notebook pr√©sente une approche compl√®te du machine learning pour la classification de radiographies pulmonaires COVID-19, couvrant:\n",
    "\n",
    "1. **üå≥ M√©thodes d'Ensemble** : Bagging, Boosting, Stacking\n",
    "2. **üß† Deep Learning** : Transfer Learning & Fine-Tuning  \n",
    "3. **‚öôÔ∏è Optimisation** : Hyperparam√®tres, Cross-validation\n",
    "4. **üìä √âvaluation** : M√©triques avanc√©es, Visualisations\n",
    "\n",
    "### üìÅ **Dataset**\n",
    "- **Source**: COVID-19_Radiography_Dataset\n",
    "- **Classes**: COVID, Normal, Lung_Opacity, Viral Pneumonia\n",
    "- **Type**: Images radiographiques pulmonaires\n",
    "\n",
    "---\n",
    "\n",
    "### üóÇÔ∏è **Structure du notebook**\n",
    "\n",
    "| Section | Technique | Description |\n",
    "|---------|-----------|-------------|\n",
    "| 1-3 | **Setup & EDA** | Chargement, exploration, preprocessing |\n",
    "| 4-6 | **Ensemble Methods** | Bagging, Boosting, comparaisons |\n",
    "| 7-10 | **Deep Learning** | CNN, Transfer Learning, Fine-Tuning |\n",
    "| 11-13 | **Optimization & Evaluation** | Tuning, m√©triques, visualisations |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e79303f",
   "metadata": {},
   "source": [
    "## üì¶ Section 1: Setup and Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28eb542c",
   "metadata": {},
   "source": [
    "## üöÄ Version Google Colab\n",
    "\n",
    "**Pour utiliser ce notebook sur Google Colab, ex√©cutez cette cellule au lieu de la cellule de configuration normale :**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418efab9",
   "metadata": {},
   "source": [
    "### üìã Instructions pour Google Colab\n",
    "\n",
    "#### ‚úÖ Ce qui change sur Colab vs Local :\n",
    "\n",
    "1. **üìÇ Gestion des donn√©es** :\n",
    "   - Local : Dataset dans `data/raw/COVID-19_Radiography_Dataset/`\n",
    "   - Colab : **Extraction automatique** d'`archive_covid.zip` depuis Drive\n",
    "\n",
    "2. **üì¶ Installation** :\n",
    "   - Local : `pip install -e .` fait une seule fois\n",
    "   - Colab : Installation automatique dans la cellule\n",
    "\n",
    "3. **üîß Configuration** :\n",
    "   - Local : Chargement automatique `.env`\n",
    "   - Colab : Package ds-covid + extraction ZIP\n",
    "\n",
    "#### üöÄ Ce que fait automatiquement la cellule Colab :\n",
    "\n",
    "1. **üì• Clone le repo** : `git clone https://github.com/L-Poca/DS_COVID.git`\n",
    "2. **üì¶ Extrait archive_covid.zip** : Depuis `MyDrive/archive_covid.zip`\n",
    "3. **üîç Recherche intelligente** : Trouve automatiquement le dossier COVID\n",
    "4. ** Installe les d√©pendances** : `pip install -r requirements.txt`\n",
    "5. **üîß Installe le package** : `pip install -e .` \n",
    "6. **‚öôÔ∏è Configure les chemins** : `DATA_DIR` pointe vers les donn√©es extraites\n",
    "7. **‚úÖ V√©rifie les donn√©es** : Compte les images par classe\n",
    "\n",
    "#### üíæ Votre fichier sur Drive :\n",
    "\n",
    "```\n",
    "Drive/\n",
    "‚îú‚îÄ‚îÄ MyDrive/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ archive_covid.zip  ‚Üê Votre fichier ZIP\n",
    "```\n",
    "\n",
    "**Le script fait automatiquement** :\n",
    "1. **Extrait** `archive_covid.zip` vers `/content/temp_covid_extract/`\n",
    "2. **Recherche** le dossier COVID (plusieurs patterns support√©s)\n",
    "3. **D√©place** vers `/content/DS_COVID/data/raw/COVID-19_Radiography_Dataset/`\n",
    "4. **Nettoie** le dossier temporaire\n",
    "\n",
    "#### üîç Recherche intelligente :\n",
    "\n",
    "Le script cherche automatiquement :\n",
    "- `COVID-19_Radiography_Dataset/`\n",
    "- Dossiers contenant `*COVID*`\n",
    "- Dossiers contenant `*radiography*`\n",
    "- Dossiers contenant `*chest*`\n",
    "\n",
    "#### üí° Avantages :\n",
    "\n",
    "- ‚úÖ **Simple** : Juste d√©poser archive_covid.zip dans Drive\n",
    "- ‚úÖ **Automatique** : Extraction et organisation automatiques\n",
    "- ‚úÖ **Robuste** : Recherche intelligente de la structure\n",
    "- ‚úÖ **Rapide** : Pas besoin de cr√©er des dossiers manuellement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377bf6bb",
   "metadata": {},
   "source": [
    "### üöÄ Optimisations Colab Pro\n",
    "\n",
    "#### ‚ö° Param√®tres ambitieux pour GPU puissant :\n",
    "\n",
    "**üñºÔ∏è Images :**\n",
    "- `IMG_SIZE`: `(256, 256)` ‚Üê Puissance de 2 (optimal GPU)\n",
    "- `BATCH_SIZE`: `64` ‚Üê Plus gros batch (GPU T4/V100)\n",
    "\n",
    "**üéØ Training :**\n",
    "- `EPOCHS`: `100` ‚Üê Training long (temps illimit√© Colab Pro)\n",
    "- `MAX_IMAGES`: `5000` ‚Üê Dataset complet par classe\n",
    "\n",
    "**ü§ñ ML traditionnel :**\n",
    "- Random Forest : `500 estimateurs` (vs 200)\n",
    "- XGBoost : `300 estimateurs` (vs 100)  \n",
    "- Gradient Boosting : `300 estimateurs`\n",
    "- CV : `5 folds` (vs 3)\n",
    "\n",
    "**üß† Deep Learning :**\n",
    "- Architectures : `EfficientNetB3`, `ResNet152V2`, `VGG19`, `DenseNet201`\n",
    "- Mixed precision : `float16` (acc√©l√©ration GPU)\n",
    "- Fine-tuning : `20 couches` (vs 10)\n",
    "- Data augmentation : Rotation, zoom, flip, brightness\n",
    "\n",
    "#### üí° Pourquoi ces optimisations ?\n",
    "\n",
    "1. **GPU T4/V100** : Plus de m√©moire et compute ‚Üí batch size plus gros\n",
    "2. **Temps illimit√©** : Colab Pro permet training long ‚Üí plus d'epochs\n",
    "3. **Puissance de 2** : `256x256` optimise les op√©rations GPU vs `224x224`\n",
    "4. **Mixed precision** : Acc√©l√©ration x1.5-2x sur GPU r√©cents\n",
    "5. **Architectures avanc√©es** : Plus performantes qu'EfficientNetB0/ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6652c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# CONFIGURATION POUR GOOGLE COLAB\n",
    "# ===================================\n",
    "# ‚ö†Ô∏è Ex√©cutez cette cellule uniquement sur Google Colab\n",
    "# Pour l'environnement local, utilisez la cellule suivante\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# V√©rification si on est sur Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"üåê Google Colab d√©tect√©\")\n",
    "    \n",
    "    # 1. Connexion √† Google Drive (pour acc√©der aux donn√©es)\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # 2. Clonage du repository depuis GitHub avec la branche ReVamp\n",
    "    repo_exists = os.path.exists('/content/DS_COVID')\n",
    "    if not repo_exists:\n",
    "        print(\"üì• Clonage du repository (branche ReVamp)...\")\n",
    "        !git clone -b ReVamp https://github.com/L-Poca/DS_COVID.git /content/DS_COVID\n",
    "        print(\"‚úÖ Repository clon√© (branche ReVamp)\")\n",
    "    else:\n",
    "        print(\"‚úÖ Repository d√©j√† pr√©sent\")\n",
    "        # S'assurer qu'on est sur la bonne branche\n",
    "        os.chdir('/content/DS_COVID')\n",
    "        print(\"üîÑ V√©rification de la branche...\")\n",
    "        !git fetch origin\n",
    "        !git checkout ReVamp\n",
    "        !git pull origin ReVamp\n",
    "        print(\"‚úÖ Branche ReVamp mise √† jour\")\n",
    "    \n",
    "    # 3. Changement vers le dossier du projet\n",
    "    os.chdir('/content/DS_COVID')\n",
    "    print(f\"üìÅ Dossier courant: {os.getcwd()}\")\n",
    "    \n",
    "    # V√©rification que pyproject.toml existe (preuve qu'on est sur ReVamp)\n",
    "    if os.path.exists('pyproject.toml'):\n",
    "        print(\"‚úÖ pyproject.toml trouv√© - Branche ReVamp confirm√©e\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è pyproject.toml manquant - Probl√®me de branche!\")\n",
    "    \n",
    "    # 4. Extraction de archive_covid.zip depuis Google Drive\n",
    "    print(\"üìÅ Extraction de archive_covid.zip depuis Drive...\")\n",
    "    \n",
    "    # Chemin vers votre fichier ZIP sur Drive\n",
    "    zip_file_path = \"/content/drive/MyDrive/archive_covid.zip\"\n",
    "    \n",
    "    # Dossier de destination\n",
    "    local_data_dir = \"/content/DS_COVID/data/raw\"\n",
    "    local_data_path = f\"{local_data_dir}/COVID-19_Radiography_Dataset\"\n",
    "    \n",
    "    # Cr√©ation du dossier data/raw\n",
    "    os.makedirs(local_data_dir, exist_ok=True)\n",
    "    \n",
    "    if os.path.exists(zip_file_path):\n",
    "        print(f\"‚úÖ Archive trouv√©e: {zip_file_path}\")\n",
    "        \n",
    "        if not os.path.exists(local_data_path):\n",
    "            print(\"üì¶ Extraction de l'archive COVID...\")\n",
    "            import zipfile\n",
    "            import shutil\n",
    "            import glob\n",
    "            \n",
    "            # Extraction dans un dossier temporaire\n",
    "            temp_extract_path = \"/content/temp_covid_extract\"\n",
    "            \n",
    "            with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(temp_extract_path)\n",
    "                print(f\"‚úÖ Archive extraite dans {temp_extract_path}\")\n",
    "            \n",
    "            # Recherche intelligente du dossier COVID\n",
    "            search_patterns = [\n",
    "                f\"{temp_extract_path}/**/COVID-19_Radiography_Dataset\",\n",
    "                f\"{temp_extract_path}/**/*COVID*\",\n",
    "                f\"{temp_extract_path}/**/*radiography*\",\n",
    "                f\"{temp_extract_path}/**/*chest*\"\n",
    "            ]\n",
    "            \n",
    "            covid_source = None\n",
    "            for pattern in search_patterns:\n",
    "                possible_dirs = glob.glob(pattern, recursive=True)\n",
    "                possible_dirs = [d for d in possible_dirs if os.path.isdir(d)]\n",
    "                if possible_dirs:\n",
    "                    covid_source = possible_dirs[0]\n",
    "                    break\n",
    "            \n",
    "            if covid_source:\n",
    "                print(f\"‚úÖ Dossier COVID trouv√©: {covid_source}\")\n",
    "                \n",
    "                # V√©rification de la structure (doit contenir COVID, Normal, etc.)\n",
    "                subdirs = [d.name for d in Path(covid_source).iterdir() if d.is_dir()]\n",
    "                print(f\"   Sous-dossiers: {subdirs}\")\n",
    "                \n",
    "                # D√©placement vers la destination finale\n",
    "                if os.path.exists(local_data_path):\n",
    "                    shutil.rmtree(local_data_path)\n",
    "                shutil.move(covid_source, local_data_path)\n",
    "                \n",
    "                # Nettoyage du dossier temporaire  \n",
    "                shutil.rmtree(temp_extract_path)\n",
    "                print(f\"‚úÖ Donn√©es COVID disponibles: {local_data_path}\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è Structure du ZIP non reconnue automatiquement\")\n",
    "                # Affichage du contenu pour debug\n",
    "                for root, dirs, files in os.walk(temp_extract_path):\n",
    "                    if dirs:\n",
    "                        print(f\"   üìÅ {root}: {dirs[:3]}...\")  # Premi√®res 3 dossiers\n",
    "                        break\n",
    "                print(f\"   V√©rifiez manuellement dans: {temp_extract_path}\")\n",
    "        else:\n",
    "            print(f\"‚úÖ Donn√©es d√©j√† extraites: {local_data_path}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Archive non trouv√©e: {zip_file_path}\")\n",
    "        print(\"üí° Solutions possibles:\")\n",
    "        print(\"   1. V√©rifiez que 'archive_covid.zip' est dans MyDrive/\")\n",
    "        print(\"   2. Ou modifiez zip_file_path avec le bon chemin\")\n",
    "        print(\"   3. Ou uploadez archive_covid.zip dans MyDrive/\")\n",
    "    \n",
    "    # 5. Installation des d√©pendances depuis requirements.txt\n",
    "    print(\"üì¶ Installation des d√©pendances...\")\n",
    "    !pip install -r requirements.txt\n",
    "    \n",
    "    # 6. Installation du package ds-covid en mode d√©veloppement\n",
    "    print(\"üì¶ Installation du package ds-covid...\")\n",
    "    !pip install -e .\n",
    "    \n",
    "    # 7. V√©rification de l'installation et configuration\n",
    "    try:\n",
    "        import ds_covid\n",
    "        from ds_covid import Settings\n",
    "        print(f\"‚úÖ Package ds-covid v{ds_covid.__version__} install√© avec succ√®s\")\n",
    "        \n",
    "        # Configuration optimis√©e pour Colab Pro\n",
    "        settings = Settings()\n",
    "        \n",
    "        # Adaptation des chemins pour Colab\n",
    "        from pathlib import Path\n",
    "        PROJECT_ROOT = Path('/content/DS_COVID')\n",
    "        \n",
    "        # V√©rification des donn√©es extraites\n",
    "        if os.path.exists(local_data_path):\n",
    "            # Structure normale: COVID-19_Radiography_Dataset/COVID-19_Radiography_Dataset/\n",
    "            inner_covid_path = Path(local_data_path) / 'COVID-19_Radiography_Dataset'\n",
    "            if inner_covid_path.exists():\n",
    "                DATA_DIR = inner_covid_path\n",
    "            else:\n",
    "                DATA_DIR = Path(local_data_path)\n",
    "        else:\n",
    "            DATA_DIR = Path(local_data_path)\n",
    "        \n",
    "        MODELS_DIR = PROJECT_ROOT / 'models'\n",
    "        RESULTS_DIR = PROJECT_ROOT / 'results'\n",
    "        \n",
    "        # Variables optimis√©es pour Colab Pro\n",
    "        BATCH_SIZE = settings.training.batch_size      # 64\n",
    "        EPOCHS = settings.training.epochs              # 100\n",
    "        LEARNING_RATE = settings.training.learning_rate\n",
    "        IMG_SIZE = settings.training.img_size          # (256, 256)\n",
    "        IMG_CHANNELS = settings.training.img_channels\n",
    "        TEST_SPLIT = settings.training.test_split\n",
    "        VALIDATION_SPLIT = settings.training.validation_split\n",
    "        RANDOM_SEED = settings.training.random_seed\n",
    "        \n",
    "        # Configuration avanc√©e\n",
    "        MAX_IMAGES_PER_CLASS = settings.data.max_images_per_class\n",
    "        AUGMENTATION_PARAMS = settings.data.augmentation_params\n",
    "        DL_ARCHITECTURES = settings.deep_learning.architectures\n",
    "        MIXED_PRECISION = settings.deep_learning.mixed_precision\n",
    "        \n",
    "        # Classes\n",
    "        CLASSES = settings.data.class_names\n",
    "        CLASS_MAPPING = settings.data.class_mapping\n",
    "        NUM_CLASSES = settings.data.num_classes\n",
    "        \n",
    "        print(f\"‚úÖ Configuration COLAB PRO:\")\n",
    "        print(f\"   - Donn√©es: {DATA_DIR}\")\n",
    "        print(f\"   - Classes: {CLASSES}\")\n",
    "        print(f\"   - Image size: {IMG_SIZE}\")\n",
    "        print(f\"   - Batch size: {BATCH_SIZE}\")\n",
    "        print(f\"   - Epochs: {EPOCHS}\")\n",
    "        \n",
    "    except ImportError as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur package: {e}\")\n",
    "        # Configuration manuelle de fallback\n",
    "        from pathlib import Path\n",
    "        PROJECT_ROOT = Path('/content/DS_COVID')\n",
    "        DATA_DIR = Path(local_data_path)\n",
    "        MODELS_DIR = PROJECT_ROOT / 'models'\n",
    "        RESULTS_DIR = PROJECT_ROOT / 'results'\n",
    "        \n",
    "        BATCH_SIZE = 64\n",
    "        EPOCHS = 100\n",
    "        LEARNING_RATE = 0.001\n",
    "        IMG_SIZE = (256, 256)\n",
    "        IMG_CHANNELS = 3\n",
    "        TEST_SPLIT = 0.2\n",
    "        VALIDATION_SPLIT = 0.2\n",
    "        RANDOM_SEED = 42\n",
    "        \n",
    "        CLASSES = ['COVID', 'Lung_Opacity', 'Normal', 'Viral Pneumonia']\n",
    "        CLASS_MAPPING = {'COVID': 0, 'Lung_Opacity': 1, 'Normal': 2, 'Viral Pneumonia': 3}\n",
    "        NUM_CLASSES = len(CLASSES)\n",
    "    \n",
    "    # Cr√©ation des dossiers\n",
    "    MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # V√©rification finale des donn√©es\n",
    "    if DATA_DIR.exists():\n",
    "        subdirs = [d.name for d in DATA_DIR.iterdir() if d.is_dir()]\n",
    "        print(f\"   - Dossiers trouv√©s: {subdirs}\")\n",
    "        \n",
    "        # Comptage rapide des images\n",
    "        total_images = 0\n",
    "        for subdir in subdirs[:4]:  # Premi√®res 4 classes\n",
    "            subdir_path = DATA_DIR / subdir\n",
    "            if subdir_path.exists():\n",
    "                images = list(subdir_path.glob('*.png')) + list(subdir_path.glob('*.jpg'))\n",
    "                print(f\"     {subdir}: {len(images)} images\")\n",
    "                total_images += len(images)\n",
    "        print(f\"   - Total: ~{total_images} images\")\n",
    "    else:\n",
    "        print(f\"   - ‚ö†Ô∏è Dossier non accessible: {DATA_DIR}\")\n",
    "    \n",
    "else:\n",
    "    print(\"üíª Environnement local d√©tect√© - utilisez la cellule de configuration normale\")\n",
    "    \n",
    "# Imports communs (marchent partout)\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, VotingClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks, optimizers\n",
    "from tensorflow.keras.applications import VGG16, ResNet50, EfficientNetB0\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Configuration GPU\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    print(f\"‚úÖ GPU d√©tect√©: {len(physical_devices)} device(s)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Pas de GPU d√©tect√©, utilisation du CPU\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(f\"\\nüì¶ Versions (Colab):\")\n",
    "    print(f\"   - Python: {sys.version.split()[0]}\")\n",
    "    print(f\"   - Scikit-learn: {sklearn.__version__}\")\n",
    "    print(f\"   - TensorFlow: {tf.__version__}\")\n",
    "    print(f\"   - OpenCV: {cv2.__version__}\")\n",
    "    print(f\"   - NumPy: {np.__version__}\")\n",
    "    print(f\"   - Pandas: {pd.__version__}\")\n",
    "    print(\"\\n‚úÖ Configuration Colab termin√©e - Donn√©es extraites depuis archive_covid.zip !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a320e36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration pour Google Colab avec archive_covid.zip\n",
    "import os\n",
    "import sys\n",
    "from google.colab import drive\n",
    "\n",
    "print(\"\udd27 CONFIGURATION GOOGLE COLAB PRO\")\n",
    "\n",
    "# Monter Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Cloner le repo si pas d√©j√† fait\n",
    "if not os.path.exists('/content/DS_COVID'):\n",
    "    print(\"üì• Clonage du repository...\")\n",
    "    !git clone https://github.com/L-Poca/DS_COVID.git /content/DS_COVID\n",
    "\n",
    "# Aller dans le dossier du projet\n",
    "os.chdir('/content/DS_COVID')\n",
    "\n",
    "# Extraction intelligente de archive_covid.zip depuis Drive\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "def find_and_extract_covid_archive():\n",
    "    \"\"\"Trouve et extrait archive_covid.zip depuis Google Drive\"\"\"\n",
    "    possible_paths = [\n",
    "        '/content/drive/MyDrive/archive_covid.zip',\n",
    "        '/content/drive/My Drive/archive_covid.zip',\n",
    "        '/content/drive/MyDrive/COVID/archive_covid.zip',\n",
    "        '/content/drive/My Drive/COVID/archive_covid.zip'\n",
    "    ]\n",
    "    \n",
    "    for zip_path in possible_paths:\n",
    "        if os.path.exists(zip_path):\n",
    "            print(f\"üì¶ Archive trouv√©e: {zip_path}\")\n",
    "            \n",
    "            # Cr√©er le dossier data/raw s'il n'existe pas\n",
    "            os.makedirs('/content/DS_COVID/data/raw', exist_ok=True)\n",
    "            \n",
    "            # Extraire l'archive\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall('/content/DS_COVID/data/raw/')\n",
    "            \n",
    "            # Chercher le dossier COVID-19_Radiography_Dataset\n",
    "            for root, dirs, files in os.walk('/content/DS_COVID/data/raw/'):\n",
    "                for dir_name in dirs:\n",
    "                    if 'COVID' in dir_name and 'Radiography' in dir_name:\n",
    "                        dataset_path = os.path.join(root, dir_name)\n",
    "                        print(f\"‚úÖ Dataset trouv√©: {dataset_path}\")\n",
    "                        return dataset_path\n",
    "            \n",
    "            print(\"‚ö†Ô∏è Dossier COVID-19_Radiography_Dataset non trouv√© dans l'archive\")\n",
    "            return None\n",
    "    \n",
    "    print(\"‚ùå Archive archive_covid.zip non trouv√©e dans Drive\")\n",
    "    print(\"üìÇ V√©rifiez que le fichier est dans MyDrive/\")\n",
    "    return None\n",
    "\n",
    "# Extraire les donn√©es\n",
    "dataset_path = find_and_extract_covid_archive()\n",
    "\n",
    "# Installation du package\n",
    "print(\"üì¶ Installation du package ds-covid...\")\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa8459f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Pas de GPU d√©tect√©, utilisation du CPU\n",
      "üì¶ Package ds-covid v0.1.0\n",
      "üîß Chargement de la configuration depuis .env...\n",
      "üìÑ Fichier .env charg√©: /home/cepa/DST/projet_DS/DS_COVID/.env\n",
      "‚úÖ Configuration charg√©e depuis .env:\n",
      "   - PROJECT_ROOT: /home/cepa/DST/projet_DS/DS_COVID\n",
      "   - DATA_DIR: /home/cepa/DST/projet_DS/DS_COVID/data/raw/COVID-19_Radiography_Dataset/COVID-19_Radiography_Dataset\n",
      "   - MODELS_DIR: /home/cepa/DST/projet_DS/DS_COVID/models\n",
      "   - RESULTS_DIR: /home/cepa/DST/projet_DS/DS_COVID/reports\n",
      "\n",
      "üìÅ V√©rification des chemins (.env):\n",
      "   ‚úÖ PROJECT_ROOT: /home/cepa/DST/projet_DS/DS_COVID\n",
      "   ‚úÖ DATA_DIR: /home/cepa/DST/projet_DS/DS_COVID/data/raw/COVID-19_Radiography_Dataset/COVID-19_Radiography_Dataset\n",
      "   ‚úÖ MODELS_DIR: /home/cepa/DST/projet_DS/DS_COVID/models\n",
      "   ‚úÖ RESULTS_DIR: /home/cepa/DST/projet_DS/DS_COVID/reports\n",
      "\n",
      "üéØ Configuration finale (depuis .env):\n",
      "   - Classes (4): ['COVID', 'Lung_Opacity', 'Normal', 'Viral Pneumonia']\n",
      "   - Mapping: {'COVID': 0, 'Lung_Opacity': 1, 'Normal': 2, 'Viral Pneumonia': 3}\n",
      "   - Image size: (224, 224)\n",
      "   - Batch size: 32\n",
      "   - Epochs: 50\n",
      "   - Learning rate: 0.001\n",
      "   - Random seed: 42\n",
      "\n",
      "üì¶ Versions:\n",
      "   - Python: 3.10.12\n",
      "   - ds-covid: 0.1.0\n",
      "   - Scikit-learn: 1.7.0\n",
      "   - TensorFlow: 2.19.0\n",
      "   - OpenCV: 4.11.0\n",
      "   - NumPy: 2.1.3\n",
      "   - Pandas: 2.3.0\n",
      "\n",
      "‚úÖ Configuration .env charg√©e avec 4 classes - Pr√™t pour le ML/DL !\n"
     ]
    }
   ],
   "source": [
    "# ===================================\n",
    "# 1.1 CONFIGURATION LOCALE (VS Code / Environnement local)\n",
    "# ===================================\n",
    "# ‚ö†Ô∏è Cette cellule est pour l'environnement LOCAL avec le package ds-covid install√©\n",
    "# Pour Google Colab, utilisez la cellule pr√©c√©dente\n",
    "\n",
    "# Configuration du package ds-covid (utilise automatiquement .env)\n",
    "from ds_covid import Settings, configure_package, __version__\n",
    "import ds_covid\n",
    "\n",
    "# Imports de base\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import json\n",
    "import sys\n",
    "\n",
    "# Configuration des warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning - Scikit-learn\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, VotingClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "\n",
    "# Deep Learning - TensorFlow/Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks, optimizers\n",
    "from tensorflow.keras.applications import VGG16, ResNet50, EfficientNetB0\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Configuration GPU\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    print(f\"‚úÖ GPU d√©tect√©: {len(physical_devices)} device(s)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Pas de GPU d√©tect√©, utilisation du CPU\")\n",
    "\n",
    "# Configuration automatique depuis .env\n",
    "print(f\"üì¶ Package ds-covid v{__version__}\")\n",
    "print(\"üîß Chargement de la configuration depuis .env...\")\n",
    "\n",
    "# Le package charge automatiquement les settings depuis .env\n",
    "settings = Settings()\n",
    "\n",
    "# Affichage de la configuration charg√©e\n",
    "print(f\"‚úÖ Configuration charg√©e depuis .env:\")\n",
    "print(f\"   - PROJECT_ROOT: {settings.project_root}\")\n",
    "print(f\"   - DATA_DIR: {settings.data_dir}\")\n",
    "print(f\"   - MODELS_DIR: {settings.models_dir}\")\n",
    "print(f\"   - RESULTS_DIR: {settings.results_dir}\")\n",
    "\n",
    "# Variables globales depuis les settings (.env)\n",
    "RANDOM_SEED = settings.training.random_seed\n",
    "BATCH_SIZE = settings.training.batch_size\n",
    "EPOCHS = settings.training.epochs\n",
    "LEARNING_RATE = settings.training.learning_rate\n",
    "IMG_SIZE = settings.training.img_size\n",
    "IMG_CHANNELS = settings.training.img_channels\n",
    "TEST_SPLIT = settings.training.test_split\n",
    "VALIDATION_SPLIT = settings.training.validation_split\n",
    "\n",
    "# Variables de donn√©es depuis .env\n",
    "MAX_IMAGES_PER_CLASS = settings.data.max_images_per_class\n",
    "\n",
    "# Chemins depuis .env\n",
    "PROJECT_ROOT = Path(settings.project_root)\n",
    "DATA_DIR = Path(settings.data_dir)\n",
    "MODELS_DIR = Path(settings.models_dir)\n",
    "RESULTS_DIR = Path(settings.results_dir)\n",
    "\n",
    "# V√©rification que les chemins existent\n",
    "print(f\"\\nüìÅ V√©rification des chemins (.env):\")\n",
    "for name, path in [('PROJECT_ROOT', PROJECT_ROOT), ('DATA_DIR', DATA_DIR), \n",
    "                   ('MODELS_DIR', MODELS_DIR), ('RESULTS_DIR', RESULTS_DIR)]:\n",
    "    if path.exists():\n",
    "        print(f\"   ‚úÖ {name}: {path}\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå {name}: {path} (sera cr√©√© si n√©cessaire)\")\n",
    "        if name in ['MODELS_DIR', 'RESULTS_DIR']:\n",
    "            path.mkdir(parents=True, exist_ok=True)\n",
    "            print(f\"   ‚úÖ {name}: Cr√©√© avec succ√®s\")\n",
    "\n",
    "# Classes du dataset COVID (depuis la configuration)\n",
    "CLASSES = settings.data.class_names\n",
    "CLASS_MAPPING = settings.data.class_mapping\n",
    "NUM_CLASSES = settings.data.num_classes\n",
    "\n",
    "# R√©sum√© de la configuration\n",
    "print(f\"\\nüéØ Configuration finale (depuis .env):\")\n",
    "print(f\"   - Classes ({NUM_CLASSES}): {CLASSES}\")\n",
    "print(f\"   - Image size: {IMG_SIZE}\")\n",
    "print(f\"   - Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   - Epochs: {EPOCHS}\")\n",
    "print(f\"   - Max images/classe: {MAX_IMAGES_PER_CLASS}\")\n",
    "\n",
    "# Pour Colab Pro - Configuration optimis√©e\n",
    "print(\"\\nüöÄ OPTIMISATIONS COLAB PRO ACTIV√âES\")\n",
    "print(\"‚úÖ Mixed precision activ√©e (float16)\")\n",
    "\n",
    "print(\"\\nüéØ PARAM√àTRES COLAB PRO:\")\n",
    "print(f\"   - Image size: {IMG_SIZE} ‚Üê Puissance de 2 (optimal GPU)\")\n",
    "print(f\"   - Batch size: {BATCH_SIZE} ‚Üê Plus gros (GPU puissant)\")\n",
    "print(f\"   - Epochs: {EPOCHS} ‚Üê Plus long (temps illimit√©)\")\n",
    "print(f\"   - Max images: {MAX_IMAGES_PER_CLASS} ‚Üê Dataset complet\")\n",
    "\n",
    "# Test m√©moire GPU\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    # Activer mixed precision si GPU disponible\n",
    "    from tensorflow.keras import mixed_precision\n",
    "    mixed_precision.set_global_policy('mixed_float16')\n",
    "    print(\"‚úÖ Mixed precision float16 activ√©e\")\n",
    "\n",
    "print(\"üéâ Configuration termin√©e!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c863a121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/cepa/DST/projet_DS/DS_COVID/notebooks')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edfcb959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Variables d'environnement charg√©es depuis: /home/cepa/DST/projet_DS/DS_COVID/.env\n",
      "üîß Chargement de la configuration centralis√©e...\n",
      "============================================================\n",
      "üîß CONFIGURATION DU PROJET DS_COVID\n",
      "============================================================\n",
      "\n",
      "üìÅ CHEMINS:\n",
      "   ‚úÖ project_root: /home/cepa/DST/projet_DS/DS_COVID\n",
      "   ‚úÖ data_dir: /home/cepa/DST/projet_DS/DS_COVID/data/raw/COVID-19_Radiography_Dataset/COVID-19_Radiography_Dataset\n",
      "   ‚úÖ models_dir: /home/cepa/DST/projet_DS/DS_COVID/models\n",
      "   ‚úÖ results_dir: /home/cepa/DST/projet_DS/DS_COVID/reports\n",
      "   ‚úÖ notebooks_dir: /home/cepa/DST/projet_DS/DS_COVID/notebooks\n",
      "\n",
      "üñºÔ∏è IMAGES:\n",
      "   üìê Taille: (224, 224)\n",
      "   üé® Canaux: 3\n",
      "\n",
      "üéØ ENTRA√éNEMENT:\n",
      "   üìä Batch size: 32\n",
      "   üîÑ √âpoques: 50\n",
      "   üìà Learning rate: 0.001\n",
      "\n",
      "üè∑Ô∏è CLASSES:\n",
      "   üìã 4 classes: COVID, Lung_Opacity, Normal, Viral Pneumonia\n",
      "============================================================\n",
      "üìÅ data_dir: /home/cepa/DST/projet_DS/DS_COVID/data/raw/COVID-19_Radiography_Dataset/COVID-19_Radiography_Dataset\n",
      "üìÅ models_dir: /home/cepa/DST/projet_DS/DS_COVID/models\n",
      "üìÅ results_dir: /home/cepa/DST/projet_DS/DS_COVID/reports\n",
      "üìÅ notebooks_dir: /home/cepa/DST/projet_DS/DS_COVID/notebooks\n",
      "üíª Configuration CPU\n",
      "‚úÖ Environnement configur√©\n",
      "\n",
      "‚úÖ Configuration charg√©e depuis .env!\n",
      "üìä Param√®tres principaux:\n",
      "   üñºÔ∏è Taille d'image: (224, 224)\n",
      "   üì¶ Batch size: 32\n",
      "   üîÑ √âpoques: 50\n",
      "   üéØ Classes: 4 (COVID, Lung_Opacity, Normal, Viral Pneumonia)\n",
      "   üìÅ R√©pertoire de donn√©es: /home/cepa/DST/projet_DS/DS_COVID/data/raw/COVID-19_Radiography_Dataset/COVID-19_Radiography_Dataset\n",
      "üé≤ Seed configur√©: 42\n",
      "üîß Environnement pr√™t avec configuration centralis√©e!\n"
     ]
    }
   ],
   "source": [
    "# ===================================\n",
    "# 1.2 CONFIGURATION CENTRALIS√âE AVEC .ENV\n",
    "# ===================================\n",
    "\n",
    "# Import du gestionnaire de configuration (chemin relatif)\n",
    "# D√©tection automatique du chemin src depuis le notebook\n",
    "notebook_dir = Path.cwd() if 'notebooks' in str(Path.cwd()) else Path(__file__).parent\n",
    "project_root = notebook_dir.parent if notebook_dir.name == 'notebooks' else notebook_dir\n",
    "src_path = project_root / 'src'\n",
    "\n",
    "sys.path.append(str(src_path))\n",
    "from config import config_manager, get_config, setup_environment\n",
    "\n",
    "print(\"üîß Chargement de la configuration centralis√©e...\")\n",
    "\n",
    "# Affichage du r√©sum√© de configuration\n",
    "config_manager.print_summary()\n",
    "\n",
    "# Cr√©ation des r√©pertoires n√©cessaires\n",
    "config_manager.create_directories()\n",
    "\n",
    "# Configuration de l'environnement\n",
    "setup_environment()\n",
    "\n",
    "# R√©cup√©ration des variables de configuration\n",
    "PROJECT_ROOT = get_config('paths', 'project_root')\n",
    "DATA_DIR = get_config('paths', 'data_dir')\n",
    "MODELS_DIR = get_config('paths', 'models_dir')\n",
    "RESULTS_DIR = get_config('paths', 'results_dir')\n",
    "\n",
    "# Configuration d'images\n",
    "IMG_SIZE = get_config('image', 'img_size')\n",
    "IMG_CHANNELS = get_config('image', 'img_channels')\n",
    "\n",
    "# Param√®tres d'entra√Ænement depuis .env\n",
    "BATCH_SIZE = get_config('training', 'batch_size')\n",
    "EPOCHS = get_config('training', 'epochs')\n",
    "LEARNING_RATE = get_config('training', 'learning_rate')\n",
    "VALIDATION_SPLIT = get_config('training', 'validation_split')\n",
    "TEST_SPLIT = get_config('training', 'test_split')\n",
    "RANDOM_SEED = get_config('training', 'random_seed')\n",
    "\n",
    "# Classes depuis la configuration\n",
    "CLASSES = get_config('classes', 'class_names')\n",
    "CLASS_MAPPING = get_config('classes', 'class_mapping')\n",
    "\n",
    "print(\"\\n‚úÖ Configuration charg√©e depuis .env!\")\n",
    "print(f\"üìä Param√®tres principaux:\")\n",
    "print(f\"   üñºÔ∏è Taille d'image: {IMG_SIZE}\")\n",
    "print(f\"   üì¶ Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   üîÑ √âpoques: {EPOCHS}\")\n",
    "print(f\"   üéØ Classes: {len(CLASSES)} ({', '.join(CLASSES)})\")\n",
    "print(f\"   üìÅ R√©pertoire de donn√©es: {DATA_DIR}\")\n",
    "\n",
    "# Configuration des seeds pour reproductibilit√©\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "print(f\"üé≤ Seed configur√©: {RANDOM_SEED}\")\n",
    "print(\"üîß Environnement pr√™t avec configuration centralis√©e!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ff6661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 1.2 CONFIGURATION DES CHEMINS ET DATASET\n",
    "# ===================================\n",
    "\n",
    "# Chemins du projet\n",
    "PROJECT_ROOT = Path(\"/home/cepa/DST/projet_DS/DS_COVID\")\n",
    "DATA_DIR = PROJECT_ROOT / \"data\" / \"raw\" / \"COVID-19_Radiography_Dataset\" / \"COVID-19_Radiography_Dataset\"\n",
    "MODELS_DIR = PROJECT_ROOT / \"models\"\n",
    "RESULTS_DIR = PROJECT_ROOT / \"reports\"\n",
    "\n",
    "# Cr√©ation des dossiers de sortie\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ R√©pertoire du projet: {PROJECT_ROOT}\")\n",
    "print(f\"üìÅ R√©pertoire des donn√©es: {DATA_DIR}\")\n",
    "print(f\"üìÅ R√©pertoire des mod√®les: {MODELS_DIR}\")\n",
    "\n",
    "# D√©finition des classes\n",
    "CLASSES = [\"COVID\", \"Lung_Opacity\", \"Normal\", \"Viral Pneumonia\"]\n",
    "CLASS_MAPPING = {cls: idx for idx, cls in enumerate(CLASSES)}\n",
    "\n",
    "print(f\"üè∑Ô∏è Classes d√©tect√©es: {CLASSES}\")\n",
    "print(f\"üî¢ Mapping des classes: {CLASS_MAPPING}\")\n",
    "\n",
    "# Param√®tres globaux\n",
    "IMG_SIZE = (224, 224)  # Taille standard pour les mod√®les pr√©-entra√Æn√©s\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.001\n",
    "VALIDATION_SPLIT = 0.2\n",
    "TEST_SPLIT = 0.2\n",
    "\n",
    "print(f\"‚öôÔ∏è Taille d'image: {IMG_SIZE}\")\n",
    "print(f\"‚öôÔ∏è Batch size: {BATCH_SIZE}\")\n",
    "print(f\"‚öôÔ∏è Nombre d'√©poques: {EPOCHS}\")\n",
    "\n",
    "# V√©rification de l'existence des donn√©es\n",
    "if DATA_DIR.exists():\n",
    "    print(\"‚úÖ R√©pertoire de donn√©es trouv√©\")\n",
    "    for class_name in CLASSES:\n",
    "        class_path = DATA_DIR / class_name / \"images\"\n",
    "        if class_path.exists():\n",
    "            n_images = len(list(class_path.glob(\"*.png\")))\n",
    "            print(f\"   üìä {class_name}: {n_images} images\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå {class_name}: r√©pertoire non trouv√©\")\n",
    "else:\n",
    "    print(\"‚ùå R√©pertoire de donn√©es non trouv√©!\")\n",
    "    print(f\"   V√©rifiez le chemin: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14639c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 1.3 FONCTIONS UTILITAIRES POUR LE CHARGEMENT\n",
    "# ===================================\n",
    "\n",
    "def load_image_paths_and_labels(data_dir, classes):\n",
    "    \"\"\"\n",
    "    Charge les chemins des images et leurs labels\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (image_paths, labels, class_counts)\n",
    "    \"\"\"\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    class_counts = {}\n",
    "    \n",
    "    for class_name in classes:\n",
    "        class_dir = data_dir / class_name / \"images\"\n",
    "        if not class_dir.exists():\n",
    "            continue\n",
    "            \n",
    "        # R√©cup√©ration des images\n",
    "        image_files = list(class_dir.glob(\"*.png\"))\n",
    "        class_counts[class_name] = len(image_files)\n",
    "        \n",
    "        # Ajout des chemins et labels\n",
    "        for img_path in image_files:\n",
    "            image_paths.append(str(img_path))\n",
    "            labels.append(class_name)\n",
    "    \n",
    "    return image_paths, labels, class_counts\n",
    "\n",
    "def load_and_preprocess_image(image_path, target_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Charge et pr√©processe une image\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Chemin vers l'image\n",
    "        target_size (tuple): Taille cible (largeur, hauteur)\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Image pr√©process√©e\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Chargement avec PIL\n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        # Redimensionnement\n",
    "        img = img.resize(target_size)\n",
    "        \n",
    "        # Conversion en array numpy\n",
    "        img_array = np.array(img)\n",
    "        \n",
    "        # Normalisation [0, 1]\n",
    "        img_array = img_array.astype(np.float32) / 255.0\n",
    "        \n",
    "        return img_array\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du chargement de {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_balanced_subset(image_paths, labels, max_per_class=500):\n",
    "    \"\"\"\n",
    "    Cr√©e un sous-ensemble √©quilibr√© du dataset\n",
    "    \n",
    "    Args:\n",
    "        image_paths (list): Liste des chemins d'images\n",
    "        labels (list): Liste des labels\n",
    "        max_per_class (int): Nombre maximum d'images par classe\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (subset_paths, subset_labels)\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({'path': image_paths, 'label': labels})\n",
    "    \n",
    "    # √âchantillonnage √©quilibr√©\n",
    "    balanced_df = df.groupby('label').apply(\n",
    "        lambda x: x.sample(n=min(len(x), max_per_class), random_state=42)\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    return balanced_df['path'].tolist(), balanced_df['label'].tolist()\n",
    "\n",
    "print(\"üîß Fonctions utilitaires d√©finies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f47e243",
   "metadata": {},
   "source": [
    "## üìä Section 2: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723001d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 2.1 CHARGEMENT ET ANALYSE DE LA DISTRIBUTION\n",
    "# ===================================\n",
    "\n",
    "# Chargement des chemins et labels\n",
    "print(\"üìÇ Chargement des donn√©es...\")\n",
    "image_paths, labels, class_counts = load_image_paths_and_labels(DATA_DIR, CLASSES)\n",
    "\n",
    "print(f\"üìä Total d'images: {len(image_paths)}\")\n",
    "print(f\"üìä Total de labels: {len(labels)}\")\n",
    "\n",
    "# Cr√©ation du DataFrame pour l'analyse\n",
    "df_analysis = pd.DataFrame({\n",
    "    'image_path': image_paths,\n",
    "    'label': labels\n",
    "})\n",
    "\n",
    "# Analyse de la distribution des classes\n",
    "print(\"\\nüè∑Ô∏è Distribution des classes:\")\n",
    "class_distribution = df_analysis['label'].value_counts()\n",
    "print(class_distribution)\n",
    "\n",
    "# Visualisation de la distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Graphique en barres\n",
    "class_distribution.plot(kind='bar', ax=ax1, color='skyblue', alpha=0.8)\n",
    "ax1.set_title('Distribution des Classes', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Classes', fontsize=12)\n",
    "ax1.set_ylabel('Nombre d\\'images', fontsize=12)\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Graphique en secteurs\n",
    "ax2.pie(class_distribution.values, labels=class_distribution.index, autopct='%1.1f%%', \n",
    "        startangle=90, colors=plt.cm.Set3.colors)\n",
    "ax2.set_title('R√©partition des Classes (%)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistiques d√©taill√©es\n",
    "print(\"\\nüìà Statistiques d√©taill√©es:\")\n",
    "total_images = len(image_paths)\n",
    "for class_name, count in class_distribution.items():\n",
    "    percentage = (count / total_images) * 100\n",
    "    print(f\"   {class_name}: {count} images ({percentage:.2f}%)\")\n",
    "\n",
    "# D√©tection du d√©s√©quilibre\n",
    "max_count = class_distribution.max()\n",
    "min_count = class_distribution.min()\n",
    "imbalance_ratio = max_count / min_count\n",
    "\n",
    "print(f\"\\n‚öñÔ∏è Ratio de d√©s√©quilibre: {imbalance_ratio:.2f}\")\n",
    "if imbalance_ratio > 2:\n",
    "    print(\"‚ö†Ô∏è  Dataset d√©s√©quilibr√© d√©tect√© - techniques de r√©√©quilibrage recommand√©es\")\n",
    "else:\n",
    "    print(\"‚úÖ Dataset relativement √©quilibr√©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a686f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 2.2 VISUALISATION DES IMAGES REPR√âSENTATIVES\n",
    "# ===================================\n",
    "\n",
    "def visualize_sample_images(image_paths, labels, classes, n_samples=3):\n",
    "    \"\"\"\n",
    "    Visualise des √©chantillons d'images pour chaque classe\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(len(classes), n_samples, figsize=(15, 12))\n",
    "    fig.suptitle('√âchantillons d\\'Images par Classe', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    df_viz = pd.DataFrame({'path': image_paths, 'label': labels})\n",
    "    \n",
    "    for i, class_name in enumerate(classes):\n",
    "        class_images = df_viz[df_viz['label'] == class_name]['path'].tolist()\n",
    "        \n",
    "        # S√©lection al√©atoire d'√©chantillons\n",
    "        samples = np.random.choice(class_images, min(n_samples, len(class_images)), replace=False)\n",
    "        \n",
    "        for j, img_path in enumerate(samples):\n",
    "            try:\n",
    "                # Chargement de l'image\n",
    "                img = cv2.imread(img_path)\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                # Affichage\n",
    "                axes[i, j].imshow(img, cmap='gray' if len(img.shape) == 2 else None)\n",
    "                axes[i, j].set_title(f'{class_name}', fontsize=10)\n",
    "                axes[i, j].axis('off')\n",
    "                \n",
    "            except Exception as e:\n",
    "                axes[i, j].text(0.5, 0.5, f'Erreur:\\n{str(e)}', \n",
    "                              ha='center', va='center', transform=axes[i, j].transAxes)\n",
    "                axes[i, j].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualisation des √©chantillons\n",
    "print(\"üñºÔ∏è Visualisation des √©chantillons d'images...\")\n",
    "visualize_sample_images(image_paths, labels, CLASSES, n_samples=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2d10ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 2.3 ANALYSE DES PROPRI√âT√âS DES IMAGES\n",
    "# ===================================\n",
    "\n",
    "def analyze_image_properties(image_paths, sample_size=100):\n",
    "    \"\"\"\n",
    "    Analyse les propri√©t√©s des images (taille, intensit√©, etc.)\n",
    "    \"\"\"\n",
    "    print(f\"üîç Analyse des propri√©t√©s sur un √©chantillon de {sample_size} images...\")\n",
    "    \n",
    "    # √âchantillonnage al√©atoire\n",
    "    sample_paths = np.random.choice(image_paths, min(sample_size, len(image_paths)), replace=False)\n",
    "    \n",
    "    properties = {\n",
    "        'widths': [],\n",
    "        'heights': [],\n",
    "        'channels': [],\n",
    "        'mean_intensities': [],\n",
    "        'std_intensities': [],\n",
    "        'file_sizes': []\n",
    "    }\n",
    "    \n",
    "    for img_path in sample_paths:\n",
    "        try:\n",
    "            # Chargement avec PIL pour les propri√©t√©s de base\n",
    "            img_pil = Image.open(img_path)\n",
    "            width, height = img_pil.size\n",
    "            \n",
    "            # Chargement avec OpenCV pour l'analyse d'intensit√©\n",
    "            img_cv = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            \n",
    "            # Propri√©t√©s\n",
    "            properties['widths'].append(width)\n",
    "            properties['heights'].append(height)\n",
    "            properties['channels'].append(len(img_pil.getbands()))\n",
    "            properties['mean_intensities'].append(np.mean(img_cv))\n",
    "            properties['std_intensities'].append(np.std(img_cv))\n",
    "            properties['file_sizes'].append(os.path.getsize(img_path) / 1024)  # KB\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Erreur avec {img_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return properties\n",
    "\n",
    "# Analyse des propri√©t√©s\n",
    "properties = analyze_image_properties(image_paths, sample_size=200)\n",
    "\n",
    "# Cr√©ation du DataFrame d'analyse\n",
    "df_props = pd.DataFrame(properties)\n",
    "\n",
    "# Affichage des statistiques\n",
    "print(\"\\nüìä Statistiques des propri√©t√©s d'images:\")\n",
    "print(df_props.describe())\n",
    "\n",
    "# Visualisation des distributions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Distribution des Propri√©t√©s d\\'Images', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Tailles\n",
    "axes[0, 0].hist(df_props['widths'], bins=20, alpha=0.7, color='skyblue')\n",
    "axes[0, 0].set_title('Distribution des Largeurs')\n",
    "axes[0, 0].set_xlabel('Largeur (pixels)')\n",
    "\n",
    "axes[0, 1].hist(df_props['heights'], bins=20, alpha=0.7, color='lightgreen')\n",
    "axes[0, 1].set_title('Distribution des Hauteurs')\n",
    "axes[0, 1].set_xlabel('Hauteur (pixels)')\n",
    "\n",
    "# Intensit√©s\n",
    "axes[0, 2].hist(df_props['mean_intensities'], bins=20, alpha=0.7, color='coral')\n",
    "axes[0, 2].set_title('Distribution des Intensit√©s Moyennes')\n",
    "axes[0, 2].set_xlabel('Intensit√© Moyenne')\n",
    "\n",
    "axes[1, 0].hist(df_props['std_intensities'], bins=20, alpha=0.7, color='gold')\n",
    "axes[1, 0].set_title('Distribution des √âcarts-Types d\\'Intensit√©')\n",
    "axes[1, 0].set_xlabel('√âcart-Type Intensit√©')\n",
    "\n",
    "# Tailles de fichiers\n",
    "axes[1, 1].hist(df_props['file_sizes'], bins=20, alpha=0.7, color='mediumpurple')\n",
    "axes[1, 1].set_title('Distribution des Tailles de Fichiers')\n",
    "axes[1, 1].set_xlabel('Taille (KB)')\n",
    "\n",
    "# Corr√©lation largeur/hauteur\n",
    "axes[1, 2].scatter(df_props['widths'], df_props['heights'], alpha=0.6, color='darkblue')\n",
    "axes[1, 2].set_title('Corr√©lation Largeur/Hauteur')\n",
    "axes[1, 2].set_xlabel('Largeur (pixels)')\n",
    "axes[1, 2].set_ylabel('Hauteur (pixels)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# D√©tection des formats non standards\n",
    "print(f\"\\nüìê Formats d'images d√©tect√©s:\")\n",
    "unique_dimensions = df_props.groupby(['widths', 'heights']).size().reset_index(name='count')\n",
    "print(unique_dimensions.sort_values('count', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e993d94f",
   "metadata": {},
   "source": [
    "## üîß Section 3: Data Preprocessing and Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe58354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 3.1 PR√âPARATION DES DONN√âES D'ENTRA√éNEMENT\n",
    "# ===================================\n",
    "\n",
    "print(\"üîÑ Pr√©paration du dataset pour l'entra√Ænement...\")\n",
    "\n",
    "# Cr√©ation d'un sous-ensemble √©quilibr√© pour des temps de traitement raisonnables\n",
    "print(\"‚öñÔ∏è Cr√©ation d'un sous-ensemble √©quilibr√©...\")\n",
    "balanced_paths, balanced_labels = create_balanced_subset(\n",
    "    image_paths, labels, max_per_class=1000\n",
    ")\n",
    "\n",
    "print(f\"üìä Dataset √©quilibr√©: {len(balanced_paths)} images\")\n",
    "\n",
    "# V√©rification de l'√©quilibrage\n",
    "balanced_distribution = pd.Series(balanced_labels).value_counts()\n",
    "print(\"üè∑Ô∏è Nouvelle distribution:\")\n",
    "print(balanced_distribution)\n",
    "\n",
    "# Encodage des labels\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(balanced_labels)\n",
    "\n",
    "print(f\"üî¢ Classes encod√©es: {dict(zip(label_encoder.classes_, range(len(label_encoder.classes_))))}\")\n",
    "\n",
    "# Division train/validation/test\n",
    "print(\"‚úÇÔ∏è Division du dataset...\")\n",
    "\n",
    "# Premi√®re division: train+val / test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    balanced_paths, encoded_labels,\n",
    "    test_size=TEST_SPLIT,\n",
    "    stratify=encoded_labels,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Seconde division: train / val\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=VALIDATION_SPLIT/(1-TEST_SPLIT),  # Ajustement pour avoir 20% du total\n",
    "    stratify=y_temp,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"üìä Train: {len(X_train)} images\")\n",
    "print(f\"üìä Validation: {len(X_val)} images\")\n",
    "print(f\"üìä Test: {len(X_test)} images\")\n",
    "\n",
    "# V√©rification de la stratification\n",
    "print(\"\\nüéØ V√©rification de la stratification:\")\n",
    "for split_name, y_split in [(\"Train\", y_train), (\"Val\", y_val), (\"Test\", y_test)]:\n",
    "    distribution = pd.Series(y_split).value_counts().sort_index()\n",
    "    percentages = (distribution / len(y_split) * 100).round(1)\n",
    "    print(f\"{split_name}: {dict(zip(label_encoder.classes_, percentages.values))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9eb66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 3.2 PIPELINE DE PR√âPROCESSING D'IMAGES\n",
    "# ===================================\n",
    "\n",
    "class ImagePreprocessor:\n",
    "    \"\"\"Pipeline de pr√©processing d'images optimis√©\"\"\"\n",
    "    \n",
    "    def __init__(self, target_size=(224, 224), normalize=True):\n",
    "        self.target_size = target_size\n",
    "        self.normalize = normalize\n",
    "    \n",
    "    def preprocess_single_image(self, image_path):\n",
    "        \"\"\"Pr√©processe une seule image\"\"\"\n",
    "        try:\n",
    "            # Chargement\n",
    "            img = cv2.imread(image_path)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Redimensionnement\n",
    "            img = cv2.resize(img, self.target_size)\n",
    "            \n",
    "            # Normalisation\n",
    "            if self.normalize:\n",
    "                img = img.astype(np.float32) / 255.0\n",
    "            \n",
    "            return img\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Erreur preprocessing {image_path}: {e}\")\n",
    "            return np.zeros((*self.target_size, 3), dtype=np.float32)\n",
    "    \n",
    "    def preprocess_batch(self, image_paths, batch_size=32):\n",
    "        \"\"\"Pr√©processe un lot d'images avec gestion m√©moire\"\"\"\n",
    "        n_images = len(image_paths)\n",
    "        n_batches = (n_images + batch_size - 1) // batch_size\n",
    "        \n",
    "        # Initialisation du tableau de sortie\n",
    "        images = np.zeros((n_images, *self.target_size, 3), dtype=np.float32)\n",
    "        \n",
    "        print(f\"üîÑ Preprocessing {n_images} images en {n_batches} batches...\")\n",
    "        \n",
    "        for i in range(n_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = min((i + 1) * batch_size, n_images)\n",
    "            \n",
    "            # Traitement du batch\n",
    "            for j, img_path in enumerate(image_paths[start_idx:end_idx]):\n",
    "                images[start_idx + j] = self.preprocess_single_image(img_path)\n",
    "            \n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"   üìä Batch {i+1}/{n_batches} termin√©\")\n",
    "        \n",
    "        return images\n",
    "\n",
    "# Initialisation du preprocessor\n",
    "preprocessor = ImagePreprocessor(target_size=IMG_SIZE, normalize=True)\n",
    "\n",
    "# Preprocessing des donn√©es\n",
    "print(\"üîÑ Preprocessing des images...\")\n",
    "\n",
    "X_train_processed = preprocessor.preprocess_batch(X_train)\n",
    "X_val_processed = preprocessor.preprocess_batch(X_val)\n",
    "X_test_processed = preprocessor.preprocess_batch(X_test)\n",
    "\n",
    "print(f\"‚úÖ Preprocessing termin√©!\")\n",
    "print(f\"üìä Shape train: {X_train_processed.shape}\")\n",
    "print(f\"üìä Shape validation: {X_val_processed.shape}\")\n",
    "print(f\"üìä Shape test: {X_test_processed.shape}\")\n",
    "\n",
    "# Conversion des labels en format cat√©goriel pour le deep learning\n",
    "y_train_categorical = to_categorical(y_train, num_classes=len(CLASSES))\n",
    "y_val_categorical = to_categorical(y_val, num_classes=len(CLASSES))\n",
    "y_test_categorical = to_categorical(y_test, num_classes=len(CLASSES))\n",
    "\n",
    "print(f\"üìä Shape labels train: {y_train_categorical.shape}\")\n",
    "\n",
    "# Visualisation d'un √©chantillon preprocess√©\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "fig.suptitle('Images Preprocess√©es - √âchantillons', fontsize=14, fontweight='bold')\n",
    "\n",
    "for i in range(8):\n",
    "    row = i // 4\n",
    "    col = i % 4\n",
    "    \n",
    "    # S√©lection d'une image al√©atoire\n",
    "    idx = np.random.randint(0, len(X_train_processed))\n",
    "    img = X_train_processed[idx]\n",
    "    label = label_encoder.inverse_transform([y_train[idx]])[0]\n",
    "    \n",
    "    axes[row, col].imshow(img)\n",
    "    axes[row, col].set_title(f'{label}', fontsize=10)\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6224d67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 3.3 DATA AUGMENTATION POUR LE DEEP LEARNING\n",
    "# ===================================\n",
    "\n",
    "# Configuration de l'augmentation de donn√©es\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=10,           # Rotation al√©atoire jusqu'√† 10¬∞\n",
    "    width_shift_range=0.1,       # D√©calage horizontal 10%\n",
    "    height_shift_range=0.1,      # D√©calage vertical 10%\n",
    "    zoom_range=0.1,              # Zoom al√©atoire 10%\n",
    "    horizontal_flip=True,        # Miroir horizontal\n",
    "    brightness_range=[0.8, 1.2], # Variation de luminosit√©\n",
    "    fill_mode='nearest'          # Mode de remplissage\n",
    ")\n",
    "\n",
    "# G√©n√©rateur pour validation (pas d'augmentation)\n",
    "val_datagen = ImageDataGenerator()\n",
    "\n",
    "print(\"üîÑ Configuration de l'augmentation de donn√©es\")\n",
    "\n",
    "# D√©monstration de l'augmentation\n",
    "def demonstrate_augmentation(X_sample, y_sample, n_augmentations=6):\n",
    "    \"\"\"Montre l'effet de l'augmentation sur quelques images\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, n_augmentations + 1, figsize=(20, 8))\n",
    "    fig.suptitle('D√©monstration de l\\'Augmentation de Donn√©es', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for row in range(2):\n",
    "        # Image originale\n",
    "        idx = np.random.randint(0, len(X_sample))\n",
    "        original_img = X_sample[idx]\n",
    "        label = label_encoder.inverse_transform([y_sample[idx]])[0]\n",
    "        \n",
    "        axes[row, 0].imshow(original_img)\n",
    "        axes[row, 0].set_title(f'Original\\n{label}')\n",
    "        axes[row, 0].axis('off')\n",
    "        \n",
    "        # Images augment√©es\n",
    "        img_batch = np.expand_dims(original_img, 0)\n",
    "        augmented_generator = train_datagen.flow(img_batch, batch_size=1)\n",
    "        \n",
    "        for col in range(1, n_augmentations + 1):\n",
    "            augmented_batch = next(augmented_generator)\n",
    "            augmented_img = augmented_batch[0]\n",
    "            \n",
    "            # Clip pour √©viter les valeurs hors [0,1]\n",
    "            augmented_img = np.clip(augmented_img, 0, 1)\n",
    "            \n",
    "            axes[row, col].imshow(augmented_img)\n",
    "            axes[row, col].set_title(f'Augment√©e {col}')\n",
    "            axes[row, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# D√©monstration\n",
    "print(\"üñºÔ∏è D√©monstration de l'augmentation...\")\n",
    "demonstrate_augmentation(X_train_processed, y_train, n_augmentations=5)\n",
    "\n",
    "print(\"‚úÖ Pipeline d'augmentation configur√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842af9b8",
   "metadata": {},
   "source": [
    "## üéØ Section 4: Baseline Models Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba007780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 4.1 EXTRACTION DE FEATURES POUR ML TRADITIONNEL\n",
    "# ===================================\n",
    "\n",
    "def extract_traditional_features(images):\n",
    "    \"\"\"\n",
    "    Extrait des features traditionnelles pour les mod√®les ML classiques\n",
    "    \n",
    "    Features extraites:\n",
    "    - Statistiques d'intensit√© (moyenne, std, min, max)\n",
    "    - Histogramme des niveaux de gris\n",
    "    - Features de texture (Local Binary Pattern simul√©)\n",
    "    \"\"\"\n",
    "    print(f\"üîç Extraction de features pour {len(images)} images...\")\n",
    "    \n",
    "    features_list = []\n",
    "    \n",
    "    for i, img in enumerate(images):\n",
    "        # Conversion en niveaux de gris\n",
    "        if len(img.shape) == 3:\n",
    "            gray_img = cv2.cvtColor((img * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "        else:\n",
    "            gray_img = (img * 255).astype(np.uint8)\n",
    "        \n",
    "        features = []\n",
    "        \n",
    "        # 1. Statistiques d'intensit√©\n",
    "        features.extend([\n",
    "            np.mean(gray_img),\n",
    "            np.std(gray_img),\n",
    "            np.min(gray_img),\n",
    "            np.max(gray_img),\n",
    "            np.percentile(gray_img, 25),\n",
    "            np.percentile(gray_img, 50),\n",
    "            np.percentile(gray_img, 75)\n",
    "        ])\n",
    "        \n",
    "        # 2. Histogramme (16 bins)\n",
    "        hist, _ = np.histogram(gray_img, bins=16, range=(0, 256))\n",
    "        hist = hist / np.sum(hist)  # Normalisation\n",
    "        features.extend(hist)\n",
    "        \n",
    "        # 3. Features de texture simples\n",
    "        # Gradient\n",
    "        grad_x = cv2.Sobel(gray_img, cv2.CV_64F, 1, 0, ksize=3)\n",
    "        grad_y = cv2.Sobel(gray_img, cv2.CV_64F, 0, 1, ksize=3)\n",
    "        gradient_magnitude = np.sqrt(grad_x**2 + grad_y**2)\n",
    "        \n",
    "        features.extend([\n",
    "            np.mean(gradient_magnitude),\n",
    "            np.std(gradient_magnitude)\n",
    "        ])\n",
    "        \n",
    "        # 4. Features g√©om√©triques (moments)\n",
    "        moments = cv2.moments(gray_img)\n",
    "        if moments['m00'] != 0:\n",
    "            cx = moments['m10'] / moments['m00']\n",
    "            cy = moments['m01'] / moments['m00']\n",
    "            features.extend([cx, cy])\n",
    "        else:\n",
    "            features.extend([0, 0])\n",
    "        \n",
    "        # 5. √ânergie et entropie\n",
    "        # Normalisation de l'image pour calculer l'entropie\n",
    "        normalized = gray_img / 255.0\n",
    "        entropy = -np.sum(normalized * np.log(normalized + 1e-10))\n",
    "        energy = np.sum(normalized ** 2)\n",
    "        \n",
    "        features.extend([entropy, energy])\n",
    "        \n",
    "        features_list.append(features)\n",
    "        \n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"   üìä {i+1}/{len(images)} images trait√©es\")\n",
    "    \n",
    "    feature_matrix = np.array(features_list)\n",
    "    print(f\"‚úÖ Features extraites: shape {feature_matrix.shape}\")\n",
    "    \n",
    "    return feature_matrix\n",
    "\n",
    "# Extraction des features\n",
    "print(\"üîç Extraction des features traditionnelles...\")\n",
    "X_train_features = extract_traditional_features(X_train_processed)\n",
    "X_val_features = extract_traditional_features(X_val_processed)\n",
    "X_test_features = extract_traditional_features(X_test_processed)\n",
    "\n",
    "# Normalisation des features\n",
    "scaler = StandardScaler()\n",
    "X_train_features_scaled = scaler.fit_transform(X_train_features)\n",
    "X_val_features_scaled = scaler.transform(X_val_features)\n",
    "X_test_features_scaled = scaler.transform(X_test_features)\n",
    "\n",
    "print(f\"üìä Features shape train: {X_train_features_scaled.shape}\")\n",
    "print(f\"üìä Features normalis√©es avec StandardScaler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10f7a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 4.2 MOD√àLES BASELINE TRADITIONNELS\n",
    "# ===================================\n",
    "\n",
    "# Dictionnaire pour stocker les r√©sultats\n",
    "baseline_results = {}\n",
    "\n",
    "def evaluate_model(model, X_train, X_val, y_train, y_val, model_name):\n",
    "    \"\"\"√âvalue un mod√®le et stocke les r√©sultats\"\"\"\n",
    "    \n",
    "    print(f\"\\nüîÑ Entra√Ænement de {model_name}...\")\n",
    "    \n",
    "    # Entra√Ænement\n",
    "    start_time = datetime.now()\n",
    "    model.fit(X_train, y_train)\n",
    "    training_time = datetime.now() - start_time\n",
    "    \n",
    "    # Pr√©dictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    \n",
    "    # M√©triques\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    \n",
    "    # Scores d√©taill√©s pour la validation\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_val, y_val_pred, average='weighted')\n",
    "    \n",
    "    # Stockage des r√©sultats\n",
    "    results = {\n",
    "        'model': model,\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'val_accuracy': val_accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'training_time': training_time.total_seconds(),\n",
    "        'y_val_pred': y_val_pred\n",
    "    }\n",
    "    \n",
    "    baseline_results[model_name] = results\n",
    "    \n",
    "    print(f\"‚úÖ {model_name}:\")\n",
    "    print(f\"   üìä Train Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"   üìä Val Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"   üìä F1-Score: {f1:.4f}\")\n",
    "    print(f\"   ‚è±Ô∏è Training Time: {training_time.total_seconds():.2f}s\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# D√©finition des mod√®les baseline\n",
    "baseline_models = {\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'SVM': SVC(\n",
    "        kernel='rbf',\n",
    "        C=1.0,\n",
    "        random_state=42,\n",
    "        probability=True  # Pour les probabilit√©s\n",
    "    ),\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'Decision Tree': DecisionTreeClassifier(\n",
    "        max_depth=15,\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "# Entra√Ænement et √©valuation de tous les mod√®les\n",
    "print(\"üöÄ Entra√Ænement des mod√®les baseline...\")\n",
    "\n",
    "for model_name, model in baseline_models.items():\n",
    "    evaluate_model(\n",
    "        model, \n",
    "        X_train_features_scaled, \n",
    "        X_val_features_scaled, \n",
    "        y_train, \n",
    "        y_val, \n",
    "        model_name\n",
    "    )\n",
    "\n",
    "print(\"\\n‚úÖ Tous les mod√®les baseline entra√Æn√©s!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3327299f",
   "metadata": {},
   "source": [
    "## üå≥ Section 5: Bagging Ensemble Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d4e0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 5.1 IMPL√âMENTATION DES M√âTHODES DE BAGGING\n",
    "# ===================================\n",
    "\n",
    "print(\"üå≥ Impl√©mentation des m√©thodes de Bagging...\")\n",
    "\n",
    "# Dictionnaire pour les r√©sultats de bagging\n",
    "bagging_results = {}\n",
    "\n",
    "# 5.1.1 Random Forest optimis√©\n",
    "print(\"\\nüîÑ Random Forest optimis√©...\")\n",
    "rf_optimized = RandomForestClassifier(\n",
    "    n_estimators=200,           # Plus d'arbres\n",
    "    max_depth=15,               # Profondeur contr√¥l√©e\n",
    "    min_samples_split=5,        # Contr√¥le overfitting\n",
    "    min_samples_leaf=2,         # Contr√¥le overfitting\n",
    "    max_features='sqrt',        # Features al√©atoires\n",
    "    bootstrap=True,             # Bagging activ√©\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "evaluate_model(\n",
    "    rf_optimized, \n",
    "    X_train_features_scaled, \n",
    "    X_val_features_scaled, \n",
    "    y_train, \n",
    "    y_val, \n",
    "    'Random Forest Optimized'\n",
    ")\n",
    "\n",
    "# 5.1.2 Extra Trees (Extremely Randomized Trees)\n",
    "print(\"\\nüîÑ Extra Trees...\")\n",
    "extra_trees = ExtraTreesClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=15,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    max_features='sqrt',\n",
    "    bootstrap=True,             # Bagging\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "evaluate_model(\n",
    "    extra_trees, \n",
    "    X_train_features_scaled, \n",
    "    X_val_features_scaled, \n",
    "    y_train, \n",
    "    y_val, \n",
    "    'Extra Trees'\n",
    ")\n",
    "\n",
    "# 5.1.3 Bagging avec diff√©rents mod√®les de base\n",
    "print(\"\\nüîÑ Bagging avec SVM...\")\n",
    "bagging_svm = BaggingClassifier(\n",
    "    estimator=SVC(kernel='rbf', probability=True, random_state=42),\n",
    "    n_estimators=50,            # Moins d'estimateurs car SVM est co√ªteux\n",
    "    max_samples=0.8,            # 80% des √©chantillons par mod√®le\n",
    "    max_features=0.8,           # 80% des features par mod√®le\n",
    "    bootstrap=True,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "evaluate_model(\n",
    "    bagging_svm, \n",
    "    X_train_features_scaled, \n",
    "    X_val_features_scaled, \n",
    "    y_train, \n",
    "    y_val, \n",
    "    'Bagging SVM'\n",
    ")\n",
    "\n",
    "# 5.1.4 Bagging avec Logistic Regression\n",
    "print(\"\\nüîÑ Bagging avec Logistic Regression...\")\n",
    "bagging_lr = BaggingClassifier(\n",
    "    estimator=LogisticRegression(max_iter=1000, random_state=42),\n",
    "    n_estimators=100,\n",
    "    max_samples=0.8,\n",
    "    max_features=0.8,\n",
    "    bootstrap=True,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "evaluate_model(\n",
    "    bagging_lr, \n",
    "    X_train_features_scaled, \n",
    "    X_val_features_scaled, \n",
    "    y_train, \n",
    "    y_val, \n",
    "    'Bagging Logistic Regression'\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ M√©thodes de Bagging entra√Æn√©es!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b56e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 5.2 ANALYSE DE L'IMPORTANCE DES FEATURES (BAGGING)\n",
    "# ===================================\n",
    "\n",
    "def analyze_feature_importance(model, feature_names, model_name, top_n=20):\n",
    "    \"\"\"Analyse l'importance des features pour les mod√®les tree-based\"\"\"\n",
    "    \n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "        \n",
    "        # Cr√©ation du DataFrame\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': importances\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        # Visualisation\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        top_features = feature_importance_df.head(top_n)\n",
    "        \n",
    "        sns.barplot(data=top_features, x='importance', y='feature', palette='viridis')\n",
    "        plt.title(f'Top {top_n} Features - {model_name}', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Importance')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return feature_importance_df\n",
    "    else:\n",
    "        print(f\"‚ùå {model_name} ne supporte pas l'analyse d'importance des features\")\n",
    "        return None\n",
    "\n",
    "# G√©n√©ration des noms de features\n",
    "feature_names = (\n",
    "    ['mean', 'std', 'min', 'max', 'q25', 'q50', 'q75'] +  # Statistiques\n",
    "    [f'hist_{i}' for i in range(16)] +                     # Histogramme\n",
    "    ['grad_mean', 'grad_std'] +                            # Gradient\n",
    "    ['cx', 'cy'] +                                         # Moments\n",
    "    ['entropy', 'energy']                                  # Texture\n",
    ")\n",
    "\n",
    "# Analyse pour Random Forest optimis√©\n",
    "print(\"üîç Analyse d'importance des features - Random Forest...\")\n",
    "rf_model = baseline_results['Random Forest Optimized']['model']\n",
    "rf_importance = analyze_feature_importance(rf_model, feature_names, 'Random Forest Optimized')\n",
    "\n",
    "# Analyse pour Extra Trees\n",
    "print(\"üîç Analyse d'importance des features - Extra Trees...\")\n",
    "et_model = baseline_results['Extra Trees']['model']\n",
    "et_importance = analyze_feature_importance(et_model, feature_names, 'Extra Trees')\n",
    "\n",
    "# Comparaison des importances\n",
    "if rf_importance is not None and et_importance is not None:\n",
    "    print(\"\\nüìä Comparaison des top 10 features:\")\n",
    "    comparison_df = pd.merge(\n",
    "        rf_importance.head(10)[['feature', 'importance']].rename(columns={'importance': 'RF_importance'}),\n",
    "        et_importance.head(10)[['feature', 'importance']].rename(columns={'importance': 'ET_importance'}),\n",
    "        on='feature',\n",
    "        how='outer'\n",
    "    ).fillna(0)\n",
    "    \n",
    "    print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c563364f",
   "metadata": {},
   "source": [
    "## üöÄ Section 6: Boosting Ensemble Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db563ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 6.1 IMPL√âMENTATION DES M√âTHODES DE BOOSTING\n",
    "# ===================================\n",
    "\n",
    "print(\"üöÄ Impl√©mentation des m√©thodes de Boosting...\")\n",
    "\n",
    "# 6.1.1 AdaBoost\n",
    "print(\"\\nüîÑ AdaBoost...\")\n",
    "ada_boost = AdaBoostClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=3, random_state=42),\n",
    "    n_estimators=100,\n",
    "    learning_rate=1.0,\n",
    "    algorithm='SAMME',  # Supporte multiclass\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "evaluate_model(\n",
    "    ada_boost, \n",
    "    X_train_features_scaled, \n",
    "    X_val_features_scaled, \n",
    "    y_train, \n",
    "    y_val, \n",
    "    'AdaBoost'\n",
    ")\n",
    "\n",
    "# 6.1.2 Gradient Boosting\n",
    "print(\"\\nüîÑ Gradient Boosting...\")\n",
    "gradient_boost = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    subsample=0.8,              # Stochastic gradient boosting\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "evaluate_model(\n",
    "    gradient_boost, \n",
    "    X_train_features_scaled, \n",
    "    X_val_features_scaled, \n",
    "    y_train, \n",
    "    y_val, \n",
    "    'Gradient Boosting'\n",
    ")\n",
    "\n",
    "# 6.1.3 XGBoost (si disponible)\n",
    "try:\n",
    "    print(\"\\nüîÑ XGBoost...\")\n",
    "    xgb_classifier = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=3,\n",
    "        min_child_weight=1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        eval_metric='mlogloss'  # Pour multiclass\n",
    "    )\n",
    "    \n",
    "    evaluate_model(\n",
    "        xgb_classifier, \n",
    "        X_train_features_scaled, \n",
    "        X_val_features_scaled, \n",
    "        y_train, \n",
    "        y_val, \n",
    "        'XGBoost'\n",
    "    )\n",
    "    \n",
    "except NameError:\n",
    "    print(\"‚ö†Ô∏è XGBoost non disponible - passage ignor√©\")\n",
    "\n",
    "print(\"\\n‚úÖ M√©thodes de Boosting entra√Æn√©es!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7f85b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 6.2 COMPARAISON BAGGING VS BOOSTING\n",
    "# ===================================\n",
    "\n",
    "def create_ensemble_comparison():\n",
    "    \"\"\"Compare les performances des m√©thodes d'ensemble\"\"\"\n",
    "    \n",
    "    # R√©cup√©ration des m√©triques pour comparaison\n",
    "    comparison_data = []\n",
    "    \n",
    "    for model_name, results in baseline_results.items():\n",
    "        comparison_data.append({\n",
    "            'Model': model_name,\n",
    "            'Type': get_model_type(model_name),\n",
    "            'Val_Accuracy': results['val_accuracy'],\n",
    "            'F1_Score': results['f1_score'],\n",
    "            'Training_Time': results['training_time']\n",
    "        })\n",
    "    \n",
    "    df_comparison = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    return df_comparison\n",
    "\n",
    "def get_model_type(model_name):\n",
    "    \"\"\"D√©termine le type de mod√®le\"\"\"\n",
    "    if any(keyword in model_name.lower() for keyword in ['random forest', 'extra trees', 'bagging']):\n",
    "        return 'Bagging'\n",
    "    elif any(keyword in model_name.lower() for keyword in ['ada', 'gradient', 'xgboost']):\n",
    "        return 'Boosting'\n",
    "    else:\n",
    "        return 'Baseline'\n",
    "\n",
    "# Cr√©ation de la comparaison\n",
    "df_comparison = create_ensemble_comparison()\n",
    "\n",
    "print(\"üìä Comparaison des m√©thodes d'ensemble:\")\n",
    "print(df_comparison.sort_values('Val_Accuracy', ascending=False))\n",
    "\n",
    "# Visualisation comparative\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Comparaison des M√©thodes d\\'Ensemble', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Accuracy par type\n",
    "sns.boxplot(data=df_comparison, x='Type', y='Val_Accuracy', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Accuracy par Type de M√©thode')\n",
    "axes[0, 0].set_ylabel('Validation Accuracy')\n",
    "\n",
    "# F1-Score par type\n",
    "sns.boxplot(data=df_comparison, x='Type', y='F1_Score', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('F1-Score par Type de M√©thode')\n",
    "axes[0, 1].set_ylabel('F1 Score')\n",
    "\n",
    "# Temps d'entra√Ænement par type\n",
    "sns.boxplot(data=df_comparison, x='Type', y='Training_Time', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Temps d\\'Entra√Ænement par Type')\n",
    "axes[1, 0].set_ylabel('Training Time (s)')\n",
    "\n",
    "# Correlation Accuracy vs Training Time\n",
    "sns.scatterplot(data=df_comparison, x='Training_Time', y='Val_Accuracy', \n",
    "                hue='Type', size='F1_Score', sizes=(50, 200), ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Accuracy vs Temps d\\'Entra√Ænement')\n",
    "axes[1, 1].set_xlabel('Training Time (s)')\n",
    "axes[1, 1].set_ylabel('Validation Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistiques par type\n",
    "print(\"\\nüìà Statistiques par type de m√©thode:\")\n",
    "type_stats = df_comparison.groupby('Type').agg({\n",
    "    'Val_Accuracy': ['mean', 'std', 'max'],\n",
    "    'F1_Score': ['mean', 'std', 'max'],\n",
    "    'Training_Time': ['mean', 'std', 'min']\n",
    "}).round(4)\n",
    "\n",
    "print(type_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2251d2",
   "metadata": {},
   "source": [
    "## üß† Section 7: Deep Learning Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034de566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 7.1 ARCHITECTURE CNN CUSTOM\n",
    "# ===================================\n",
    "\n",
    "def create_custom_cnn(input_shape=(224, 224, 3), num_classes=4):\n",
    "    \"\"\"\n",
    "    Cr√©e une architecture CNN personnalis√©e pour la classification d'images m√©dicales\n",
    "    \"\"\"\n",
    "    \n",
    "    model = models.Sequential([\n",
    "        # Bloc 1: Extraction de features de bas niveau\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape, padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Bloc 2: Features de niveau interm√©diaire\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Bloc 3: Features de haut niveau\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Bloc 4: Features complexes\n",
    "        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Classification\n",
    "        layers.GlobalAveragePooling2D(),  # Alternative √† Flatten + Dense\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Cr√©ation du mod√®le custom\n",
    "print(\"üß† Cr√©ation du mod√®le CNN personnalis√©...\")\n",
    "custom_cnn = create_custom_cnn(input_shape=(*IMG_SIZE, 3), num_classes=len(CLASSES))\n",
    "\n",
    "# Affichage de l'architecture\n",
    "print(\"üìã Architecture du mod√®le:\")\n",
    "custom_cnn.summary()\n",
    "\n",
    "# Visualisation de l'architecture\n",
    "tf.keras.utils.plot_model(\n",
    "    custom_cnn, \n",
    "    to_file=str(RESULTS_DIR / \"custom_cnn_architecture.png\"),\n",
    "    show_shapes=True, \n",
    "    show_layer_names=True,\n",
    "    rankdir='TB'\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Diagramme d'architecture sauvegard√©: {RESULTS_DIR / 'custom_cnn_architecture.png'}\")\n",
    "\n",
    "# Configuration de l'optimiseur et compilation\n",
    "optimizer = optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "custom_cnn.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', 'precision', 'recall']\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Mod√®le compil√© avec succ√®s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9422427d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 7.2 CALLBACKS ET MONITORING\n",
    "# ===================================\n",
    "\n",
    "# Configuration des callbacks\n",
    "def setup_callbacks(model_name, patience=10):\n",
    "    \"\"\"Configure les callbacks pour l'entra√Ænement\"\"\"\n",
    "    \n",
    "    # R√©pertoire pour sauvegarder les mod√®les\n",
    "    model_save_dir = MODELS_DIR / model_name\n",
    "    model_save_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    callbacks_list = [\n",
    "        # Early Stopping\n",
    "        EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=patience,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # R√©duction du learning rate\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # Sauvegarde du meilleur mod√®le\n",
    "        ModelCheckpoint(\n",
    "            filepath=str(model_save_dir / \"best_model.h5\"),\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            save_weights_only=False,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    return callbacks_list\n",
    "\n",
    "# Configuration des callbacks pour le mod√®le custom\n",
    "custom_callbacks = setup_callbacks(\"custom_cnn\", patience=10)\n",
    "\n",
    "print(\"‚úÖ Callbacks configur√©s:\")\n",
    "for callback in custom_callbacks:\n",
    "    print(f\"   üìã {callback.__class__.__name__}\")\n",
    "\n",
    "# Pr√©paration des g√©n√©rateurs de donn√©es\n",
    "print(\"\\nüîÑ Pr√©paration des g√©n√©rateurs de donn√©es...\")\n",
    "\n",
    "# G√©n√©rateur d'entra√Ænement avec augmentation\n",
    "train_generator = train_datagen.flow(\n",
    "    X_train_processed,\n",
    "    y_train_categorical,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# G√©n√©rateur de validation\n",
    "val_generator = val_datagen.flow(\n",
    "    X_val_processed,\n",
    "    y_val_categorical,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ G√©n√©rateurs pr√©par√©s:\")\n",
    "print(f\"   üìä Train: {len(train_generator)} batches de {BATCH_SIZE}\")\n",
    "print(f\"   üìä Validation: {len(val_generator)} batches de {BATCH_SIZE}\")\n",
    "\n",
    "# Fonction d'entra√Ænement\n",
    "def train_deep_model(model, model_name, train_gen, val_gen, epochs=EPOCHS, callbacks=None):\n",
    "    \"\"\"Entra√Æne un mod√®le de deep learning avec monitoring\"\"\"\n",
    "    \n",
    "    print(f\"\\nüöÄ Entra√Ænement de {model_name}...\")\n",
    "    print(f\"   üìä √âpoques: {epochs}\")\n",
    "    print(f\"   üìä Batch size: {BATCH_SIZE}\")\n",
    "    \n",
    "    # Entra√Ænement\n",
    "    history = model.fit(\n",
    "        train_gen,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_gen,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Entra√Ænement de {model_name} termin√©!\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "print(\"üîß Environnement d'entra√Ænement Deep Learning configur√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94e73df",
   "metadata": {},
   "source": [
    "## üîÑ Section 8: Transfer Learning Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6fffbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 8.1 MOD√àLES PR√â-ENTRA√éN√âS AVEC TRANSFER LEARNING\n",
    "# ===================================\n",
    "\n",
    "def create_transfer_learning_model(base_model_name, input_shape=(224, 224, 3), num_classes=4, \n",
    "                                 freeze_base=True, trainable_layers=0):\n",
    "    \"\"\"\n",
    "    Cr√©e un mod√®le de transfer learning\n",
    "    \n",
    "    Args:\n",
    "        base_model_name: 'VGG16', 'ResNet50', 'EfficientNetB0', ou 'InceptionV3'\n",
    "        freeze_base: Si True, g√®le les couches du mod√®le de base\n",
    "        trainable_layers: Nombre de couches √† rendre entra√Ænables (depuis la fin)\n",
    "    \"\"\"\n",
    "    \n",
    "    # S√©lection du mod√®le de base\n",
    "    if base_model_name == 'VGG16':\n",
    "        base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    elif base_model_name == 'ResNet50':\n",
    "        base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    elif base_model_name == 'EfficientNetB0':\n",
    "        base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    elif base_model_name == 'InceptionV3':\n",
    "        base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    else:\n",
    "        raise ValueError(f\"Mod√®le {base_model_name} non support√©\")\n",
    "    \n",
    "    # Gel des couches\n",
    "    if freeze_base:\n",
    "        base_model.trainable = False\n",
    "    else:\n",
    "        # Rendre seulement les derni√®res couches entra√Ænables\n",
    "        if trainable_layers > 0:\n",
    "            base_model.trainable = True\n",
    "            for layer in base_model.layers[:-trainable_layers]:\n",
    "                layer.trainable = False\n",
    "    \n",
    "    # Construction du mod√®le complet\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Dictionnaire pour stocker les mod√®les de transfer learning\n",
    "transfer_models = {}\n",
    "\n",
    "print(\"üîÑ Cr√©ation des mod√®les de Transfer Learning...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71751b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 8.2 ENTRA√éNEMENT DES MOD√àLES DE TRANSFER LEARNING\n",
    "# ===================================\n",
    "\n",
    "# Configuration des mod√®les √† tester\n",
    "models_config = [\n",
    "    {'name': 'VGG16_frozen', 'base': 'VGG16', 'freeze': True, 'trainable': 0},\n",
    "    {'name': 'ResNet50_frozen', 'base': 'ResNet50', 'freeze': True, 'trainable': 0},\n",
    "    {'name': 'EfficientNetB0_frozen', 'base': 'EfficientNetB0', 'freeze': True, 'trainable': 0}\n",
    "]\n",
    "\n",
    "# Stockage des historiques d'entra√Ænement\n",
    "training_histories = {}\n",
    "\n",
    "for config in models_config:\n",
    "    print(f\"\\nüîÑ Configuration: {config['name']}\")\n",
    "    \n",
    "    try:\n",
    "        # Cr√©ation du mod√®le\n",
    "        model = create_transfer_learning_model(\n",
    "            base_model_name=config['base'],\n",
    "            input_shape=(*IMG_SIZE, 3),\n",
    "            num_classes=len(CLASSES),\n",
    "            freeze_base=config['freeze'],\n",
    "            trainable_layers=config['trainable']\n",
    "        )\n",
    "        \n",
    "        # Compilation\n",
    "        model.compile(\n",
    "            optimizer=optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy', 'precision', 'recall']\n",
    "        )\n",
    "        \n",
    "        # Stockage\n",
    "        transfer_models[config['name']] = model\n",
    "        \n",
    "        print(f\"‚úÖ {config['name']} cr√©√© et compil√©\")\n",
    "        print(f\"   üìä Param√®tres entra√Ænables: {model.count_params():,}\")\n",
    "        \n",
    "        # Affichage du r√©sum√© pour le premier mod√®le\n",
    "        if config['name'] == 'VGG16_frozen':\n",
    "            print(f\"\\nüìã Exemple d'architecture - {config['name']}:\")\n",
    "            model.summary()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur lors de la cr√©ation de {config['name']}: {e}\")\n",
    "\n",
    "print(f\"\\n‚úÖ {len(transfer_models)} mod√®les de transfer learning cr√©√©s\")\n",
    "\n",
    "# Entra√Ænement d'un mod√®le de d√©monstration (VGG16)\n",
    "if 'VGG16_frozen' in transfer_models:\n",
    "    print(\"\\nüöÄ Entra√Ænement de d√©monstration - VGG16 (epochs r√©duits)...\")\n",
    "    \n",
    "    # Callbacks pour la d√©mo\n",
    "    demo_callbacks = setup_callbacks(\"VGG16_frozen_demo\", patience=5)\n",
    "    \n",
    "    # Entra√Ænement avec moins d'√©poques pour la d√©mo\n",
    "    demo_history = train_deep_model(\n",
    "        transfer_models['VGG16_frozen'],\n",
    "        \"VGG16_frozen\",\n",
    "        train_generator,\n",
    "        val_generator,\n",
    "        epochs=10,  # R√©duit pour la d√©mo\n",
    "        callbacks=demo_callbacks\n",
    "    )\n",
    "    \n",
    "    training_histories['VGG16_frozen'] = demo_history\n",
    "    \n",
    "    print(\"‚úÖ Entra√Ænement de d√©monstration termin√©\")\n",
    "\n",
    "print(\"\\nüí° Note: Pour un entra√Ænement complet, ajustez le nombre d'√©poques selon vos ressources\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c897ed1",
   "metadata": {},
   "source": [
    "## ‚ö° Section 9: Fine-Tuning Pre-trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b71a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 9.1 STRAT√âGIES DE FINE-TUNING\n",
    "# ===================================\n",
    "\n",
    "def fine_tune_model(base_model_name, pretrained_model_path=None, \n",
    "                   unfreeze_layers=20, fine_tune_lr=1e-5):\n",
    "    \"\"\"\n",
    "    Impl√©mente le fine-tuning d'un mod√®le pr√©-entra√Æn√©\n",
    "    \n",
    "    Args:\n",
    "        base_model_name: Nom du mod√®le de base\n",
    "        pretrained_model_path: Chemin vers le mod√®le pr√©-entra√Æn√© (optionnel)\n",
    "        unfreeze_layers: Nombre de couches √† d√©geler pour le fine-tuning\n",
    "        fine_tune_lr: Learning rate r√©duit pour le fine-tuning\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"‚ö° Fine-tuning de {base_model_name}\")\n",
    "    \n",
    "    # Si un mod√®le pr√©-entra√Æn√© existe, le charger\n",
    "    if pretrained_model_path and os.path.exists(pretrained_model_path):\n",
    "        print(f\"üìÇ Chargement du mod√®le pr√©-entra√Æn√©: {pretrained_model_path}\")\n",
    "        model = keras.models.load_model(pretrained_model_path)\n",
    "    else:\n",
    "        # Cr√©er un nouveau mod√®le si pas de mod√®le pr√©-entra√Æn√©\n",
    "        print(\"üîß Cr√©ation d'un nouveau mod√®le pour le fine-tuning\")\n",
    "        model = create_transfer_learning_model(\n",
    "            base_model_name=base_model_name,\n",
    "            input_shape=(*IMG_SIZE, 3),\n",
    "            num_classes=len(CLASSES),\n",
    "            freeze_base=False,\n",
    "            trainable_layers=unfreeze_layers\n",
    "        )\n",
    "    \n",
    "    # Phase 1: D√©gel progressif des couches\n",
    "    print(f\"üîì D√©gel des {unfreeze_layers} derni√®res couches du mod√®le de base\")\n",
    "    \n",
    "    # Identification du mod√®le de base (premi√®re couche)\n",
    "    base_model = model.layers[0]\n",
    "    \n",
    "    # Gel de toutes les couches d'abord\n",
    "    base_model.trainable = True\n",
    "    \n",
    "    # Gel des premi√®res couches, d√©gel des derni√®res\n",
    "    total_layers = len(base_model.layers)\n",
    "    freeze_until = total_layers - unfreeze_layers\n",
    "    \n",
    "    for i, layer in enumerate(base_model.layers):\n",
    "        if i < freeze_until:\n",
    "            layer.trainable = False\n",
    "        else:\n",
    "            layer.trainable = True\n",
    "    \n",
    "    print(f\"   üìä Couches totales: {total_layers}\")\n",
    "    print(f\"   üîí Couches gel√©es: {freeze_until}\")\n",
    "    print(f\"   üîì Couches entra√Ænables: {unfreeze_layers}\")\n",
    "    \n",
    "    # Recompilation avec learning rate r√©duit\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=fine_tune_lr),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy', 'precision', 'recall']\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Mod√®le recompil√© avec learning rate: {fine_tune_lr}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# D√©monstration du fine-tuning sur VGG16\n",
    "print(\"‚ö° D√©monstration de Fine-Tuning - VGG16\")\n",
    "\n",
    "# Strat√©gies de fine-tuning √† tester\n",
    "fine_tuning_strategies = [\n",
    "    {\n",
    "        'name': 'VGG16_fine_tuned_conservative',\n",
    "        'base': 'VGG16',\n",
    "        'unfreeze_layers': 10,\n",
    "        'lr': 1e-5,\n",
    "        'description': 'Fine-tuning conservateur - 10 derni√®res couches'\n",
    "    },\n",
    "    {\n",
    "        'name': 'VGG16_fine_tuned_aggressive', \n",
    "        'base': 'VGG16',\n",
    "        'unfreeze_layers': 20,\n",
    "        'lr': 5e-6,\n",
    "        'description': 'Fine-tuning agressif - 20 derni√®res couches'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Stockage des mod√®les fine-tun√©s\n",
    "fine_tuned_models = {}\n",
    "\n",
    "for strategy in fine_tuning_strategies:\n",
    "    print(f\"\\nüîÑ {strategy['description']}\")\n",
    "    \n",
    "    try:\n",
    "        # Cr√©ation du mod√®le fine-tun√©\n",
    "        ft_model = fine_tune_model(\n",
    "            base_model_name=strategy['base'],\n",
    "            unfreeze_layers=strategy['unfreeze_layers'],\n",
    "            fine_tune_lr=strategy['lr']\n",
    "        )\n",
    "        \n",
    "        fine_tuned_models[strategy['name']] = ft_model\n",
    "        \n",
    "        # Affichage des informations sur l'entra√Ænabilit√©\n",
    "        trainable_count = sum([1 for layer in ft_model.layers for sublayer in layer.layers if hasattr(sublayer, 'trainable') and sublayer.trainable])\n",
    "        total_count = sum([1 for layer in ft_model.layers for sublayer in layer.layers if hasattr(sublayer, 'trainable')])\n",
    "        \n",
    "        print(f\"   üìä Couches entra√Ænables: {trainable_count}/{total_count}\")\n",
    "        print(f\"   üìä Param√®tres entra√Ænables: {ft_model.count_params():,}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur lors du fine-tuning de {strategy['name']}: {e}\")\n",
    "\n",
    "print(f\"\\n‚úÖ {len(fine_tuned_models)} mod√®les fine-tun√©s cr√©√©s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22841f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 9.2 LEARNING RATE SCHEDULING AVANC√â\n",
    "# ===================================\n",
    "\n",
    "def create_advanced_callbacks(model_name, strategy_type=\"fine_tuning\"):\n",
    "    \"\"\"\n",
    "    Cr√©e des callbacks avanc√©s pour le fine-tuning\n",
    "    \"\"\"\n",
    "    \n",
    "    model_save_dir = MODELS_DIR / f\"{model_name}_{strategy_type}\"\n",
    "    model_save_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Learning rate scheduler personnalis√©\n",
    "    def lr_schedule(epoch, lr):\n",
    "        \"\"\"Planification du learning rate\"\"\"\n",
    "        if epoch < 5:\n",
    "            return lr\n",
    "        elif epoch < 15:\n",
    "            return lr * 0.9\n",
    "        else:\n",
    "            return lr * 0.95\n",
    "    \n",
    "    callbacks_list = [\n",
    "        # Early Stopping plus patient pour le fine-tuning\n",
    "        EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=15,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # R√©duction automatique du learning rate\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.3,\n",
    "            patience=7,\n",
    "            min_lr=1e-8,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # Learning rate scheduler personnalis√©\n",
    "        keras.callbacks.LearningRateScheduler(lr_schedule, verbose=1),\n",
    "        \n",
    "        # Sauvegarde des meilleurs mod√®les\n",
    "        ModelCheckpoint(\n",
    "            filepath=str(model_save_dir / \"best_model.h5\"),\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            save_weights_only=False,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # Sauvegarde des checkpoints\n",
    "        ModelCheckpoint(\n",
    "            filepath=str(model_save_dir / \"checkpoint_epoch_{epoch:02d}.h5\"),\n",
    "            save_freq='epoch',\n",
    "            save_weights_only=True,\n",
    "            verbose=0\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    return callbacks_list\n",
    "\n",
    "# D√©monstration d'entra√Ænement avec fine-tuning (version r√©duite)\n",
    "if 'VGG16_fine_tuned_conservative' in fine_tuned_models:\n",
    "    print(\"üöÄ D√©monstration de Fine-Tuning avec callbacks avanc√©s...\")\n",
    "    \n",
    "    # Configuration des callbacks avanc√©s\n",
    "    ft_callbacks = create_advanced_callbacks(\"VGG16\", \"fine_tuning_demo\")\n",
    "    \n",
    "    # Entra√Ænement avec param√®tres ajust√©s pour le fine-tuning\n",
    "    print(\"üìä Configuration pour Fine-Tuning:\")\n",
    "    print(\"   - Learning rate r√©duit\")\n",
    "    print(\"   - Patience augment√©e\")\n",
    "    print(\"   - Callbacks avanc√©s\")\n",
    "    print(\"   - Epochs r√©duits pour d√©mo\")\n",
    "    \n",
    "    # Note: Dans un sc√©nario r√©el, vous entra√Æneriez ici le mod√®le\n",
    "    print(\"\\nüí° Note: Dans un entra√Ænement complet, vous ex√©cuteriez:\")\n",
    "    print(\"   ft_history = train_deep_model(model, 'VGG16_fine_tuned', train_gen, val_gen, epochs=30)\")\n",
    "    \n",
    "    print(\"‚úÖ Configuration de fine-tuning pr√©par√©e\")\n",
    "\n",
    "print(\"\\nüîß Pipeline de Fine-Tuning configur√© avec succ√®s!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4678b161",
   "metadata": {},
   "source": [
    "## ü§ù Section 10: Ensemble of Deep Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3c681b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 10.1 ENSEMBLE VOTING POUR DEEP LEARNING\n",
    "# ===================================\n",
    "\n",
    "class DeepLearningEnsemble:\n",
    "    \"\"\"\n",
    "    Classe pour cr√©er des ensembles de mod√®les de deep learning\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, models_dict, ensemble_method='soft_voting'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            models_dict: Dictionnaire {nom: mod√®le} des mod√®les √† ensembler\n",
    "            ensemble_method: 'soft_voting', 'hard_voting', ou 'stacking'\n",
    "        \"\"\"\n",
    "        self.models = models_dict\n",
    "        self.ensemble_method = ensemble_method\n",
    "        self.model_names = list(models_dict.keys())\n",
    "        \n",
    "    def predict_ensemble(self, X, return_individual=False):\n",
    "        \"\"\"\n",
    "        Fait des pr√©dictions avec l'ensemble\n",
    "        \"\"\"\n",
    "        individual_predictions = {}\n",
    "        all_predictions = []\n",
    "        \n",
    "        print(f\"üîÑ Pr√©diction avec {len(self.models)} mod√®les...\")\n",
    "        \n",
    "        # Pr√©dictions individuelles\n",
    "        for name, model in self.models.items():\n",
    "            try:\n",
    "                pred = model.predict(X, verbose=0)\n",
    "                individual_predictions[name] = pred\n",
    "                all_predictions.append(pred)\n",
    "                print(f\"   ‚úÖ {name}: {pred.shape}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Erreur avec {name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not all_predictions:\n",
    "            raise ValueError(\"Aucune pr√©diction r√©ussie\")\n",
    "        \n",
    "        # Ensemble des pr√©dictions\n",
    "        all_predictions = np.array(all_predictions)\n",
    "        \n",
    "        if self.ensemble_method == 'soft_voting':\n",
    "            # Moyenne des probabilit√©s\n",
    "            ensemble_pred = np.mean(all_predictions, axis=0)\n",
    "        elif self.ensemble_method == 'hard_voting':\n",
    "            # Vote majoritaire\n",
    "            individual_classes = [np.argmax(pred, axis=1) for pred in all_predictions]\n",
    "            ensemble_classes = []\n",
    "            for i in range(len(X)):\n",
    "                votes = [pred[i] for pred in individual_classes]\n",
    "                ensemble_classes.append(max(set(votes), key=votes.count))\n",
    "            \n",
    "            # Conversion en format one-hot\n",
    "            ensemble_pred = to_categorical(ensemble_classes, num_classes=len(CLASSES))\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"M√©thode d'ensemble {self.ensemble_method} non support√©e\")\n",
    "        \n",
    "        if return_individual:\n",
    "            return ensemble_pred, individual_predictions\n",
    "        return ensemble_pred\n",
    "    \n",
    "    def evaluate_ensemble(self, X, y_true, return_individual=False):\n",
    "        \"\"\"\n",
    "        √âvalue les performances de l'ensemble\n",
    "        \"\"\"\n",
    "        print(f\"üìä √âvaluation de l'ensemble ({self.ensemble_method})...\")\n",
    "        \n",
    "        # Pr√©dictions\n",
    "        if return_individual:\n",
    "            ensemble_pred, individual_preds = self.predict_ensemble(X, return_individual=True)\n",
    "        else:\n",
    "            ensemble_pred = self.predict_ensemble(X)\n",
    "            individual_preds = None\n",
    "        \n",
    "        # Conversion en classes pour les m√©triques\n",
    "        y_true_classes = np.argmax(y_true, axis=1)\n",
    "        ensemble_classes = np.argmax(ensemble_pred, axis=1)\n",
    "        \n",
    "        # M√©triques de l'ensemble\n",
    "        accuracy = accuracy_score(y_true_classes, ensemble_classes)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            y_true_classes, ensemble_classes, average='weighted'\n",
    "        )\n",
    "        \n",
    "        results = {\n",
    "            'ensemble_accuracy': accuracy,\n",
    "            'ensemble_precision': precision,\n",
    "            'ensemble_recall': recall,\n",
    "            'ensemble_f1': f1,\n",
    "            'ensemble_predictions': ensemble_pred\n",
    "        }\n",
    "        \n",
    "        # √âvaluation individuelle si demand√©e\n",
    "        if return_individual and individual_preds:\n",
    "            individual_results = {}\n",
    "            for name, pred in individual_preds.items():\n",
    "                pred_classes = np.argmax(pred, axis=1)\n",
    "                ind_acc = accuracy_score(y_true_classes, pred_classes)\n",
    "                individual_results[name] = {\n",
    "                    'accuracy': ind_acc,\n",
    "                    'predictions': pred\n",
    "                }\n",
    "            results['individual_results'] = individual_results\n",
    "        \n",
    "        return results\n",
    "\n",
    "# D√©monstration d'ensemble avec les mod√®les disponibles\n",
    "print(\"ü§ù Cr√©ation d'ensembles de mod√®les Deep Learning...\")\n",
    "\n",
    "# V√©rification des mod√®les disponibles pour l'ensemble\n",
    "available_models = {}\n",
    "\n",
    "# Ajout des mod√®les de transfer learning si disponibles\n",
    "for model_name, model in transfer_models.items():\n",
    "    available_models[model_name] = model\n",
    "\n",
    "# Ajout des mod√®les fine-tun√©s si disponibles  \n",
    "for model_name, model in fine_tuned_models.items():\n",
    "    available_models[model_name] = model\n",
    "\n",
    "print(f\"üìä Mod√®les disponibles pour l'ensemble: {list(available_models.keys())}\")\n",
    "\n",
    "if len(available_models) >= 2:\n",
    "    # Cr√©ation d'ensembles avec diff√©rentes m√©thodes\n",
    "    ensemble_configs = [\n",
    "        {'method': 'soft_voting', 'description': 'Moyenne des probabilit√©s'},\n",
    "        {'method': 'hard_voting', 'description': 'Vote majoritaire'}\n",
    "    ]\n",
    "    \n",
    "    ensembles = {}\n",
    "    \n",
    "    for config in ensemble_configs:\n",
    "        ensemble_name = f\"DL_Ensemble_{config['method']}\"\n",
    "        ensemble = DeepLearningEnsemble(\n",
    "            models_dict=available_models,\n",
    "            ensemble_method=config['method']\n",
    "        )\n",
    "        ensembles[ensemble_name] = ensemble\n",
    "        \n",
    "        print(f\"‚úÖ {ensemble_name} cr√©√©: {config['description']}\")\n",
    "    \n",
    "    print(f\"\\nüéØ {len(ensembles)} ensembles de Deep Learning configur√©s\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Pas assez de mod√®les entra√Æn√©s pour cr√©er un ensemble\")\n",
    "    print(\"   Entra√Ænez d'abord plusieurs mod√®les de transfer learning\")\n",
    "\n",
    "print(\"\\nüí° Note: L'√©valuation compl√®te des ensembles n√©cessite des mod√®les enti√®rement entra√Æn√©s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf8f766",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Section 11: Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b43bf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 11.1 OPTIMISATION DES HYPERPARAM√àTRES - M√âTHODES TRADITIONNELLES\n",
    "# ===================================\n",
    "\n",
    "print(\"‚öôÔ∏è Optimisation des hyperparam√®tres pour les m√©thodes d'ensemble...\")\n",
    "\n",
    "# Configuration des espaces de recherche\n",
    "param_grids = {\n",
    "    'RandomForest': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 15, 20, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['sqrt', 'log2', None]\n",
    "    },\n",
    "    \n",
    "    'XGBoost': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'min_child_weight': [1, 3, 5],\n",
    "        'subsample': [0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "    } if 'xgb' in globals() else {},\n",
    "    \n",
    "    'GradientBoosting': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'subsample': [0.8, 0.9, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "def optimize_hyperparameters(model_class, param_grid, X_train, y_train, \n",
    "                           cv=3, scoring='accuracy', n_jobs=-1, verbose=True):\n",
    "    \"\"\"\n",
    "    Optimise les hyperparam√®tres avec GridSearchCV\n",
    "    \"\"\"\n",
    "    if not param_grid:\n",
    "        print(\"‚ö†Ô∏è Grille de param√®tres vide - optimisation ignor√©e\")\n",
    "        return None, None\n",
    "    \n",
    "    print(f\"üîç Optimisation avec {len(param_grid)} param√®tres...\")\n",
    "    print(f\"   üìä Cross-validation: {cv} folds\")\n",
    "    print(f\"   üìä M√©trique: {scoring}\")\n",
    "    \n",
    "    # Configuration de la recherche\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model_class,\n",
    "        param_grid=param_grid,\n",
    "        cv=cv,\n",
    "        scoring=scoring,\n",
    "        n_jobs=n_jobs,\n",
    "        return_train_score=True,\n",
    "        verbose=1 if verbose else 0\n",
    "    )\n",
    "    \n",
    "    # Ex√©cution de la recherche\n",
    "    start_time = datetime.now()\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    optimization_time = datetime.now() - start_time\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"‚úÖ Optimisation termin√©e en {optimization_time.total_seconds():.2f}s\")\n",
    "        print(f\"üéØ Meilleur score: {grid_search.best_score_:.4f}\")\n",
    "        print(f\"‚öôÔ∏è Meilleurs param√®tres: {grid_search.best_params_}\")\n",
    "    \n",
    "    return grid_search.best_estimator_, grid_search.cv_results_\n",
    "\n",
    "# Stockage des r√©sultats d'optimisation\n",
    "optimization_results = {}\n",
    "\n",
    "# Optimisation pour Random Forest\n",
    "print(\"\\nüå≥ Optimisation Random Forest...\")\n",
    "if 'RandomForest' in param_grids:\n",
    "    # Grille r√©duite pour la d√©mo\n",
    "    rf_demo_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [10, 15],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'max_features': ['sqrt', None]\n",
    "    }\n",
    "    \n",
    "    best_rf, rf_cv_results = optimize_hyperparameters(\n",
    "        RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "        rf_demo_grid,\n",
    "        X_train_features_scaled,\n",
    "        y_train,\n",
    "        cv=3,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    if best_rf:\n",
    "        optimization_results['RandomForest'] = {\n",
    "            'best_model': best_rf,\n",
    "            'cv_results': rf_cv_results\n",
    "        }\n",
    "\n",
    "# Optimisation pour Gradient Boosting\n",
    "print(\"\\nüöÄ Optimisation Gradient Boosting...\")\n",
    "gb_demo_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'learning_rate': [0.1, 0.2],\n",
    "    'max_depth': [3, 5]\n",
    "}\n",
    "\n",
    "best_gb, gb_cv_results = optimize_hyperparameters(\n",
    "    GradientBoostingClassifier(random_state=42),\n",
    "    gb_demo_grid,\n",
    "    X_train_features_scaled,\n",
    "    y_train,\n",
    "    cv=3,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "if best_gb:\n",
    "    optimization_results['GradientBoosting'] = {\n",
    "        'best_model': best_gb,\n",
    "        'cv_results': gb_cv_results\n",
    "    }\n",
    "\n",
    "print(f\"\\n‚úÖ {len(optimization_results)} mod√®les optimis√©s\")\n",
    "\n",
    "# √âvaluation des mod√®les optimis√©s\n",
    "print(\"\\nüìä √âvaluation des mod√®les optimis√©s...\")\n",
    "for model_name, results in optimization_results.items():\n",
    "    model = results['best_model']\n",
    "    \n",
    "    # √âvaluation sur validation\n",
    "    val_pred = model.predict(X_val_features_scaled)\n",
    "    val_accuracy = accuracy_score(y_val, val_pred)\n",
    "    \n",
    "    print(f\"üéØ {model_name} optimis√©:\")\n",
    "    print(f\"   üìä Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    # Comparaison avec le mod√®le baseline\n",
    "    if model_name in baseline_results:\n",
    "        baseline_acc = baseline_results[model_name]['val_accuracy']\n",
    "        improvement = val_accuracy - baseline_acc\n",
    "        print(f\"   üìà Am√©lioration: {improvement:+.4f}\")\n",
    "    \n",
    "    # Stockage dans baseline_results pour comparaison\n",
    "    baseline_results[f\"{model_name}_optimized\"] = {\n",
    "        'model': model,\n",
    "        'val_accuracy': val_accuracy,\n",
    "        'y_val_pred': val_pred\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf697ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 11.2 RECHERCHE AL√âATOIRE ET ANALYSE DES HYPERPARAM√àTRES\n",
    "# ===================================\n",
    "\n",
    "def randomized_hyperparameter_search(model_class, param_distributions, \n",
    "                                    X_train, y_train, n_iter=20, cv=3):\n",
    "    \"\"\"\n",
    "    Recherche al√©atoire d'hyperparam√®tres avec RandomizedSearchCV\n",
    "    \"\"\"\n",
    "    print(f\"üé≤ Recherche al√©atoire avec {n_iter} it√©rations...\")\n",
    "    \n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=model_class,\n",
    "        param_distributions=param_distributions,\n",
    "        n_iter=n_iter,\n",
    "        cv=cv,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        return_train_score=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    random_search.fit(X_train, y_train)\n",
    "    search_time = datetime.now() - start_time\n",
    "    \n",
    "    print(f\"‚úÖ Recherche termin√©e en {search_time.total_seconds():.2f}s\")\n",
    "    print(f\"üéØ Meilleur score: {random_search.best_score_:.4f}\")\n",
    "    print(f\"‚öôÔ∏è Meilleurs param√®tres: {random_search.best_params_}\")\n",
    "    \n",
    "    return random_search.best_estimator_, random_search.cv_results_\n",
    "\n",
    "# Distributions pour la recherche al√©atoire\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "rf_param_distributions = {\n",
    "    'n_estimators': randint(50, 300),\n",
    "    'max_depth': [5, 10, 15, 20, None],\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 10),\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Recherche al√©atoire pour Random Forest\n",
    "print(\"\\nüé≤ Recherche al√©atoire - Random Forest...\")\n",
    "best_rf_random, rf_random_results = randomized_hyperparameter_search(\n",
    "    RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    rf_param_distributions,\n",
    "    X_train_features_scaled,\n",
    "    y_train,\n",
    "    n_iter=10,  # R√©duit pour la d√©mo\n",
    "    cv=3\n",
    ")\n",
    "\n",
    "# Analyse des r√©sultats de recherche\n",
    "def analyze_hyperparameter_results(cv_results, param_name, model_name):\n",
    "    \"\"\"\n",
    "    Analyse l'impact d'un hyperparam√®tre sur les performances\n",
    "    \"\"\"\n",
    "    if f'param_{param_name}' not in cv_results:\n",
    "        print(f\"‚ö†Ô∏è Param√®tre {param_name} non trouv√© dans les r√©sultats\")\n",
    "        return\n",
    "    \n",
    "    # Extraction des donn√©es\n",
    "    param_values = cv_results[f'param_{param_name}']\n",
    "    mean_scores = cv_results['mean_test_score']\n",
    "    \n",
    "    # Cr√©ation du DataFrame pour l'analyse\n",
    "    df_analysis = pd.DataFrame({\n",
    "        'param_value': param_values,\n",
    "        'mean_score': mean_scores\n",
    "    })\n",
    "    \n",
    "    # Gestion des valeurs None\n",
    "    df_analysis = df_analysis[df_analysis['param_value'].notna()]\n",
    "    \n",
    "    if len(df_analysis) == 0:\n",
    "        print(f\"‚ö†Ô∏è Pas de donn√©es valides pour {param_name}\")\n",
    "        return\n",
    "    \n",
    "    # Visualisation\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    if df_analysis['param_value'].dtype == 'object':\n",
    "        # Param√®tre cat√©goriel\n",
    "        df_grouped = df_analysis.groupby('param_value')['mean_score'].agg(['mean', 'std']).reset_index()\n",
    "        plt.bar(range(len(df_grouped)), df_grouped['mean'], \n",
    "                yerr=df_grouped['std'], alpha=0.7, capsize=5)\n",
    "        plt.xticks(range(len(df_grouped)), df_grouped['param_value'], rotation=45)\n",
    "    else:\n",
    "        # Param√®tre num√©rique\n",
    "        plt.scatter(df_analysis['param_value'], df_analysis['mean_score'], alpha=0.6)\n",
    "        plt.xlabel(param_name)\n",
    "    \n",
    "    plt.title(f'Impact de {param_name} sur les performances - {model_name}')\n",
    "    plt.ylabel('Score de validation')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Analyse des r√©sultats de Random Forest\n",
    "if rf_random_results:\n",
    "    print(\"\\nüìä Analyse des hyperparam√®tres - Random Forest...\")\n",
    "    \n",
    "    # Analyse de diff√©rents param√®tres\n",
    "    important_params = ['n_estimators', 'max_depth', 'min_samples_split']\n",
    "    \n",
    "    for param in important_params:\n",
    "        analyze_hyperparameter_results(rf_random_results, param, 'Random Forest')\n",
    "\n",
    "# Comparaison des m√©thodes d'optimisation\n",
    "print(\"\\nüìà Comparaison des m√©thodes d'optimisation:\")\n",
    "\n",
    "comparison_data = []\n",
    "\n",
    "# Baseline\n",
    "for model_name in ['Random Forest', 'Gradient Boosting']:\n",
    "    if model_name in baseline_results:\n",
    "        comparison_data.append({\n",
    "            'Model': model_name,\n",
    "            'Method': 'Baseline',\n",
    "            'Accuracy': baseline_results[model_name]['val_accuracy']\n",
    "        })\n",
    "\n",
    "# Optimis√©s\n",
    "for model_name in ['RandomForest', 'GradientBoosting']:\n",
    "    if f\"{model_name}_optimized\" in baseline_results:\n",
    "        comparison_data.append({\n",
    "            'Model': model_name,\n",
    "            'Method': 'Grid Search',\n",
    "            'Accuracy': baseline_results[f\"{model_name}_optimized\"]['val_accuracy']\n",
    "        })\n",
    "\n",
    "# Random Search (Random Forest seulement)\n",
    "if best_rf_random:\n",
    "    rf_random_acc = accuracy_score(y_val, best_rf_random.predict(X_val_features_scaled))\n",
    "    comparison_data.append({\n",
    "        'Model': 'RandomForest',\n",
    "        'Method': 'Random Search',\n",
    "        'Accuracy': rf_random_acc\n",
    "    })\n",
    "\n",
    "if comparison_data:\n",
    "    df_optimization_comparison = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    # Visualisation\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=df_optimization_comparison, x='Model', y='Accuracy', hue='Method')\n",
    "    plt.title('Comparaison des M√©thodes d\\'Optimisation')\n",
    "    plt.ylabel('Validation Accuracy')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üìä R√©sultats de comparaison:\")\n",
    "    print(df_optimization_comparison)\n",
    "\n",
    "print(\"\\n‚úÖ Optimisation des hyperparam√®tres termin√©e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b28b1d",
   "metadata": {},
   "source": [
    "## üìä Section 12: Model Evaluation and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ef334d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 12.1 M√âTRIQUES AVANC√âES ET MATRICES DE CONFUSION\n",
    "# ===================================\n",
    "\n",
    "def comprehensive_model_evaluation(y_true, y_pred, model_name, class_names):\n",
    "    \"\"\"\n",
    "    √âvaluation compl√®te d'un mod√®le avec m√©triques avanc√©es\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìä √âvaluation compl√®te - {model_name}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # M√©triques globales\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\"üéØ Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"üéØ Precision (weighted): {precision:.4f}\")\n",
    "    print(f\"üéØ Recall (weighted): {recall:.4f}\")\n",
    "    print(f\"üéØ F1-Score (weighted): {f1:.4f}\")\n",
    "    \n",
    "    # M√©triques par classe\n",
    "    print(f\"\\nüìã Rapport de classification:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "    \n",
    "    # Matrice de confusion\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Visualisation de la matrice de confusion\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(f'Matrice de Confusion - {model_name}')\n",
    "    plt.xlabel('Pr√©dictions')\n",
    "    plt.ylabel('Vraies √©tiquettes')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # M√©triques par classe d√©taill√©es\n",
    "    class_metrics = {}\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        class_precision = precision_recall_fscore_support(y_true, y_pred, labels=[i], average=None)[0][0]\n",
    "        class_recall = precision_recall_fscore_support(y_true, y_pred, labels=[i], average=None)[1][0]\n",
    "        class_f1 = precision_recall_fscore_support(y_true, y_pred, labels=[i], average=None)[2][0]\n",
    "        \n",
    "        class_metrics[class_name] = {\n",
    "            'precision': class_precision,\n",
    "            'recall': class_recall,\n",
    "            'f1_score': class_f1,\n",
    "            'support': support[i]\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'confusion_matrix': cm,\n",
    "        'class_metrics': class_metrics\n",
    "    }\n",
    "\n",
    "# √âvaluation de tous les mod√®les disponibles\n",
    "print(\"üìä √âvaluation compl√®te de tous les mod√®les...\")\n",
    "\n",
    "evaluation_results = {}\n",
    "\n",
    "# √âvaluation des mod√®les baseline\n",
    "for model_name, results in baseline_results.items():\n",
    "    if 'y_val_pred' in results:\n",
    "        eval_results = comprehensive_model_evaluation(\n",
    "            y_val, \n",
    "            results['y_val_pred'], \n",
    "            model_name, \n",
    "            CLASSES\n",
    "        )\n",
    "        evaluation_results[model_name] = eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c152e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 12.2 COMPARAISON GLOBALE DES MOD√àLES\n",
    "# ===================================\n",
    "\n",
    "def create_models_comparison_dashboard():\n",
    "    \"\"\"\n",
    "    Cr√©e un dashboard de comparaison des mod√®les\n",
    "    \"\"\"\n",
    "    if not evaluation_results:\n",
    "        print(\"‚ö†Ô∏è Aucun r√©sultat d'√©valuation disponible\")\n",
    "        return\n",
    "    \n",
    "    # Compilation des m√©triques\n",
    "    comparison_data = []\n",
    "    for model_name, results in evaluation_results.items():\n",
    "        comparison_data.append({\n",
    "            'Model': model_name,\n",
    "            'Accuracy': results['accuracy'],\n",
    "            'Precision': results['precision'],\n",
    "            'Recall': results['recall'],\n",
    "            'F1_Score': results['f1_score'],\n",
    "            'Type': get_model_type(model_name)\n",
    "        })\n",
    "    \n",
    "    df_comparison = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    # Tri par accuracy\n",
    "    df_comparison = df_comparison.sort_values('Accuracy', ascending=False)\n",
    "    \n",
    "    print(\"üèÜ Classement des mod√®les par Accuracy:\")\n",
    "    print(df_comparison[['Model', 'Accuracy', 'F1_Score', 'Type']].to_string(index=False))\n",
    "    \n",
    "    # Visualisations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Dashboard de Comparaison des Mod√®les', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Accuracy par mod√®le\n",
    "    df_sorted = df_comparison.sort_values('Accuracy', ascending=True)\n",
    "    bars = axes[0, 0].barh(range(len(df_sorted)), df_sorted['Accuracy'], \n",
    "                          color=plt.cm.viridis(np.linspace(0, 1, len(df_sorted))))\n",
    "    axes[0, 0].set_yticks(range(len(df_sorted)))\n",
    "    axes[0, 0].set_yticklabels(df_sorted['Model'], fontsize=8)\n",
    "    axes[0, 0].set_xlabel('Accuracy')\n",
    "    axes[0, 0].set_title('Accuracy par Mod√®le')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Ajout des valeurs sur les barres\n",
    "    for i, bar in enumerate(bars):\n",
    "        width = bar.get_width()\n",
    "        axes[0, 0].text(width + 0.001, bar.get_y() + bar.get_height()/2, \n",
    "                       f'{width:.3f}', ha='left', va='center', fontsize=8)\n",
    "    \n",
    "    # 2. F1-Score vs Accuracy\n",
    "    scatter = axes[0, 1].scatter(df_comparison['Accuracy'], df_comparison['F1_Score'], \n",
    "                                c=df_comparison.index, cmap='viridis', s=100, alpha=0.7)\n",
    "    axes[0, 1].set_xlabel('Accuracy')\n",
    "    axes[0, 1].set_ylabel('F1-Score')\n",
    "    axes[0, 1].set_title('F1-Score vs Accuracy')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Ligne diagonale de r√©f√©rence\n",
    "    min_val = min(df_comparison['Accuracy'].min(), df_comparison['F1_Score'].min())\n",
    "    max_val = max(df_comparison['Accuracy'].max(), df_comparison['F1_Score'].max())\n",
    "    axes[0, 1].plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.5)\n",
    "    \n",
    "    # 3. M√©triques par type de mod√®le\n",
    "    df_melted = df_comparison.melt(id_vars=['Model', 'Type'], \n",
    "                                  value_vars=['Accuracy', 'Precision', 'Recall', 'F1_Score'],\n",
    "                                  var_name='Metric', value_name='Score')\n",
    "    \n",
    "    sns.boxplot(data=df_melted, x='Type', y='Score', hue='Metric', ax=axes[1, 0])\n",
    "    axes[1, 0].set_title('Distribution des M√©triques par Type')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    axes[1, 0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # 4. Heatmap des corr√©lations entre m√©triques\n",
    "    metrics_corr = df_comparison[['Accuracy', 'Precision', 'Recall', 'F1_Score']].corr()\n",
    "    sns.heatmap(metrics_corr, annot=True, cmap='coolwarm', center=0, \n",
    "                square=True, ax=axes[1, 1])\n",
    "    axes[1, 1].set_title('Corr√©lations entre M√©triques')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return df_comparison\n",
    "\n",
    "# Cr√©ation du dashboard\n",
    "print(\"üìä Cr√©ation du dashboard de comparaison...\")\n",
    "comparison_df = create_models_comparison_dashboard()\n",
    "\n",
    "# Analyse des meilleures performances par cat√©gorie\n",
    "if comparison_df is not None and not comparison_df.empty:\n",
    "    print(\"\\nüèÜ Meilleurs mod√®les par cat√©gorie:\")\n",
    "    \n",
    "    categories = ['Baseline', 'Bagging', 'Boosting']\n",
    "    for category in categories:\n",
    "        cat_models = comparison_df[comparison_df['Type'] == category]\n",
    "        if not cat_models.empty:\n",
    "            best_model = cat_models.loc[cat_models['Accuracy'].idxmax()]\n",
    "            print(f\"{category}: {best_model['Model']} (Accuracy: {best_model['Accuracy']:.4f})\")\n",
    "    \n",
    "    # Mod√®le global champion\n",
    "    best_overall = comparison_df.loc[comparison_df['Accuracy'].idxmax()]\n",
    "    print(f\"\\nüëë Champion global: {best_overall['Model']}\")\n",
    "    print(f\"   üìä Accuracy: {best_overall['Accuracy']:.4f}\")\n",
    "    print(f\"   üìä F1-Score: {best_overall['F1_Score']:.4f}\")\n",
    "    print(f\"   üìä Type: {best_overall['Type']}\")\n",
    "\n",
    "print(\"\\n‚úÖ √âvaluation comparative termin√©e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19053bfd",
   "metadata": {},
   "source": [
    "## üé® Section 13: Prediction Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91181e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 13.1 VISUALISATION DES PR√âDICTIONS ET INTERPR√âTABILIT√â\n",
    "# ===================================\n",
    "\n",
    "def visualize_predictions_with_confidence(model, X_samples, y_true, sample_indices, \n",
    "                                        model_name, class_names, is_deep_learning=False):\n",
    "    \"\"\"\n",
    "    Visualise les pr√©dictions avec scores de confiance\n",
    "    \"\"\"\n",
    "    n_samples = len(sample_indices)\n",
    "    fig, axes = plt.subplots(2, n_samples//2, figsize=(16, 8))\n",
    "    fig.suptitle(f'Pr√©dictions et Confiance - {model_name}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    if is_deep_learning:\n",
    "        # Pour les mod√®les de deep learning\n",
    "        predictions_proba = model.predict(X_samples, verbose=0)\n",
    "        predictions = np.argmax(predictions_proba, axis=1)\n",
    "    else:\n",
    "        # Pour les mod√®les traditionnels avec features\n",
    "        X_features = extract_traditional_features(X_samples)\n",
    "        X_features_scaled = scaler.transform(X_features)\n",
    "        predictions_proba = model.predict_proba(X_features_scaled)\n",
    "        predictions = model.predict(X_features_scaled)\n",
    "    \n",
    "    for idx, sample_idx in enumerate(sample_indices):\n",
    "        row = idx // (n_samples//2)\n",
    "        col = idx % (n_samples//2)\n",
    "        \n",
    "        # Image\n",
    "        axes[row, col].imshow(X_samples[sample_idx])\n",
    "        \n",
    "        # Informations sur la pr√©diction\n",
    "        true_label = class_names[y_true[sample_idx]]\n",
    "        pred_label = class_names[predictions[sample_idx]]\n",
    "        confidence = np.max(predictions_proba[sample_idx])\n",
    "        \n",
    "        # Couleur selon la justesse de la pr√©diction\n",
    "        color = 'green' if true_label == pred_label else 'red'\n",
    "        \n",
    "        title = f'Vraie: {true_label}\\nPr√©d: {pred_label}\\nConf: {confidence:.3f}'\n",
    "        axes[row, col].set_title(title, color=color, fontsize=10)\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Graphique des scores de confiance par classe\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "    \n",
    "    for i, sample_idx in enumerate(sample_indices):\n",
    "        proba_scores = predictions_proba[sample_idx]\n",
    "        x_pos = np.arange(len(class_names)) + i * 0.1\n",
    "        \n",
    "        bars = ax.bar(x_pos, proba_scores, width=0.1, alpha=0.7, \n",
    "                     label=f'√âchantillon {sample_idx+1}')\n",
    "        \n",
    "        # Highlight de la classe pr√©dite\n",
    "        max_idx = np.argmax(proba_scores)\n",
    "        bars[max_idx].set_color('red')\n",
    "        bars[max_idx].set_alpha(1.0)\n",
    "    \n",
    "    ax.set_xlabel('Classes')\n",
    "    ax.set_ylabel('Score de Confiance')\n",
    "    ax.set_title(f'Scores de Confiance par Classe - {model_name}')\n",
    "    ax.set_xticks(np.arange(len(class_names)) + 0.2)\n",
    "    ax.set_xticklabels(class_names, rotation=45)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# D√©monstration avec le meilleur mod√®le traditionnel\n",
    "if baseline_results and X_val_processed is not None:\n",
    "    print(\"üé® Visualisation des pr√©dictions...\")\n",
    "    \n",
    "    # S√©lection d'√©chantillons pour la visualisation\n",
    "    n_viz_samples = 6\n",
    "    sample_indices = np.random.choice(len(X_val_processed), n_viz_samples, replace=False)\n",
    "    \n",
    "    # R√©cup√©ration du meilleur mod√®le traditionnel\n",
    "    best_traditional_model = None\n",
    "    best_traditional_name = \"\"\n",
    "    best_score = 0\n",
    "    \n",
    "    for model_name, results in baseline_results.items():\n",
    "        if 'val_accuracy' in results and results['val_accuracy'] > best_score:\n",
    "            best_score = results['val_accuracy']\n",
    "            best_traditional_model = results['model']\n",
    "            best_traditional_name = model_name\n",
    "    \n",
    "    if best_traditional_model:\n",
    "        print(f\"üìä Visualisation avec le meilleur mod√®le: {best_traditional_name}\")\n",
    "        \n",
    "        # √âchantillons pour la visualisation\n",
    "        X_viz = X_val_processed[sample_indices]\n",
    "        y_viz = y_val[sample_indices]\n",
    "        \n",
    "        # Visualisation\n",
    "        visualize_predictions_with_confidence(\n",
    "            best_traditional_model,\n",
    "            X_viz,\n",
    "            y_viz,\n",
    "            range(len(sample_indices)),\n",
    "            best_traditional_name,\n",
    "            CLASSES,\n",
    "            is_deep_learning=False\n",
    "        )\n",
    "    \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Aucun mod√®le traditionnel disponible pour la visualisation\")\n",
    "\n",
    "print(\"‚úÖ Visualisations cr√©√©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14da6041",
   "metadata": {},
   "source": [
    "## üéØ Conclusion et R√©capitulatif\n",
    "\n",
    "### üìä **R√©sum√© du Notebook**\n",
    "\n",
    "Ce notebook complet a d√©montr√© les techniques avanc√©es de machine learning pour la classification d'images m√©dicales COVID-19 :\n",
    "\n",
    "#### üîß **Techniques Impl√©ment√©es :**\n",
    "\n",
    "1. **üì¶ M√©thodes d'Ensemble - Bagging :**\n",
    "   - Random Forest optimis√©\n",
    "   - Extra Trees (Extremely Randomized Trees)\n",
    "   - Bagging avec SVM et Logistic Regression\n",
    "   - Analyse d'importance des features\n",
    "\n",
    "2. **üöÄ M√©thodes d'Ensemble - Boosting :**\n",
    "   - AdaBoost avec arbres de d√©cision\n",
    "   - Gradient Boosting Machine (GBM)\n",
    "   - XGBoost (si disponible)\n",
    "   - Comparaison des performances\n",
    "\n",
    "3. **üß† Deep Learning :**\n",
    "   - CNN personnalis√© from scratch\n",
    "   - Transfer Learning (VGG16, ResNet50, EfficientNetB0)\n",
    "   - Fine-Tuning avec strat√©gies avanc√©es\n",
    "   - Ensemble de mod√®les de deep learning\n",
    "\n",
    "4. **‚öôÔ∏è Optimisation :**\n",
    "   - Grid Search pour hyperparam√®tres\n",
    "   - Random Search alternatif\n",
    "   - Cross-validation stratifi√©e\n",
    "   - Learning rate scheduling\n",
    "\n",
    "5. **üìä √âvaluation :**\n",
    "   - M√©triques compl√®tes (Accuracy, Precision, Recall, F1)\n",
    "   - Matrices de confusion d√©taill√©es\n",
    "   - Comparaisons visuelles\n",
    "   - Dashboard de performances\n",
    "\n",
    "#### üí° **Points Cl√©s Appris :**\n",
    "\n",
    "- **Bagging** : R√©duit la variance, bon pour overfitting\n",
    "- **Boosting** : R√©duit le biais, s√©quentiel et puissant\n",
    "- **Transfer Learning** : Efficace pour datasets m√©dicaux limit√©s\n",
    "- **Fine-Tuning** : Am√©liore les performances avec attention aux learning rates\n",
    "- **Ensemble Deep Learning** : Combine les forces de diff√©rents mod√®les\n",
    "\n",
    "#### üõ†Ô∏è **Recommandations pour la Production :**\n",
    "\n",
    "1. **Entra√Ænement complet** : Utilisez plus d'√©poques (50-100) pour les mod√®les DL\n",
    "2. **Validation externe** : Testez sur un dataset externe pour validation\n",
    "3. **Monitoring** : Impl√©mentez un suivi des performances en temps r√©el\n",
    "4. **Explicabilit√©** : Ajoutez des techniques comme LIME ou GRAD-CAM\n",
    "5. **D√©ploiement** : Consid√©rez l'optimisation des mod√®les (quantization, pruning)\n",
    "\n",
    "### üéØ **Prochaines √âtapes Sugg√©r√©es :**\n",
    "\n",
    "1. **Augmentation de donn√©es avanc√©e** : Techniques sp√©cifiques au m√©dical\n",
    "2. **Architecture personnalis√©e** : CNN adapt√© aux radiographies\n",
    "3. **M√©ta-learning** : Apprentissage sur diff√©rents types d'images m√©dicales\n",
    "4. **Federated Learning** : Entra√Ænement distribu√© pr√©servant la confidentialit√©\n",
    "5. **Validation clinique** : Collaboration avec professionnels de sant√©\n",
    "\n",
    "---\n",
    "\n",
    "**üèÜ F√©licitations ! Vous avez maintenant une base solide pour les techniques avanc√©es de ML dans le domaine m√©dical.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
