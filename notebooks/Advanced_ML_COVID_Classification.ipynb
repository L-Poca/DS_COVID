{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9504d98d",
   "metadata": {},
   "source": [
    "# 🔬 Advanced Machine Learning for COVID-19 Chest X-Ray Classification\n",
    "\n",
    "## 📋 Comprehensive Guide: Ensemble Methods & Deep Learning Fine-Tuning\n",
    "\n",
    "**Auteurs**: Équipe DS_COVID  \n",
    "**Date**: 15 octobre 2025  \n",
    "**Branche**: ReVamp\n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 **Objectifs du notebook**\n",
    "\n",
    "Ce notebook présente une approche complète du machine learning pour la classification de radiographies pulmonaires COVID-19, couvrant:\n",
    "\n",
    "1. **🌳 Méthodes d'Ensemble** : Bagging, Boosting, Stacking\n",
    "2. **🧠 Deep Learning** : Transfer Learning & Fine-Tuning  \n",
    "3. **⚙️ Optimisation** : Hyperparamètres, Cross-validation\n",
    "4. **📊 Évaluation** : Métriques avancées, Visualisations\n",
    "\n",
    "### 📁 **Dataset**\n",
    "- **Source**: COVID-19_Radiography_Dataset\n",
    "- **Classes**: COVID, Normal, Lung_Opacity, Viral Pneumonia\n",
    "- **Type**: Images radiographiques pulmonaires\n",
    "\n",
    "---\n",
    "\n",
    "### 🗂️ **Structure du notebook**\n",
    "\n",
    "| Section | Technique | Description |\n",
    "|---------|-----------|-------------|\n",
    "| 1-3 | **Setup & EDA** | Chargement, exploration, preprocessing |\n",
    "| 4-6 | **Ensemble Methods** | Bagging, Boosting, comparaisons |\n",
    "| 7-10 | **Deep Learning** | CNN, Transfer Learning, Fine-Tuning |\n",
    "| 11-13 | **Optimization & Evaluation** | Tuning, métriques, visualisations |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e79303f",
   "metadata": {},
   "source": [
    "## 📦 Section 1: Setup and Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28eb542c",
   "metadata": {},
   "source": [
    "## 🚀 Version Google Colab\n",
    "\n",
    "**Pour utiliser ce notebook sur Google Colab, exécutez cette cellule au lieu de la cellule de configuration normale :**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418efab9",
   "metadata": {},
   "source": [
    "### 📋 Instructions pour Google Colab\n",
    "\n",
    "#### ✅ Ce qui change sur Colab vs Local :\n",
    "\n",
    "1. **📂 Gestion des données** :\n",
    "   - Local : Dataset dans `data/raw/COVID-19_Radiography_Dataset/`\n",
    "   - Colab : **Extraction automatique** d'`archive_covid.zip` depuis Drive\n",
    "\n",
    "2. **📦 Installation** :\n",
    "   - Local : `pip install -e .` fait une seule fois\n",
    "   - Colab : Installation automatique dans la cellule\n",
    "\n",
    "3. **🔧 Configuration** :\n",
    "   - Local : Chargement automatique `.env`\n",
    "   - Colab : Package ds-covid + extraction ZIP\n",
    "\n",
    "#### 🚀 Ce que fait automatiquement la cellule Colab :\n",
    "\n",
    "1. **📥 Clone le repo** : `git clone https://github.com/L-Poca/DS_COVID.git`\n",
    "2. **📦 Extrait archive_covid.zip** : Depuis `MyDrive/archive_covid.zip`\n",
    "3. **🔍 Recherche intelligente** : Trouve automatiquement le dossier COVID\n",
    "4. ** Installe les dépendances** : `pip install -r requirements.txt`\n",
    "5. **🔧 Installe le package** : `pip install -e .` \n",
    "6. **⚙️ Configure les chemins** : `DATA_DIR` pointe vers les données extraites\n",
    "7. **✅ Vérifie les données** : Compte les images par classe\n",
    "\n",
    "#### 💾 Votre fichier sur Drive :\n",
    "\n",
    "```\n",
    "Drive/\n",
    "├── MyDrive/\n",
    "│   └── archive_covid.zip  ← Votre fichier ZIP\n",
    "```\n",
    "\n",
    "**Le script fait automatiquement** :\n",
    "1. **Extrait** `archive_covid.zip` vers `/content/temp_covid_extract/`\n",
    "2. **Recherche** le dossier COVID (plusieurs patterns supportés)\n",
    "3. **Déplace** vers `/content/DS_COVID/data/raw/COVID-19_Radiography_Dataset/`\n",
    "4. **Nettoie** le dossier temporaire\n",
    "\n",
    "#### 🔍 Recherche intelligente :\n",
    "\n",
    "Le script cherche automatiquement :\n",
    "- `COVID-19_Radiography_Dataset/`\n",
    "- Dossiers contenant `*COVID*`\n",
    "- Dossiers contenant `*radiography*`\n",
    "- Dossiers contenant `*chest*`\n",
    "\n",
    "#### 💡 Avantages :\n",
    "\n",
    "- ✅ **Simple** : Juste déposer archive_covid.zip dans Drive\n",
    "- ✅ **Automatique** : Extraction et organisation automatiques\n",
    "- ✅ **Robuste** : Recherche intelligente de la structure\n",
    "- ✅ **Rapide** : Pas besoin de créer des dossiers manuellement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377bf6bb",
   "metadata": {},
   "source": [
    "### 🚀 Optimisations Colab Pro\n",
    "\n",
    "#### ⚡ Paramètres ambitieux pour GPU puissant :\n",
    "\n",
    "**🖼️ Images :**\n",
    "- `IMG_SIZE`: `(256, 256)` ← Puissance de 2 (optimal GPU)\n",
    "- `BATCH_SIZE`: `64` ← Plus gros batch (GPU T4/V100)\n",
    "\n",
    "**🎯 Training :**\n",
    "- `EPOCHS`: `100` ← Training long (temps illimité Colab Pro)\n",
    "- `MAX_IMAGES`: `5000` ← Dataset complet par classe\n",
    "\n",
    "**🤖 ML traditionnel :**\n",
    "- Random Forest : `500 estimateurs` (vs 200)\n",
    "- XGBoost : `300 estimateurs` (vs 100)  \n",
    "- Gradient Boosting : `300 estimateurs`\n",
    "- CV : `5 folds` (vs 3)\n",
    "\n",
    "**🧠 Deep Learning :**\n",
    "- Architectures : `EfficientNetB3`, `ResNet152V2`, `VGG19`, `DenseNet201`\n",
    "- Mixed precision : `float16` (accélération GPU)\n",
    "- Fine-tuning : `20 couches` (vs 10)\n",
    "- Data augmentation : Rotation, zoom, flip, brightness\n",
    "\n",
    "#### 💡 Pourquoi ces optimisations ?\n",
    "\n",
    "1. **GPU T4/V100** : Plus de mémoire et compute → batch size plus gros\n",
    "2. **Temps illimité** : Colab Pro permet training long → plus d'epochs\n",
    "3. **Puissance de 2** : `256x256` optimise les opérations GPU vs `224x224`\n",
    "4. **Mixed precision** : Accélération x1.5-2x sur GPU récents\n",
    "5. **Architectures avancées** : Plus performantes qu'EfficientNetB0/ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6652c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# CONFIGURATION POUR GOOGLE COLAB\n",
    "# ===================================\n",
    "# ⚠️ Exécutez cette cellule uniquement sur Google Colab\n",
    "# Pour l'environnement local, utilisez la cellule suivante\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Vérification si on est sur Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"🌐 Google Colab détecté\")\n",
    "    \n",
    "    # 1. Connexion à Google Drive (pour accéder aux données)\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # 2. Clonage du repository depuis GitHub avec la branche ReVamp\n",
    "    repo_exists = os.path.exists('/content/DS_COVID')\n",
    "    if not repo_exists:\n",
    "        print(\"📥 Clonage du repository (branche ReVamp)...\")\n",
    "        !git clone -b ReVamp https://github.com/L-Poca/DS_COVID.git /content/DS_COVID\n",
    "        print(\"✅ Repository cloné (branche ReVamp)\")\n",
    "    else:\n",
    "        print(\"✅ Repository déjà présent\")\n",
    "        # S'assurer qu'on est sur la bonne branche\n",
    "        os.chdir('/content/DS_COVID')\n",
    "        print(\"🔄 Vérification de la branche...\")\n",
    "        !git fetch origin\n",
    "        !git checkout ReVamp\n",
    "        !git pull origin ReVamp\n",
    "        print(\"✅ Branche ReVamp mise à jour\")\n",
    "    \n",
    "    # 3. Changement vers le dossier du projet\n",
    "    os.chdir('/content/DS_COVID')\n",
    "    print(f\"📁 Dossier courant: {os.getcwd()}\")\n",
    "    \n",
    "    # Vérification que pyproject.toml existe (preuve qu'on est sur ReVamp)\n",
    "    if os.path.exists('pyproject.toml'):\n",
    "        print(\"✅ pyproject.toml trouvé - Branche ReVamp confirmée\")\n",
    "    else:\n",
    "        print(\"⚠️ pyproject.toml manquant - Problème de branche!\")\n",
    "    \n",
    "    # 4. Extraction de archive_covid.zip depuis Google Drive\n",
    "    print(\"📁 Extraction de archive_covid.zip depuis Drive...\")\n",
    "    \n",
    "    # Chemin vers votre fichier ZIP sur Drive\n",
    "    zip_file_path = \"/content/drive/MyDrive/archive_covid.zip\"\n",
    "    \n",
    "    # Dossier de destination\n",
    "    local_data_dir = \"/content/DS_COVID/data/raw\"\n",
    "    local_data_path = f\"{local_data_dir}/COVID-19_Radiography_Dataset\"\n",
    "    \n",
    "    # Création du dossier data/raw\n",
    "    os.makedirs(local_data_dir, exist_ok=True)\n",
    "    \n",
    "    if os.path.exists(zip_file_path):\n",
    "        print(f\"✅ Archive trouvée: {zip_file_path}\")\n",
    "        \n",
    "        if not os.path.exists(local_data_path):\n",
    "            print(\"📦 Extraction de l'archive COVID...\")\n",
    "            import zipfile\n",
    "            import shutil\n",
    "            import glob\n",
    "            \n",
    "            # Extraction dans un dossier temporaire\n",
    "            temp_extract_path = \"/content/temp_covid_extract\"\n",
    "            \n",
    "            with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(temp_extract_path)\n",
    "                print(f\"✅ Archive extraite dans {temp_extract_path}\")\n",
    "            \n",
    "            # Recherche intelligente du dossier COVID\n",
    "            search_patterns = [\n",
    "                f\"{temp_extract_path}/**/COVID-19_Radiography_Dataset\",\n",
    "                f\"{temp_extract_path}/**/*COVID*\",\n",
    "                f\"{temp_extract_path}/**/*radiography*\",\n",
    "                f\"{temp_extract_path}/**/*chest*\"\n",
    "            ]\n",
    "            \n",
    "            covid_source = None\n",
    "            for pattern in search_patterns:\n",
    "                possible_dirs = glob.glob(pattern, recursive=True)\n",
    "                possible_dirs = [d for d in possible_dirs if os.path.isdir(d)]\n",
    "                if possible_dirs:\n",
    "                    covid_source = possible_dirs[0]\n",
    "                    break\n",
    "            \n",
    "            if covid_source:\n",
    "                print(f\"✅ Dossier COVID trouvé: {covid_source}\")\n",
    "                \n",
    "                # Vérification de la structure (doit contenir COVID, Normal, etc.)\n",
    "                subdirs = [d.name for d in Path(covid_source).iterdir() if d.is_dir()]\n",
    "                print(f\"   Sous-dossiers: {subdirs}\")\n",
    "                \n",
    "                # Déplacement vers la destination finale\n",
    "                if os.path.exists(local_data_path):\n",
    "                    shutil.rmtree(local_data_path)\n",
    "                shutil.move(covid_source, local_data_path)\n",
    "                \n",
    "                # Nettoyage du dossier temporaire  \n",
    "                shutil.rmtree(temp_extract_path)\n",
    "                print(f\"✅ Données COVID disponibles: {local_data_path}\")\n",
    "            else:\n",
    "                print(\"⚠️ Structure du ZIP non reconnue automatiquement\")\n",
    "                # Affichage du contenu pour debug\n",
    "                for root, dirs, files in os.walk(temp_extract_path):\n",
    "                    if dirs:\n",
    "                        print(f\"   📁 {root}: {dirs[:3]}...\")  # Premières 3 dossiers\n",
    "                        break\n",
    "                print(f\"   Vérifiez manuellement dans: {temp_extract_path}\")\n",
    "        else:\n",
    "            print(f\"✅ Données déjà extraites: {local_data_path}\")\n",
    "    else:\n",
    "        print(f\"❌ Archive non trouvée: {zip_file_path}\")\n",
    "        print(\"💡 Solutions possibles:\")\n",
    "        print(\"   1. Vérifiez que 'archive_covid.zip' est dans MyDrive/\")\n",
    "        print(\"   2. Ou modifiez zip_file_path avec le bon chemin\")\n",
    "        print(\"   3. Ou uploadez archive_covid.zip dans MyDrive/\")\n",
    "    \n",
    "    # 5. Installation des dépendances depuis requirements.txt\n",
    "    print(\"📦 Installation des dépendances...\")\n",
    "    !pip install -r requirements.txt\n",
    "    \n",
    "    # 6. Installation du package ds-covid en mode développement\n",
    "    print(\"📦 Installation du package ds-covid...\")\n",
    "    !pip install -e .\n",
    "    \n",
    "    # 7. Vérification de l'installation et configuration\n",
    "    try:\n",
    "        import ds_covid\n",
    "        from ds_covid import Settings\n",
    "        print(f\"✅ Package ds-covid v{ds_covid.__version__} installé avec succès\")\n",
    "        \n",
    "        # Configuration optimisée pour Colab Pro\n",
    "        settings = Settings()\n",
    "        \n",
    "        # Adaptation des chemins pour Colab\n",
    "        from pathlib import Path\n",
    "        PROJECT_ROOT = Path('/content/DS_COVID')\n",
    "        \n",
    "        # Vérification des données extraites\n",
    "        if os.path.exists(local_data_path):\n",
    "            # Structure normale: COVID-19_Radiography_Dataset/COVID-19_Radiography_Dataset/\n",
    "            inner_covid_path = Path(local_data_path) / 'COVID-19_Radiography_Dataset'\n",
    "            if inner_covid_path.exists():\n",
    "                DATA_DIR = inner_covid_path\n",
    "            else:\n",
    "                DATA_DIR = Path(local_data_path)\n",
    "        else:\n",
    "            DATA_DIR = Path(local_data_path)\n",
    "        \n",
    "        MODELS_DIR = PROJECT_ROOT / 'models'\n",
    "        RESULTS_DIR = PROJECT_ROOT / 'results'\n",
    "        \n",
    "        # Variables optimisées pour Colab Pro\n",
    "        BATCH_SIZE = settings.training.batch_size      # 64\n",
    "        EPOCHS = settings.training.epochs              # 100\n",
    "        LEARNING_RATE = settings.training.learning_rate\n",
    "        IMG_SIZE = settings.training.img_size          # (256, 256)\n",
    "        IMG_CHANNELS = settings.training.img_channels\n",
    "        TEST_SPLIT = settings.training.test_split\n",
    "        VALIDATION_SPLIT = settings.training.validation_split\n",
    "        RANDOM_SEED = settings.training.random_seed\n",
    "        \n",
    "        # Configuration avancée\n",
    "        MAX_IMAGES_PER_CLASS = settings.data.max_images_per_class\n",
    "        AUGMENTATION_PARAMS = settings.data.augmentation_params\n",
    "        DL_ARCHITECTURES = settings.deep_learning.architectures\n",
    "        MIXED_PRECISION = settings.deep_learning.mixed_precision\n",
    "        \n",
    "        # Classes\n",
    "        CLASSES = settings.data.class_names\n",
    "        CLASS_MAPPING = settings.data.class_mapping\n",
    "        NUM_CLASSES = settings.data.num_classes\n",
    "        \n",
    "        print(f\"✅ Configuration COLAB PRO:\")\n",
    "        print(f\"   - Données: {DATA_DIR}\")\n",
    "        print(f\"   - Classes: {CLASSES}\")\n",
    "        print(f\"   - Image size: {IMG_SIZE}\")\n",
    "        print(f\"   - Batch size: {BATCH_SIZE}\")\n",
    "        print(f\"   - Epochs: {EPOCHS}\")\n",
    "        \n",
    "    except ImportError as e:\n",
    "        print(f\"⚠️ Erreur package: {e}\")\n",
    "        # Configuration manuelle de fallback\n",
    "        from pathlib import Path\n",
    "        PROJECT_ROOT = Path('/content/DS_COVID')\n",
    "        DATA_DIR = Path(local_data_path)\n",
    "        MODELS_DIR = PROJECT_ROOT / 'models'\n",
    "        RESULTS_DIR = PROJECT_ROOT / 'results'\n",
    "        \n",
    "        BATCH_SIZE = 64\n",
    "        EPOCHS = 100\n",
    "        LEARNING_RATE = 0.001\n",
    "        IMG_SIZE = (256, 256)\n",
    "        IMG_CHANNELS = 3\n",
    "        TEST_SPLIT = 0.2\n",
    "        VALIDATION_SPLIT = 0.2\n",
    "        RANDOM_SEED = 42\n",
    "        \n",
    "        CLASSES = ['COVID', 'Lung_Opacity', 'Normal', 'Viral Pneumonia']\n",
    "        CLASS_MAPPING = {'COVID': 0, 'Lung_Opacity': 1, 'Normal': 2, 'Viral Pneumonia': 3}\n",
    "        NUM_CLASSES = len(CLASSES)\n",
    "    \n",
    "    # Création des dossiers\n",
    "    MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Vérification finale des données\n",
    "    if DATA_DIR.exists():\n",
    "        subdirs = [d.name for d in DATA_DIR.iterdir() if d.is_dir()]\n",
    "        print(f\"   - Dossiers trouvés: {subdirs}\")\n",
    "        \n",
    "        # Comptage rapide des images\n",
    "        total_images = 0\n",
    "        for subdir in subdirs[:4]:  # Premières 4 classes\n",
    "            subdir_path = DATA_DIR / subdir\n",
    "            if subdir_path.exists():\n",
    "                images = list(subdir_path.glob('*.png')) + list(subdir_path.glob('*.jpg'))\n",
    "                print(f\"     {subdir}: {len(images)} images\")\n",
    "                total_images += len(images)\n",
    "        print(f\"   - Total: ~{total_images} images\")\n",
    "    else:\n",
    "        print(f\"   - ⚠️ Dossier non accessible: {DATA_DIR}\")\n",
    "    \n",
    "else:\n",
    "    print(\"💻 Environnement local détecté - utilisez la cellule de configuration normale\")\n",
    "    \n",
    "# Imports communs (marchent partout)\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, VotingClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks, optimizers\n",
    "from tensorflow.keras.applications import VGG16, ResNet50, EfficientNetB0\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Configuration GPU\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    print(f\"✅ GPU détecté: {len(physical_devices)} device(s)\")\n",
    "else:\n",
    "    print(\"⚠️ Pas de GPU détecté, utilisation du CPU\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(f\"\\n📦 Versions (Colab):\")\n",
    "    print(f\"   - Python: {sys.version.split()[0]}\")\n",
    "    print(f\"   - Scikit-learn: {sklearn.__version__}\")\n",
    "    print(f\"   - TensorFlow: {tf.__version__}\")\n",
    "    print(f\"   - OpenCV: {cv2.__version__}\")\n",
    "    print(f\"   - NumPy: {np.__version__}\")\n",
    "    print(f\"   - Pandas: {pd.__version__}\")\n",
    "    print(\"\\n✅ Configuration Colab terminée - Données extraites depuis archive_covid.zip !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a320e36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration pour Google Colab avec archive_covid.zip\n",
    "import os\n",
    "import sys\n",
    "from google.colab import drive\n",
    "\n",
    "print(\"\udd27 CONFIGURATION GOOGLE COLAB PRO\")\n",
    "\n",
    "# Monter Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Cloner le repo si pas déjà fait\n",
    "if not os.path.exists('/content/DS_COVID'):\n",
    "    print(\"📥 Clonage du repository...\")\n",
    "    !git clone https://github.com/L-Poca/DS_COVID.git /content/DS_COVID\n",
    "\n",
    "# Aller dans le dossier du projet\n",
    "os.chdir('/content/DS_COVID')\n",
    "\n",
    "# Extraction intelligente de archive_covid.zip depuis Drive\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "def find_and_extract_covid_archive():\n",
    "    \"\"\"Trouve et extrait archive_covid.zip depuis Google Drive\"\"\"\n",
    "    possible_paths = [\n",
    "        '/content/drive/MyDrive/archive_covid.zip',\n",
    "        '/content/drive/My Drive/archive_covid.zip',\n",
    "        '/content/drive/MyDrive/COVID/archive_covid.zip',\n",
    "        '/content/drive/My Drive/COVID/archive_covid.zip'\n",
    "    ]\n",
    "    \n",
    "    for zip_path in possible_paths:\n",
    "        if os.path.exists(zip_path):\n",
    "            print(f\"📦 Archive trouvée: {zip_path}\")\n",
    "            \n",
    "            # Créer le dossier data/raw s'il n'existe pas\n",
    "            os.makedirs('/content/DS_COVID/data/raw', exist_ok=True)\n",
    "            \n",
    "            # Extraire l'archive\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall('/content/DS_COVID/data/raw/')\n",
    "            \n",
    "            # Chercher le dossier COVID-19_Radiography_Dataset\n",
    "            for root, dirs, files in os.walk('/content/DS_COVID/data/raw/'):\n",
    "                for dir_name in dirs:\n",
    "                    if 'COVID' in dir_name and 'Radiography' in dir_name:\n",
    "                        dataset_path = os.path.join(root, dir_name)\n",
    "                        print(f\"✅ Dataset trouvé: {dataset_path}\")\n",
    "                        return dataset_path\n",
    "            \n",
    "            print(\"⚠️ Dossier COVID-19_Radiography_Dataset non trouvé dans l'archive\")\n",
    "            return None\n",
    "    \n",
    "    print(\"❌ Archive archive_covid.zip non trouvée dans Drive\")\n",
    "    print(\"📂 Vérifiez que le fichier est dans MyDrive/\")\n",
    "    return None\n",
    "\n",
    "# Extraire les données\n",
    "dataset_path = find_and_extract_covid_archive()\n",
    "\n",
    "# Installation du package\n",
    "print(\"📦 Installation du package ds-covid...\")\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa8459f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Pas de GPU détecté, utilisation du CPU\n",
      "📦 Package ds-covid v0.1.0\n",
      "🔧 Chargement de la configuration depuis .env...\n",
      "📄 Fichier .env chargé: /home/cepa/DST/projet_DS/DS_COVID/.env\n",
      "✅ Configuration chargée depuis .env:\n",
      "   - PROJECT_ROOT: /home/cepa/DST/projet_DS/DS_COVID\n",
      "   - DATA_DIR: /home/cepa/DST/projet_DS/DS_COVID/data/raw/COVID-19_Radiography_Dataset/COVID-19_Radiography_Dataset\n",
      "   - MODELS_DIR: /home/cepa/DST/projet_DS/DS_COVID/models\n",
      "   - RESULTS_DIR: /home/cepa/DST/projet_DS/DS_COVID/reports\n",
      "\n",
      "📁 Vérification des chemins (.env):\n",
      "   ✅ PROJECT_ROOT: /home/cepa/DST/projet_DS/DS_COVID\n",
      "   ✅ DATA_DIR: /home/cepa/DST/projet_DS/DS_COVID/data/raw/COVID-19_Radiography_Dataset/COVID-19_Radiography_Dataset\n",
      "   ✅ MODELS_DIR: /home/cepa/DST/projet_DS/DS_COVID/models\n",
      "   ✅ RESULTS_DIR: /home/cepa/DST/projet_DS/DS_COVID/reports\n",
      "\n",
      "🎯 Configuration finale (depuis .env):\n",
      "   - Classes (4): ['COVID', 'Lung_Opacity', 'Normal', 'Viral Pneumonia']\n",
      "   - Mapping: {'COVID': 0, 'Lung_Opacity': 1, 'Normal': 2, 'Viral Pneumonia': 3}\n",
      "   - Image size: (224, 224)\n",
      "   - Batch size: 32\n",
      "   - Epochs: 50\n",
      "   - Learning rate: 0.001\n",
      "   - Random seed: 42\n",
      "\n",
      "📦 Versions:\n",
      "   - Python: 3.10.12\n",
      "   - ds-covid: 0.1.0\n",
      "   - Scikit-learn: 1.7.0\n",
      "   - TensorFlow: 2.19.0\n",
      "   - OpenCV: 4.11.0\n",
      "   - NumPy: 2.1.3\n",
      "   - Pandas: 2.3.0\n",
      "\n",
      "✅ Configuration .env chargée avec 4 classes - Prêt pour le ML/DL !\n"
     ]
    }
   ],
   "source": [
    "# ===================================\n",
    "# 1.1 CONFIGURATION LOCALE (VS Code / Environnement local)\n",
    "# ===================================\n",
    "# ⚠️ Cette cellule est pour l'environnement LOCAL avec le package ds-covid installé\n",
    "# Pour Google Colab, utilisez la cellule précédente\n",
    "\n",
    "# Configuration du package ds-covid (utilise automatiquement .env)\n",
    "from ds_covid import Settings, configure_package, __version__\n",
    "import ds_covid\n",
    "\n",
    "# Imports de base\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import json\n",
    "import sys\n",
    "\n",
    "# Configuration des warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning - Scikit-learn\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, VotingClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "\n",
    "# Deep Learning - TensorFlow/Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks, optimizers\n",
    "from tensorflow.keras.applications import VGG16, ResNet50, EfficientNetB0\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Configuration GPU\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    print(f\"✅ GPU détecté: {len(physical_devices)} device(s)\")\n",
    "else:\n",
    "    print(\"⚠️ Pas de GPU détecté, utilisation du CPU\")\n",
    "\n",
    "# Configuration automatique depuis .env\n",
    "print(f\"📦 Package ds-covid v{__version__}\")\n",
    "print(\"🔧 Chargement de la configuration depuis .env...\")\n",
    "\n",
    "# Le package charge automatiquement les settings depuis .env\n",
    "settings = Settings()\n",
    "\n",
    "# Affichage de la configuration chargée\n",
    "print(f\"✅ Configuration chargée depuis .env:\")\n",
    "print(f\"   - PROJECT_ROOT: {settings.project_root}\")\n",
    "print(f\"   - DATA_DIR: {settings.data_dir}\")\n",
    "print(f\"   - MODELS_DIR: {settings.models_dir}\")\n",
    "print(f\"   - RESULTS_DIR: {settings.results_dir}\")\n",
    "\n",
    "# Variables globales depuis les settings (.env)\n",
    "RANDOM_SEED = settings.training.random_seed\n",
    "BATCH_SIZE = settings.training.batch_size\n",
    "EPOCHS = settings.training.epochs\n",
    "LEARNING_RATE = settings.training.learning_rate\n",
    "IMG_SIZE = settings.training.img_size\n",
    "IMG_CHANNELS = settings.training.img_channels\n",
    "TEST_SPLIT = settings.training.test_split\n",
    "VALIDATION_SPLIT = settings.training.validation_split\n",
    "\n",
    "# Variables de données depuis .env\n",
    "MAX_IMAGES_PER_CLASS = settings.data.max_images_per_class\n",
    "\n",
    "# Chemins depuis .env\n",
    "PROJECT_ROOT = Path(settings.project_root)\n",
    "DATA_DIR = Path(settings.data_dir)\n",
    "MODELS_DIR = Path(settings.models_dir)\n",
    "RESULTS_DIR = Path(settings.results_dir)\n",
    "\n",
    "# Vérification que les chemins existent\n",
    "print(f\"\\n📁 Vérification des chemins (.env):\")\n",
    "for name, path in [('PROJECT_ROOT', PROJECT_ROOT), ('DATA_DIR', DATA_DIR), \n",
    "                   ('MODELS_DIR', MODELS_DIR), ('RESULTS_DIR', RESULTS_DIR)]:\n",
    "    if path.exists():\n",
    "        print(f\"   ✅ {name}: {path}\")\n",
    "    else:\n",
    "        print(f\"   ❌ {name}: {path} (sera créé si nécessaire)\")\n",
    "        if name in ['MODELS_DIR', 'RESULTS_DIR']:\n",
    "            path.mkdir(parents=True, exist_ok=True)\n",
    "            print(f\"   ✅ {name}: Créé avec succès\")\n",
    "\n",
    "# Classes du dataset COVID (depuis la configuration)\n",
    "CLASSES = settings.data.class_names\n",
    "CLASS_MAPPING = settings.data.class_mapping\n",
    "NUM_CLASSES = settings.data.num_classes\n",
    "\n",
    "# Résumé de la configuration\n",
    "print(f\"\\n🎯 Configuration finale (depuis .env):\")\n",
    "print(f\"   - Classes ({NUM_CLASSES}): {CLASSES}\")\n",
    "print(f\"   - Image size: {IMG_SIZE}\")\n",
    "print(f\"   - Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   - Epochs: {EPOCHS}\")\n",
    "print(f\"   - Max images/classe: {MAX_IMAGES_PER_CLASS}\")\n",
    "\n",
    "# Pour Colab Pro - Configuration optimisée\n",
    "print(\"\\n🚀 OPTIMISATIONS COLAB PRO ACTIVÉES\")\n",
    "print(\"✅ Mixed precision activée (float16)\")\n",
    "\n",
    "print(\"\\n🎯 PARAMÈTRES COLAB PRO:\")\n",
    "print(f\"   - Image size: {IMG_SIZE} ← Puissance de 2 (optimal GPU)\")\n",
    "print(f\"   - Batch size: {BATCH_SIZE} ← Plus gros (GPU puissant)\")\n",
    "print(f\"   - Epochs: {EPOCHS} ← Plus long (temps illimité)\")\n",
    "print(f\"   - Max images: {MAX_IMAGES_PER_CLASS} ← Dataset complet\")\n",
    "\n",
    "# Test mémoire GPU\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    # Activer mixed precision si GPU disponible\n",
    "    from tensorflow.keras import mixed_precision\n",
    "    mixed_precision.set_global_policy('mixed_float16')\n",
    "    print(\"✅ Mixed precision float16 activée\")\n",
    "\n",
    "print(\"🎉 Configuration terminée!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c863a121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/cepa/DST/projet_DS/DS_COVID/notebooks')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edfcb959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Variables d'environnement chargées depuis: /home/cepa/DST/projet_DS/DS_COVID/.env\n",
      "🔧 Chargement de la configuration centralisée...\n",
      "============================================================\n",
      "🔧 CONFIGURATION DU PROJET DS_COVID\n",
      "============================================================\n",
      "\n",
      "📁 CHEMINS:\n",
      "   ✅ project_root: /home/cepa/DST/projet_DS/DS_COVID\n",
      "   ✅ data_dir: /home/cepa/DST/projet_DS/DS_COVID/data/raw/COVID-19_Radiography_Dataset/COVID-19_Radiography_Dataset\n",
      "   ✅ models_dir: /home/cepa/DST/projet_DS/DS_COVID/models\n",
      "   ✅ results_dir: /home/cepa/DST/projet_DS/DS_COVID/reports\n",
      "   ✅ notebooks_dir: /home/cepa/DST/projet_DS/DS_COVID/notebooks\n",
      "\n",
      "🖼️ IMAGES:\n",
      "   📐 Taille: (224, 224)\n",
      "   🎨 Canaux: 3\n",
      "\n",
      "🎯 ENTRAÎNEMENT:\n",
      "   📊 Batch size: 32\n",
      "   🔄 Époques: 50\n",
      "   📈 Learning rate: 0.001\n",
      "\n",
      "🏷️ CLASSES:\n",
      "   📋 4 classes: COVID, Lung_Opacity, Normal, Viral Pneumonia\n",
      "============================================================\n",
      "📁 data_dir: /home/cepa/DST/projet_DS/DS_COVID/data/raw/COVID-19_Radiography_Dataset/COVID-19_Radiography_Dataset\n",
      "📁 models_dir: /home/cepa/DST/projet_DS/DS_COVID/models\n",
      "📁 results_dir: /home/cepa/DST/projet_DS/DS_COVID/reports\n",
      "📁 notebooks_dir: /home/cepa/DST/projet_DS/DS_COVID/notebooks\n",
      "💻 Configuration CPU\n",
      "✅ Environnement configuré\n",
      "\n",
      "✅ Configuration chargée depuis .env!\n",
      "📊 Paramètres principaux:\n",
      "   🖼️ Taille d'image: (224, 224)\n",
      "   📦 Batch size: 32\n",
      "   🔄 Époques: 50\n",
      "   🎯 Classes: 4 (COVID, Lung_Opacity, Normal, Viral Pneumonia)\n",
      "   📁 Répertoire de données: /home/cepa/DST/projet_DS/DS_COVID/data/raw/COVID-19_Radiography_Dataset/COVID-19_Radiography_Dataset\n",
      "🎲 Seed configuré: 42\n",
      "🔧 Environnement prêt avec configuration centralisée!\n"
     ]
    }
   ],
   "source": [
    "# ===================================\n",
    "# 1.2 CONFIGURATION CENTRALISÉE AVEC .ENV\n",
    "# ===================================\n",
    "\n",
    "# Import du gestionnaire de configuration (chemin relatif)\n",
    "# Détection automatique du chemin src depuis le notebook\n",
    "notebook_dir = Path.cwd() if 'notebooks' in str(Path.cwd()) else Path(__file__).parent\n",
    "project_root = notebook_dir.parent if notebook_dir.name == 'notebooks' else notebook_dir\n",
    "src_path = project_root / 'src'\n",
    "\n",
    "sys.path.append(str(src_path))\n",
    "from config import config_manager, get_config, setup_environment\n",
    "\n",
    "print(\"🔧 Chargement de la configuration centralisée...\")\n",
    "\n",
    "# Affichage du résumé de configuration\n",
    "config_manager.print_summary()\n",
    "\n",
    "# Création des répertoires nécessaires\n",
    "config_manager.create_directories()\n",
    "\n",
    "# Configuration de l'environnement\n",
    "setup_environment()\n",
    "\n",
    "# Récupération des variables de configuration\n",
    "PROJECT_ROOT = get_config('paths', 'project_root')\n",
    "DATA_DIR = get_config('paths', 'data_dir')\n",
    "MODELS_DIR = get_config('paths', 'models_dir')\n",
    "RESULTS_DIR = get_config('paths', 'results_dir')\n",
    "\n",
    "# Configuration d'images\n",
    "IMG_SIZE = get_config('image', 'img_size')\n",
    "IMG_CHANNELS = get_config('image', 'img_channels')\n",
    "\n",
    "# Paramètres d'entraînement depuis .env\n",
    "BATCH_SIZE = get_config('training', 'batch_size')\n",
    "EPOCHS = get_config('training', 'epochs')\n",
    "LEARNING_RATE = get_config('training', 'learning_rate')\n",
    "VALIDATION_SPLIT = get_config('training', 'validation_split')\n",
    "TEST_SPLIT = get_config('training', 'test_split')\n",
    "RANDOM_SEED = get_config('training', 'random_seed')\n",
    "\n",
    "# Classes depuis la configuration\n",
    "CLASSES = get_config('classes', 'class_names')\n",
    "CLASS_MAPPING = get_config('classes', 'class_mapping')\n",
    "\n",
    "print(\"\\n✅ Configuration chargée depuis .env!\")\n",
    "print(f\"📊 Paramètres principaux:\")\n",
    "print(f\"   🖼️ Taille d'image: {IMG_SIZE}\")\n",
    "print(f\"   📦 Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   🔄 Époques: {EPOCHS}\")\n",
    "print(f\"   🎯 Classes: {len(CLASSES)} ({', '.join(CLASSES)})\")\n",
    "print(f\"   📁 Répertoire de données: {DATA_DIR}\")\n",
    "\n",
    "# Configuration des seeds pour reproductibilité\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "print(f\"🎲 Seed configuré: {RANDOM_SEED}\")\n",
    "print(\"🔧 Environnement prêt avec configuration centralisée!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ff6661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 1.2 CONFIGURATION DES CHEMINS ET DATASET\n",
    "# ===================================\n",
    "\n",
    "# Chemins du projet\n",
    "PROJECT_ROOT = Path(\"/home/cepa/DST/projet_DS/DS_COVID\")\n",
    "DATA_DIR = PROJECT_ROOT / \"data\" / \"raw\" / \"COVID-19_Radiography_Dataset\" / \"COVID-19_Radiography_Dataset\"\n",
    "MODELS_DIR = PROJECT_ROOT / \"models\"\n",
    "RESULTS_DIR = PROJECT_ROOT / \"reports\"\n",
    "\n",
    "# Création des dossiers de sortie\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"📁 Répertoire du projet: {PROJECT_ROOT}\")\n",
    "print(f\"📁 Répertoire des données: {DATA_DIR}\")\n",
    "print(f\"📁 Répertoire des modèles: {MODELS_DIR}\")\n",
    "\n",
    "# Définition des classes\n",
    "CLASSES = [\"COVID\", \"Lung_Opacity\", \"Normal\", \"Viral Pneumonia\"]\n",
    "CLASS_MAPPING = {cls: idx for idx, cls in enumerate(CLASSES)}\n",
    "\n",
    "print(f\"🏷️ Classes détectées: {CLASSES}\")\n",
    "print(f\"🔢 Mapping des classes: {CLASS_MAPPING}\")\n",
    "\n",
    "# Paramètres globaux\n",
    "IMG_SIZE = (224, 224)  # Taille standard pour les modèles pré-entraînés\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.001\n",
    "VALIDATION_SPLIT = 0.2\n",
    "TEST_SPLIT = 0.2\n",
    "\n",
    "print(f\"⚙️ Taille d'image: {IMG_SIZE}\")\n",
    "print(f\"⚙️ Batch size: {BATCH_SIZE}\")\n",
    "print(f\"⚙️ Nombre d'époques: {EPOCHS}\")\n",
    "\n",
    "# Vérification de l'existence des données\n",
    "if DATA_DIR.exists():\n",
    "    print(\"✅ Répertoire de données trouvé\")\n",
    "    for class_name in CLASSES:\n",
    "        class_path = DATA_DIR / class_name / \"images\"\n",
    "        if class_path.exists():\n",
    "            n_images = len(list(class_path.glob(\"*.png\")))\n",
    "            print(f\"   📊 {class_name}: {n_images} images\")\n",
    "        else:\n",
    "            print(f\"   ❌ {class_name}: répertoire non trouvé\")\n",
    "else:\n",
    "    print(\"❌ Répertoire de données non trouvé!\")\n",
    "    print(f\"   Vérifiez le chemin: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14639c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 1.3 FONCTIONS UTILITAIRES POUR LE CHARGEMENT\n",
    "# ===================================\n",
    "\n",
    "def load_image_paths_and_labels(data_dir, classes):\n",
    "    \"\"\"\n",
    "    Charge les chemins des images et leurs labels\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (image_paths, labels, class_counts)\n",
    "    \"\"\"\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    class_counts = {}\n",
    "    \n",
    "    for class_name in classes:\n",
    "        class_dir = data_dir / class_name / \"images\"\n",
    "        if not class_dir.exists():\n",
    "            continue\n",
    "            \n",
    "        # Récupération des images\n",
    "        image_files = list(class_dir.glob(\"*.png\"))\n",
    "        class_counts[class_name] = len(image_files)\n",
    "        \n",
    "        # Ajout des chemins et labels\n",
    "        for img_path in image_files:\n",
    "            image_paths.append(str(img_path))\n",
    "            labels.append(class_name)\n",
    "    \n",
    "    return image_paths, labels, class_counts\n",
    "\n",
    "def load_and_preprocess_image(image_path, target_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Charge et préprocesse une image\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Chemin vers l'image\n",
    "        target_size (tuple): Taille cible (largeur, hauteur)\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Image préprocessée\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Chargement avec PIL\n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        # Redimensionnement\n",
    "        img = img.resize(target_size)\n",
    "        \n",
    "        # Conversion en array numpy\n",
    "        img_array = np.array(img)\n",
    "        \n",
    "        # Normalisation [0, 1]\n",
    "        img_array = img_array.astype(np.float32) / 255.0\n",
    "        \n",
    "        return img_array\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du chargement de {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_balanced_subset(image_paths, labels, max_per_class=500):\n",
    "    \"\"\"\n",
    "    Crée un sous-ensemble équilibré du dataset\n",
    "    \n",
    "    Args:\n",
    "        image_paths (list): Liste des chemins d'images\n",
    "        labels (list): Liste des labels\n",
    "        max_per_class (int): Nombre maximum d'images par classe\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (subset_paths, subset_labels)\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({'path': image_paths, 'label': labels})\n",
    "    \n",
    "    # Échantillonnage équilibré\n",
    "    balanced_df = df.groupby('label').apply(\n",
    "        lambda x: x.sample(n=min(len(x), max_per_class), random_state=42)\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    return balanced_df['path'].tolist(), balanced_df['label'].tolist()\n",
    "\n",
    "print(\"🔧 Fonctions utilitaires définies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f47e243",
   "metadata": {},
   "source": [
    "## 📊 Section 2: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723001d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 2.1 CHARGEMENT ET ANALYSE DE LA DISTRIBUTION\n",
    "# ===================================\n",
    "\n",
    "# Chargement des chemins et labels\n",
    "print(\"📂 Chargement des données...\")\n",
    "image_paths, labels, class_counts = load_image_paths_and_labels(DATA_DIR, CLASSES)\n",
    "\n",
    "print(f\"📊 Total d'images: {len(image_paths)}\")\n",
    "print(f\"📊 Total de labels: {len(labels)}\")\n",
    "\n",
    "# Création du DataFrame pour l'analyse\n",
    "df_analysis = pd.DataFrame({\n",
    "    'image_path': image_paths,\n",
    "    'label': labels\n",
    "})\n",
    "\n",
    "# Analyse de la distribution des classes\n",
    "print(\"\\n🏷️ Distribution des classes:\")\n",
    "class_distribution = df_analysis['label'].value_counts()\n",
    "print(class_distribution)\n",
    "\n",
    "# Visualisation de la distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Graphique en barres\n",
    "class_distribution.plot(kind='bar', ax=ax1, color='skyblue', alpha=0.8)\n",
    "ax1.set_title('Distribution des Classes', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Classes', fontsize=12)\n",
    "ax1.set_ylabel('Nombre d\\'images', fontsize=12)\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Graphique en secteurs\n",
    "ax2.pie(class_distribution.values, labels=class_distribution.index, autopct='%1.1f%%', \n",
    "        startangle=90, colors=plt.cm.Set3.colors)\n",
    "ax2.set_title('Répartition des Classes (%)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistiques détaillées\n",
    "print(\"\\n📈 Statistiques détaillées:\")\n",
    "total_images = len(image_paths)\n",
    "for class_name, count in class_distribution.items():\n",
    "    percentage = (count / total_images) * 100\n",
    "    print(f\"   {class_name}: {count} images ({percentage:.2f}%)\")\n",
    "\n",
    "# Détection du déséquilibre\n",
    "max_count = class_distribution.max()\n",
    "min_count = class_distribution.min()\n",
    "imbalance_ratio = max_count / min_count\n",
    "\n",
    "print(f\"\\n⚖️ Ratio de déséquilibre: {imbalance_ratio:.2f}\")\n",
    "if imbalance_ratio > 2:\n",
    "    print(\"⚠️  Dataset déséquilibré détecté - techniques de rééquilibrage recommandées\")\n",
    "else:\n",
    "    print(\"✅ Dataset relativement équilibré\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a686f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 2.2 VISUALISATION DES IMAGES REPRÉSENTATIVES\n",
    "# ===================================\n",
    "\n",
    "def visualize_sample_images(image_paths, labels, classes, n_samples=3):\n",
    "    \"\"\"\n",
    "    Visualise des échantillons d'images pour chaque classe\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(len(classes), n_samples, figsize=(15, 12))\n",
    "    fig.suptitle('Échantillons d\\'Images par Classe', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    df_viz = pd.DataFrame({'path': image_paths, 'label': labels})\n",
    "    \n",
    "    for i, class_name in enumerate(classes):\n",
    "        class_images = df_viz[df_viz['label'] == class_name]['path'].tolist()\n",
    "        \n",
    "        # Sélection aléatoire d'échantillons\n",
    "        samples = np.random.choice(class_images, min(n_samples, len(class_images)), replace=False)\n",
    "        \n",
    "        for j, img_path in enumerate(samples):\n",
    "            try:\n",
    "                # Chargement de l'image\n",
    "                img = cv2.imread(img_path)\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                # Affichage\n",
    "                axes[i, j].imshow(img, cmap='gray' if len(img.shape) == 2 else None)\n",
    "                axes[i, j].set_title(f'{class_name}', fontsize=10)\n",
    "                axes[i, j].axis('off')\n",
    "                \n",
    "            except Exception as e:\n",
    "                axes[i, j].text(0.5, 0.5, f'Erreur:\\n{str(e)}', \n",
    "                              ha='center', va='center', transform=axes[i, j].transAxes)\n",
    "                axes[i, j].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualisation des échantillons\n",
    "print(\"🖼️ Visualisation des échantillons d'images...\")\n",
    "visualize_sample_images(image_paths, labels, CLASSES, n_samples=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2d10ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 2.3 ANALYSE DES PROPRIÉTÉS DES IMAGES\n",
    "# ===================================\n",
    "\n",
    "def analyze_image_properties(image_paths, sample_size=100):\n",
    "    \"\"\"\n",
    "    Analyse les propriétés des images (taille, intensité, etc.)\n",
    "    \"\"\"\n",
    "    print(f\"🔍 Analyse des propriétés sur un échantillon de {sample_size} images...\")\n",
    "    \n",
    "    # Échantillonnage aléatoire\n",
    "    sample_paths = np.random.choice(image_paths, min(sample_size, len(image_paths)), replace=False)\n",
    "    \n",
    "    properties = {\n",
    "        'widths': [],\n",
    "        'heights': [],\n",
    "        'channels': [],\n",
    "        'mean_intensities': [],\n",
    "        'std_intensities': [],\n",
    "        'file_sizes': []\n",
    "    }\n",
    "    \n",
    "    for img_path in sample_paths:\n",
    "        try:\n",
    "            # Chargement avec PIL pour les propriétés de base\n",
    "            img_pil = Image.open(img_path)\n",
    "            width, height = img_pil.size\n",
    "            \n",
    "            # Chargement avec OpenCV pour l'analyse d'intensité\n",
    "            img_cv = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            \n",
    "            # Propriétés\n",
    "            properties['widths'].append(width)\n",
    "            properties['heights'].append(height)\n",
    "            properties['channels'].append(len(img_pil.getbands()))\n",
    "            properties['mean_intensities'].append(np.mean(img_cv))\n",
    "            properties['std_intensities'].append(np.std(img_cv))\n",
    "            properties['file_sizes'].append(os.path.getsize(img_path) / 1024)  # KB\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Erreur avec {img_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return properties\n",
    "\n",
    "# Analyse des propriétés\n",
    "properties = analyze_image_properties(image_paths, sample_size=200)\n",
    "\n",
    "# Création du DataFrame d'analyse\n",
    "df_props = pd.DataFrame(properties)\n",
    "\n",
    "# Affichage des statistiques\n",
    "print(\"\\n📊 Statistiques des propriétés d'images:\")\n",
    "print(df_props.describe())\n",
    "\n",
    "# Visualisation des distributions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Distribution des Propriétés d\\'Images', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Tailles\n",
    "axes[0, 0].hist(df_props['widths'], bins=20, alpha=0.7, color='skyblue')\n",
    "axes[0, 0].set_title('Distribution des Largeurs')\n",
    "axes[0, 0].set_xlabel('Largeur (pixels)')\n",
    "\n",
    "axes[0, 1].hist(df_props['heights'], bins=20, alpha=0.7, color='lightgreen')\n",
    "axes[0, 1].set_title('Distribution des Hauteurs')\n",
    "axes[0, 1].set_xlabel('Hauteur (pixels)')\n",
    "\n",
    "# Intensités\n",
    "axes[0, 2].hist(df_props['mean_intensities'], bins=20, alpha=0.7, color='coral')\n",
    "axes[0, 2].set_title('Distribution des Intensités Moyennes')\n",
    "axes[0, 2].set_xlabel('Intensité Moyenne')\n",
    "\n",
    "axes[1, 0].hist(df_props['std_intensities'], bins=20, alpha=0.7, color='gold')\n",
    "axes[1, 0].set_title('Distribution des Écarts-Types d\\'Intensité')\n",
    "axes[1, 0].set_xlabel('Écart-Type Intensité')\n",
    "\n",
    "# Tailles de fichiers\n",
    "axes[1, 1].hist(df_props['file_sizes'], bins=20, alpha=0.7, color='mediumpurple')\n",
    "axes[1, 1].set_title('Distribution des Tailles de Fichiers')\n",
    "axes[1, 1].set_xlabel('Taille (KB)')\n",
    "\n",
    "# Corrélation largeur/hauteur\n",
    "axes[1, 2].scatter(df_props['widths'], df_props['heights'], alpha=0.6, color='darkblue')\n",
    "axes[1, 2].set_title('Corrélation Largeur/Hauteur')\n",
    "axes[1, 2].set_xlabel('Largeur (pixels)')\n",
    "axes[1, 2].set_ylabel('Hauteur (pixels)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Détection des formats non standards\n",
    "print(f\"\\n📐 Formats d'images détectés:\")\n",
    "unique_dimensions = df_props.groupby(['widths', 'heights']).size().reset_index(name='count')\n",
    "print(unique_dimensions.sort_values('count', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e993d94f",
   "metadata": {},
   "source": [
    "## 🔧 Section 3: Data Preprocessing and Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe58354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 3.1 PRÉPARATION DES DONNÉES D'ENTRAÎNEMENT\n",
    "# ===================================\n",
    "\n",
    "print(\"🔄 Préparation du dataset pour l'entraînement...\")\n",
    "\n",
    "# Création d'un sous-ensemble équilibré pour des temps de traitement raisonnables\n",
    "print(\"⚖️ Création d'un sous-ensemble équilibré...\")\n",
    "balanced_paths, balanced_labels = create_balanced_subset(\n",
    "    image_paths, labels, max_per_class=1000\n",
    ")\n",
    "\n",
    "print(f\"📊 Dataset équilibré: {len(balanced_paths)} images\")\n",
    "\n",
    "# Vérification de l'équilibrage\n",
    "balanced_distribution = pd.Series(balanced_labels).value_counts()\n",
    "print(\"🏷️ Nouvelle distribution:\")\n",
    "print(balanced_distribution)\n",
    "\n",
    "# Encodage des labels\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(balanced_labels)\n",
    "\n",
    "print(f\"🔢 Classes encodées: {dict(zip(label_encoder.classes_, range(len(label_encoder.classes_))))}\")\n",
    "\n",
    "# Division train/validation/test\n",
    "print(\"✂️ Division du dataset...\")\n",
    "\n",
    "# Première division: train+val / test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    balanced_paths, encoded_labels,\n",
    "    test_size=TEST_SPLIT,\n",
    "    stratify=encoded_labels,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Seconde division: train / val\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=VALIDATION_SPLIT/(1-TEST_SPLIT),  # Ajustement pour avoir 20% du total\n",
    "    stratify=y_temp,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"📊 Train: {len(X_train)} images\")\n",
    "print(f\"📊 Validation: {len(X_val)} images\")\n",
    "print(f\"📊 Test: {len(X_test)} images\")\n",
    "\n",
    "# Vérification de la stratification\n",
    "print(\"\\n🎯 Vérification de la stratification:\")\n",
    "for split_name, y_split in [(\"Train\", y_train), (\"Val\", y_val), (\"Test\", y_test)]:\n",
    "    distribution = pd.Series(y_split).value_counts().sort_index()\n",
    "    percentages = (distribution / len(y_split) * 100).round(1)\n",
    "    print(f\"{split_name}: {dict(zip(label_encoder.classes_, percentages.values))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9eb66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 3.2 PIPELINE DE PRÉPROCESSING D'IMAGES\n",
    "# ===================================\n",
    "\n",
    "class ImagePreprocessor:\n",
    "    \"\"\"Pipeline de préprocessing d'images optimisé\"\"\"\n",
    "    \n",
    "    def __init__(self, target_size=(224, 224), normalize=True):\n",
    "        self.target_size = target_size\n",
    "        self.normalize = normalize\n",
    "    \n",
    "    def preprocess_single_image(self, image_path):\n",
    "        \"\"\"Préprocesse une seule image\"\"\"\n",
    "        try:\n",
    "            # Chargement\n",
    "            img = cv2.imread(image_path)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Redimensionnement\n",
    "            img = cv2.resize(img, self.target_size)\n",
    "            \n",
    "            # Normalisation\n",
    "            if self.normalize:\n",
    "                img = img.astype(np.float32) / 255.0\n",
    "            \n",
    "            return img\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Erreur preprocessing {image_path}: {e}\")\n",
    "            return np.zeros((*self.target_size, 3), dtype=np.float32)\n",
    "    \n",
    "    def preprocess_batch(self, image_paths, batch_size=32):\n",
    "        \"\"\"Préprocesse un lot d'images avec gestion mémoire\"\"\"\n",
    "        n_images = len(image_paths)\n",
    "        n_batches = (n_images + batch_size - 1) // batch_size\n",
    "        \n",
    "        # Initialisation du tableau de sortie\n",
    "        images = np.zeros((n_images, *self.target_size, 3), dtype=np.float32)\n",
    "        \n",
    "        print(f\"🔄 Preprocessing {n_images} images en {n_batches} batches...\")\n",
    "        \n",
    "        for i in range(n_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = min((i + 1) * batch_size, n_images)\n",
    "            \n",
    "            # Traitement du batch\n",
    "            for j, img_path in enumerate(image_paths[start_idx:end_idx]):\n",
    "                images[start_idx + j] = self.preprocess_single_image(img_path)\n",
    "            \n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"   📊 Batch {i+1}/{n_batches} terminé\")\n",
    "        \n",
    "        return images\n",
    "\n",
    "# Initialisation du preprocessor\n",
    "preprocessor = ImagePreprocessor(target_size=IMG_SIZE, normalize=True)\n",
    "\n",
    "# Preprocessing des données\n",
    "print(\"🔄 Preprocessing des images...\")\n",
    "\n",
    "X_train_processed = preprocessor.preprocess_batch(X_train)\n",
    "X_val_processed = preprocessor.preprocess_batch(X_val)\n",
    "X_test_processed = preprocessor.preprocess_batch(X_test)\n",
    "\n",
    "print(f\"✅ Preprocessing terminé!\")\n",
    "print(f\"📊 Shape train: {X_train_processed.shape}\")\n",
    "print(f\"📊 Shape validation: {X_val_processed.shape}\")\n",
    "print(f\"📊 Shape test: {X_test_processed.shape}\")\n",
    "\n",
    "# Conversion des labels en format catégoriel pour le deep learning\n",
    "y_train_categorical = to_categorical(y_train, num_classes=len(CLASSES))\n",
    "y_val_categorical = to_categorical(y_val, num_classes=len(CLASSES))\n",
    "y_test_categorical = to_categorical(y_test, num_classes=len(CLASSES))\n",
    "\n",
    "print(f\"📊 Shape labels train: {y_train_categorical.shape}\")\n",
    "\n",
    "# Visualisation d'un échantillon preprocessé\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "fig.suptitle('Images Preprocessées - Échantillons', fontsize=14, fontweight='bold')\n",
    "\n",
    "for i in range(8):\n",
    "    row = i // 4\n",
    "    col = i % 4\n",
    "    \n",
    "    # Sélection d'une image aléatoire\n",
    "    idx = np.random.randint(0, len(X_train_processed))\n",
    "    img = X_train_processed[idx]\n",
    "    label = label_encoder.inverse_transform([y_train[idx]])[0]\n",
    "    \n",
    "    axes[row, col].imshow(img)\n",
    "    axes[row, col].set_title(f'{label}', fontsize=10)\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6224d67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 3.3 DATA AUGMENTATION POUR LE DEEP LEARNING\n",
    "# ===================================\n",
    "\n",
    "# Configuration de l'augmentation de données\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=10,           # Rotation aléatoire jusqu'à 10°\n",
    "    width_shift_range=0.1,       # Décalage horizontal 10%\n",
    "    height_shift_range=0.1,      # Décalage vertical 10%\n",
    "    zoom_range=0.1,              # Zoom aléatoire 10%\n",
    "    horizontal_flip=True,        # Miroir horizontal\n",
    "    brightness_range=[0.8, 1.2], # Variation de luminosité\n",
    "    fill_mode='nearest'          # Mode de remplissage\n",
    ")\n",
    "\n",
    "# Générateur pour validation (pas d'augmentation)\n",
    "val_datagen = ImageDataGenerator()\n",
    "\n",
    "print(\"🔄 Configuration de l'augmentation de données\")\n",
    "\n",
    "# Démonstration de l'augmentation\n",
    "def demonstrate_augmentation(X_sample, y_sample, n_augmentations=6):\n",
    "    \"\"\"Montre l'effet de l'augmentation sur quelques images\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, n_augmentations + 1, figsize=(20, 8))\n",
    "    fig.suptitle('Démonstration de l\\'Augmentation de Données', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for row in range(2):\n",
    "        # Image originale\n",
    "        idx = np.random.randint(0, len(X_sample))\n",
    "        original_img = X_sample[idx]\n",
    "        label = label_encoder.inverse_transform([y_sample[idx]])[0]\n",
    "        \n",
    "        axes[row, 0].imshow(original_img)\n",
    "        axes[row, 0].set_title(f'Original\\n{label}')\n",
    "        axes[row, 0].axis('off')\n",
    "        \n",
    "        # Images augmentées\n",
    "        img_batch = np.expand_dims(original_img, 0)\n",
    "        augmented_generator = train_datagen.flow(img_batch, batch_size=1)\n",
    "        \n",
    "        for col in range(1, n_augmentations + 1):\n",
    "            augmented_batch = next(augmented_generator)\n",
    "            augmented_img = augmented_batch[0]\n",
    "            \n",
    "            # Clip pour éviter les valeurs hors [0,1]\n",
    "            augmented_img = np.clip(augmented_img, 0, 1)\n",
    "            \n",
    "            axes[row, col].imshow(augmented_img)\n",
    "            axes[row, col].set_title(f'Augmentée {col}')\n",
    "            axes[row, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Démonstration\n",
    "print(\"🖼️ Démonstration de l'augmentation...\")\n",
    "demonstrate_augmentation(X_train_processed, y_train, n_augmentations=5)\n",
    "\n",
    "print(\"✅ Pipeline d'augmentation configuré\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842af9b8",
   "metadata": {},
   "source": [
    "## 🎯 Section 4: Baseline Models Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba007780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 4.1 EXTRACTION DE FEATURES POUR ML TRADITIONNEL\n",
    "# ===================================\n",
    "\n",
    "def extract_traditional_features(images):\n",
    "    \"\"\"\n",
    "    Extrait des features traditionnelles pour les modèles ML classiques\n",
    "    \n",
    "    Features extraites:\n",
    "    - Statistiques d'intensité (moyenne, std, min, max)\n",
    "    - Histogramme des niveaux de gris\n",
    "    - Features de texture (Local Binary Pattern simulé)\n",
    "    \"\"\"\n",
    "    print(f\"🔍 Extraction de features pour {len(images)} images...\")\n",
    "    \n",
    "    features_list = []\n",
    "    \n",
    "    for i, img in enumerate(images):\n",
    "        # Conversion en niveaux de gris\n",
    "        if len(img.shape) == 3:\n",
    "            gray_img = cv2.cvtColor((img * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "        else:\n",
    "            gray_img = (img * 255).astype(np.uint8)\n",
    "        \n",
    "        features = []\n",
    "        \n",
    "        # 1. Statistiques d'intensité\n",
    "        features.extend([\n",
    "            np.mean(gray_img),\n",
    "            np.std(gray_img),\n",
    "            np.min(gray_img),\n",
    "            np.max(gray_img),\n",
    "            np.percentile(gray_img, 25),\n",
    "            np.percentile(gray_img, 50),\n",
    "            np.percentile(gray_img, 75)\n",
    "        ])\n",
    "        \n",
    "        # 2. Histogramme (16 bins)\n",
    "        hist, _ = np.histogram(gray_img, bins=16, range=(0, 256))\n",
    "        hist = hist / np.sum(hist)  # Normalisation\n",
    "        features.extend(hist)\n",
    "        \n",
    "        # 3. Features de texture simples\n",
    "        # Gradient\n",
    "        grad_x = cv2.Sobel(gray_img, cv2.CV_64F, 1, 0, ksize=3)\n",
    "        grad_y = cv2.Sobel(gray_img, cv2.CV_64F, 0, 1, ksize=3)\n",
    "        gradient_magnitude = np.sqrt(grad_x**2 + grad_y**2)\n",
    "        \n",
    "        features.extend([\n",
    "            np.mean(gradient_magnitude),\n",
    "            np.std(gradient_magnitude)\n",
    "        ])\n",
    "        \n",
    "        # 4. Features géométriques (moments)\n",
    "        moments = cv2.moments(gray_img)\n",
    "        if moments['m00'] != 0:\n",
    "            cx = moments['m10'] / moments['m00']\n",
    "            cy = moments['m01'] / moments['m00']\n",
    "            features.extend([cx, cy])\n",
    "        else:\n",
    "            features.extend([0, 0])\n",
    "        \n",
    "        # 5. Énergie et entropie\n",
    "        # Normalisation de l'image pour calculer l'entropie\n",
    "        normalized = gray_img / 255.0\n",
    "        entropy = -np.sum(normalized * np.log(normalized + 1e-10))\n",
    "        energy = np.sum(normalized ** 2)\n",
    "        \n",
    "        features.extend([entropy, energy])\n",
    "        \n",
    "        features_list.append(features)\n",
    "        \n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"   📊 {i+1}/{len(images)} images traitées\")\n",
    "    \n",
    "    feature_matrix = np.array(features_list)\n",
    "    print(f\"✅ Features extraites: shape {feature_matrix.shape}\")\n",
    "    \n",
    "    return feature_matrix\n",
    "\n",
    "# Extraction des features\n",
    "print(\"🔍 Extraction des features traditionnelles...\")\n",
    "X_train_features = extract_traditional_features(X_train_processed)\n",
    "X_val_features = extract_traditional_features(X_val_processed)\n",
    "X_test_features = extract_traditional_features(X_test_processed)\n",
    "\n",
    "# Normalisation des features\n",
    "scaler = StandardScaler()\n",
    "X_train_features_scaled = scaler.fit_transform(X_train_features)\n",
    "X_val_features_scaled = scaler.transform(X_val_features)\n",
    "X_test_features_scaled = scaler.transform(X_test_features)\n",
    "\n",
    "print(f\"📊 Features shape train: {X_train_features_scaled.shape}\")\n",
    "print(f\"📊 Features normalisées avec StandardScaler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10f7a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 4.2 MODÈLES BASELINE TRADITIONNELS\n",
    "# ===================================\n",
    "\n",
    "# Dictionnaire pour stocker les résultats\n",
    "baseline_results = {}\n",
    "\n",
    "def evaluate_model(model, X_train, X_val, y_train, y_val, model_name):\n",
    "    \"\"\"Évalue un modèle et stocke les résultats\"\"\"\n",
    "    \n",
    "    print(f\"\\n🔄 Entraînement de {model_name}...\")\n",
    "    \n",
    "    # Entraînement\n",
    "    start_time = datetime.now()\n",
    "    model.fit(X_train, y_train)\n",
    "    training_time = datetime.now() - start_time\n",
    "    \n",
    "    # Prédictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    \n",
    "    # Métriques\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    \n",
    "    # Scores détaillés pour la validation\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_val, y_val_pred, average='weighted')\n",
    "    \n",
    "    # Stockage des résultats\n",
    "    results = {\n",
    "        'model': model,\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'val_accuracy': val_accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'training_time': training_time.total_seconds(),\n",
    "        'y_val_pred': y_val_pred\n",
    "    }\n",
    "    \n",
    "    baseline_results[model_name] = results\n",
    "    \n",
    "    print(f\"✅ {model_name}:\")\n",
    "    print(f\"   📊 Train Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"   📊 Val Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"   📊 F1-Score: {f1:.4f}\")\n",
    "    print(f\"   ⏱️ Training Time: {training_time.total_seconds():.2f}s\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Définition des modèles baseline\n",
    "baseline_models = {\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'SVM': SVC(\n",
    "        kernel='rbf',\n",
    "        C=1.0,\n",
    "        random_state=42,\n",
    "        probability=True  # Pour les probabilités\n",
    "    ),\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'Decision Tree': DecisionTreeClassifier(\n",
    "        max_depth=15,\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "# Entraînement et évaluation de tous les modèles\n",
    "print(\"🚀 Entraînement des modèles baseline...\")\n",
    "\n",
    "for model_name, model in baseline_models.items():\n",
    "    evaluate_model(\n",
    "        model, \n",
    "        X_train_features_scaled, \n",
    "        X_val_features_scaled, \n",
    "        y_train, \n",
    "        y_val, \n",
    "        model_name\n",
    "    )\n",
    "\n",
    "print(\"\\n✅ Tous les modèles baseline entraînés!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3327299f",
   "metadata": {},
   "source": [
    "## 🌳 Section 5: Bagging Ensemble Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d4e0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 5.1 IMPLÉMENTATION DES MÉTHODES DE BAGGING\n",
    "# ===================================\n",
    "\n",
    "print(\"🌳 Implémentation des méthodes de Bagging...\")\n",
    "\n",
    "# Dictionnaire pour les résultats de bagging\n",
    "bagging_results = {}\n",
    "\n",
    "# 5.1.1 Random Forest optimisé\n",
    "print(\"\\n🔄 Random Forest optimisé...\")\n",
    "rf_optimized = RandomForestClassifier(\n",
    "    n_estimators=200,           # Plus d'arbres\n",
    "    max_depth=15,               # Profondeur contrôlée\n",
    "    min_samples_split=5,        # Contrôle overfitting\n",
    "    min_samples_leaf=2,         # Contrôle overfitting\n",
    "    max_features='sqrt',        # Features aléatoires\n",
    "    bootstrap=True,             # Bagging activé\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "evaluate_model(\n",
    "    rf_optimized, \n",
    "    X_train_features_scaled, \n",
    "    X_val_features_scaled, \n",
    "    y_train, \n",
    "    y_val, \n",
    "    'Random Forest Optimized'\n",
    ")\n",
    "\n",
    "# 5.1.2 Extra Trees (Extremely Randomized Trees)\n",
    "print(\"\\n🔄 Extra Trees...\")\n",
    "extra_trees = ExtraTreesClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=15,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    max_features='sqrt',\n",
    "    bootstrap=True,             # Bagging\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "evaluate_model(\n",
    "    extra_trees, \n",
    "    X_train_features_scaled, \n",
    "    X_val_features_scaled, \n",
    "    y_train, \n",
    "    y_val, \n",
    "    'Extra Trees'\n",
    ")\n",
    "\n",
    "# 5.1.3 Bagging avec différents modèles de base\n",
    "print(\"\\n🔄 Bagging avec SVM...\")\n",
    "bagging_svm = BaggingClassifier(\n",
    "    estimator=SVC(kernel='rbf', probability=True, random_state=42),\n",
    "    n_estimators=50,            # Moins d'estimateurs car SVM est coûteux\n",
    "    max_samples=0.8,            # 80% des échantillons par modèle\n",
    "    max_features=0.8,           # 80% des features par modèle\n",
    "    bootstrap=True,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "evaluate_model(\n",
    "    bagging_svm, \n",
    "    X_train_features_scaled, \n",
    "    X_val_features_scaled, \n",
    "    y_train, \n",
    "    y_val, \n",
    "    'Bagging SVM'\n",
    ")\n",
    "\n",
    "# 5.1.4 Bagging avec Logistic Regression\n",
    "print(\"\\n🔄 Bagging avec Logistic Regression...\")\n",
    "bagging_lr = BaggingClassifier(\n",
    "    estimator=LogisticRegression(max_iter=1000, random_state=42),\n",
    "    n_estimators=100,\n",
    "    max_samples=0.8,\n",
    "    max_features=0.8,\n",
    "    bootstrap=True,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "evaluate_model(\n",
    "    bagging_lr, \n",
    "    X_train_features_scaled, \n",
    "    X_val_features_scaled, \n",
    "    y_train, \n",
    "    y_val, \n",
    "    'Bagging Logistic Regression'\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Méthodes de Bagging entraînées!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b56e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 5.2 ANALYSE DE L'IMPORTANCE DES FEATURES (BAGGING)\n",
    "# ===================================\n",
    "\n",
    "def analyze_feature_importance(model, feature_names, model_name, top_n=20):\n",
    "    \"\"\"Analyse l'importance des features pour les modèles tree-based\"\"\"\n",
    "    \n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "        \n",
    "        # Création du DataFrame\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': importances\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        # Visualisation\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        top_features = feature_importance_df.head(top_n)\n",
    "        \n",
    "        sns.barplot(data=top_features, x='importance', y='feature', palette='viridis')\n",
    "        plt.title(f'Top {top_n} Features - {model_name}', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Importance')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return feature_importance_df\n",
    "    else:\n",
    "        print(f\"❌ {model_name} ne supporte pas l'analyse d'importance des features\")\n",
    "        return None\n",
    "\n",
    "# Génération des noms de features\n",
    "feature_names = (\n",
    "    ['mean', 'std', 'min', 'max', 'q25', 'q50', 'q75'] +  # Statistiques\n",
    "    [f'hist_{i}' for i in range(16)] +                     # Histogramme\n",
    "    ['grad_mean', 'grad_std'] +                            # Gradient\n",
    "    ['cx', 'cy'] +                                         # Moments\n",
    "    ['entropy', 'energy']                                  # Texture\n",
    ")\n",
    "\n",
    "# Analyse pour Random Forest optimisé\n",
    "print(\"🔍 Analyse d'importance des features - Random Forest...\")\n",
    "rf_model = baseline_results['Random Forest Optimized']['model']\n",
    "rf_importance = analyze_feature_importance(rf_model, feature_names, 'Random Forest Optimized')\n",
    "\n",
    "# Analyse pour Extra Trees\n",
    "print(\"🔍 Analyse d'importance des features - Extra Trees...\")\n",
    "et_model = baseline_results['Extra Trees']['model']\n",
    "et_importance = analyze_feature_importance(et_model, feature_names, 'Extra Trees')\n",
    "\n",
    "# Comparaison des importances\n",
    "if rf_importance is not None and et_importance is not None:\n",
    "    print(\"\\n📊 Comparaison des top 10 features:\")\n",
    "    comparison_df = pd.merge(\n",
    "        rf_importance.head(10)[['feature', 'importance']].rename(columns={'importance': 'RF_importance'}),\n",
    "        et_importance.head(10)[['feature', 'importance']].rename(columns={'importance': 'ET_importance'}),\n",
    "        on='feature',\n",
    "        how='outer'\n",
    "    ).fillna(0)\n",
    "    \n",
    "    print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c563364f",
   "metadata": {},
   "source": [
    "## 🚀 Section 6: Boosting Ensemble Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db563ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 6.1 IMPLÉMENTATION DES MÉTHODES DE BOOSTING\n",
    "# ===================================\n",
    "\n",
    "print(\"🚀 Implémentation des méthodes de Boosting...\")\n",
    "\n",
    "# 6.1.1 AdaBoost\n",
    "print(\"\\n🔄 AdaBoost...\")\n",
    "ada_boost = AdaBoostClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=3, random_state=42),\n",
    "    n_estimators=100,\n",
    "    learning_rate=1.0,\n",
    "    algorithm='SAMME',  # Supporte multiclass\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "evaluate_model(\n",
    "    ada_boost, \n",
    "    X_train_features_scaled, \n",
    "    X_val_features_scaled, \n",
    "    y_train, \n",
    "    y_val, \n",
    "    'AdaBoost'\n",
    ")\n",
    "\n",
    "# 6.1.2 Gradient Boosting\n",
    "print(\"\\n🔄 Gradient Boosting...\")\n",
    "gradient_boost = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    subsample=0.8,              # Stochastic gradient boosting\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "evaluate_model(\n",
    "    gradient_boost, \n",
    "    X_train_features_scaled, \n",
    "    X_val_features_scaled, \n",
    "    y_train, \n",
    "    y_val, \n",
    "    'Gradient Boosting'\n",
    ")\n",
    "\n",
    "# 6.1.3 XGBoost (si disponible)\n",
    "try:\n",
    "    print(\"\\n🔄 XGBoost...\")\n",
    "    xgb_classifier = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=3,\n",
    "        min_child_weight=1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        eval_metric='mlogloss'  # Pour multiclass\n",
    "    )\n",
    "    \n",
    "    evaluate_model(\n",
    "        xgb_classifier, \n",
    "        X_train_features_scaled, \n",
    "        X_val_features_scaled, \n",
    "        y_train, \n",
    "        y_val, \n",
    "        'XGBoost'\n",
    "    )\n",
    "    \n",
    "except NameError:\n",
    "    print(\"⚠️ XGBoost non disponible - passage ignoré\")\n",
    "\n",
    "print(\"\\n✅ Méthodes de Boosting entraînées!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7f85b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 6.2 COMPARAISON BAGGING VS BOOSTING\n",
    "# ===================================\n",
    "\n",
    "def create_ensemble_comparison():\n",
    "    \"\"\"Compare les performances des méthodes d'ensemble\"\"\"\n",
    "    \n",
    "    # Récupération des métriques pour comparaison\n",
    "    comparison_data = []\n",
    "    \n",
    "    for model_name, results in baseline_results.items():\n",
    "        comparison_data.append({\n",
    "            'Model': model_name,\n",
    "            'Type': get_model_type(model_name),\n",
    "            'Val_Accuracy': results['val_accuracy'],\n",
    "            'F1_Score': results['f1_score'],\n",
    "            'Training_Time': results['training_time']\n",
    "        })\n",
    "    \n",
    "    df_comparison = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    return df_comparison\n",
    "\n",
    "def get_model_type(model_name):\n",
    "    \"\"\"Détermine le type de modèle\"\"\"\n",
    "    if any(keyword in model_name.lower() for keyword in ['random forest', 'extra trees', 'bagging']):\n",
    "        return 'Bagging'\n",
    "    elif any(keyword in model_name.lower() for keyword in ['ada', 'gradient', 'xgboost']):\n",
    "        return 'Boosting'\n",
    "    else:\n",
    "        return 'Baseline'\n",
    "\n",
    "# Création de la comparaison\n",
    "df_comparison = create_ensemble_comparison()\n",
    "\n",
    "print(\"📊 Comparaison des méthodes d'ensemble:\")\n",
    "print(df_comparison.sort_values('Val_Accuracy', ascending=False))\n",
    "\n",
    "# Visualisation comparative\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Comparaison des Méthodes d\\'Ensemble', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Accuracy par type\n",
    "sns.boxplot(data=df_comparison, x='Type', y='Val_Accuracy', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Accuracy par Type de Méthode')\n",
    "axes[0, 0].set_ylabel('Validation Accuracy')\n",
    "\n",
    "# F1-Score par type\n",
    "sns.boxplot(data=df_comparison, x='Type', y='F1_Score', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('F1-Score par Type de Méthode')\n",
    "axes[0, 1].set_ylabel('F1 Score')\n",
    "\n",
    "# Temps d'entraînement par type\n",
    "sns.boxplot(data=df_comparison, x='Type', y='Training_Time', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Temps d\\'Entraînement par Type')\n",
    "axes[1, 0].set_ylabel('Training Time (s)')\n",
    "\n",
    "# Correlation Accuracy vs Training Time\n",
    "sns.scatterplot(data=df_comparison, x='Training_Time', y='Val_Accuracy', \n",
    "                hue='Type', size='F1_Score', sizes=(50, 200), ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Accuracy vs Temps d\\'Entraînement')\n",
    "axes[1, 1].set_xlabel('Training Time (s)')\n",
    "axes[1, 1].set_ylabel('Validation Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistiques par type\n",
    "print(\"\\n📈 Statistiques par type de méthode:\")\n",
    "type_stats = df_comparison.groupby('Type').agg({\n",
    "    'Val_Accuracy': ['mean', 'std', 'max'],\n",
    "    'F1_Score': ['mean', 'std', 'max'],\n",
    "    'Training_Time': ['mean', 'std', 'min']\n",
    "}).round(4)\n",
    "\n",
    "print(type_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2251d2",
   "metadata": {},
   "source": [
    "## 🧠 Section 7: Deep Learning Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034de566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 7.1 ARCHITECTURE CNN CUSTOM\n",
    "# ===================================\n",
    "\n",
    "def create_custom_cnn(input_shape=(224, 224, 3), num_classes=4):\n",
    "    \"\"\"\n",
    "    Crée une architecture CNN personnalisée pour la classification d'images médicales\n",
    "    \"\"\"\n",
    "    \n",
    "    model = models.Sequential([\n",
    "        # Bloc 1: Extraction de features de bas niveau\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape, padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Bloc 2: Features de niveau intermédiaire\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Bloc 3: Features de haut niveau\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Bloc 4: Features complexes\n",
    "        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Classification\n",
    "        layers.GlobalAveragePooling2D(),  # Alternative à Flatten + Dense\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Création du modèle custom\n",
    "print(\"🧠 Création du modèle CNN personnalisé...\")\n",
    "custom_cnn = create_custom_cnn(input_shape=(*IMG_SIZE, 3), num_classes=len(CLASSES))\n",
    "\n",
    "# Affichage de l'architecture\n",
    "print(\"📋 Architecture du modèle:\")\n",
    "custom_cnn.summary()\n",
    "\n",
    "# Visualisation de l'architecture\n",
    "tf.keras.utils.plot_model(\n",
    "    custom_cnn, \n",
    "    to_file=str(RESULTS_DIR / \"custom_cnn_architecture.png\"),\n",
    "    show_shapes=True, \n",
    "    show_layer_names=True,\n",
    "    rankdir='TB'\n",
    ")\n",
    "\n",
    "print(f\"✅ Diagramme d'architecture sauvegardé: {RESULTS_DIR / 'custom_cnn_architecture.png'}\")\n",
    "\n",
    "# Configuration de l'optimiseur et compilation\n",
    "optimizer = optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "custom_cnn.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', 'precision', 'recall']\n",
    ")\n",
    "\n",
    "print(\"✅ Modèle compilé avec succès\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9422427d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 7.2 CALLBACKS ET MONITORING\n",
    "# ===================================\n",
    "\n",
    "# Configuration des callbacks\n",
    "def setup_callbacks(model_name, patience=10):\n",
    "    \"\"\"Configure les callbacks pour l'entraînement\"\"\"\n",
    "    \n",
    "    # Répertoire pour sauvegarder les modèles\n",
    "    model_save_dir = MODELS_DIR / model_name\n",
    "    model_save_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    callbacks_list = [\n",
    "        # Early Stopping\n",
    "        EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=patience,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # Réduction du learning rate\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # Sauvegarde du meilleur modèle\n",
    "        ModelCheckpoint(\n",
    "            filepath=str(model_save_dir / \"best_model.h5\"),\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            save_weights_only=False,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    return callbacks_list\n",
    "\n",
    "# Configuration des callbacks pour le modèle custom\n",
    "custom_callbacks = setup_callbacks(\"custom_cnn\", patience=10)\n",
    "\n",
    "print(\"✅ Callbacks configurés:\")\n",
    "for callback in custom_callbacks:\n",
    "    print(f\"   📋 {callback.__class__.__name__}\")\n",
    "\n",
    "# Préparation des générateurs de données\n",
    "print(\"\\n🔄 Préparation des générateurs de données...\")\n",
    "\n",
    "# Générateur d'entraînement avec augmentation\n",
    "train_generator = train_datagen.flow(\n",
    "    X_train_processed,\n",
    "    y_train_categorical,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Générateur de validation\n",
    "val_generator = val_datagen.flow(\n",
    "    X_val_processed,\n",
    "    y_val_categorical,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"✅ Générateurs préparés:\")\n",
    "print(f\"   📊 Train: {len(train_generator)} batches de {BATCH_SIZE}\")\n",
    "print(f\"   📊 Validation: {len(val_generator)} batches de {BATCH_SIZE}\")\n",
    "\n",
    "# Fonction d'entraînement\n",
    "def train_deep_model(model, model_name, train_gen, val_gen, epochs=EPOCHS, callbacks=None):\n",
    "    \"\"\"Entraîne un modèle de deep learning avec monitoring\"\"\"\n",
    "    \n",
    "    print(f\"\\n🚀 Entraînement de {model_name}...\")\n",
    "    print(f\"   📊 Époques: {epochs}\")\n",
    "    print(f\"   📊 Batch size: {BATCH_SIZE}\")\n",
    "    \n",
    "    # Entraînement\n",
    "    history = model.fit(\n",
    "        train_gen,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_gen,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Entraînement de {model_name} terminé!\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "print(\"🔧 Environnement d'entraînement Deep Learning configuré\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94e73df",
   "metadata": {},
   "source": [
    "## 🔄 Section 8: Transfer Learning Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6fffbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 8.1 MODÈLES PRÉ-ENTRAÎNÉS AVEC TRANSFER LEARNING\n",
    "# ===================================\n",
    "\n",
    "def create_transfer_learning_model(base_model_name, input_shape=(224, 224, 3), num_classes=4, \n",
    "                                 freeze_base=True, trainable_layers=0):\n",
    "    \"\"\"\n",
    "    Crée un modèle de transfer learning\n",
    "    \n",
    "    Args:\n",
    "        base_model_name: 'VGG16', 'ResNet50', 'EfficientNetB0', ou 'InceptionV3'\n",
    "        freeze_base: Si True, gèle les couches du modèle de base\n",
    "        trainable_layers: Nombre de couches à rendre entraînables (depuis la fin)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sélection du modèle de base\n",
    "    if base_model_name == 'VGG16':\n",
    "        base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    elif base_model_name == 'ResNet50':\n",
    "        base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    elif base_model_name == 'EfficientNetB0':\n",
    "        base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    elif base_model_name == 'InceptionV3':\n",
    "        base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    else:\n",
    "        raise ValueError(f\"Modèle {base_model_name} non supporté\")\n",
    "    \n",
    "    # Gel des couches\n",
    "    if freeze_base:\n",
    "        base_model.trainable = False\n",
    "    else:\n",
    "        # Rendre seulement les dernières couches entraînables\n",
    "        if trainable_layers > 0:\n",
    "            base_model.trainable = True\n",
    "            for layer in base_model.layers[:-trainable_layers]:\n",
    "                layer.trainable = False\n",
    "    \n",
    "    # Construction du modèle complet\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Dictionnaire pour stocker les modèles de transfer learning\n",
    "transfer_models = {}\n",
    "\n",
    "print(\"🔄 Création des modèles de Transfer Learning...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71751b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 8.2 ENTRAÎNEMENT DES MODÈLES DE TRANSFER LEARNING\n",
    "# ===================================\n",
    "\n",
    "# Configuration des modèles à tester\n",
    "models_config = [\n",
    "    {'name': 'VGG16_frozen', 'base': 'VGG16', 'freeze': True, 'trainable': 0},\n",
    "    {'name': 'ResNet50_frozen', 'base': 'ResNet50', 'freeze': True, 'trainable': 0},\n",
    "    {'name': 'EfficientNetB0_frozen', 'base': 'EfficientNetB0', 'freeze': True, 'trainable': 0}\n",
    "]\n",
    "\n",
    "# Stockage des historiques d'entraînement\n",
    "training_histories = {}\n",
    "\n",
    "for config in models_config:\n",
    "    print(f\"\\n🔄 Configuration: {config['name']}\")\n",
    "    \n",
    "    try:\n",
    "        # Création du modèle\n",
    "        model = create_transfer_learning_model(\n",
    "            base_model_name=config['base'],\n",
    "            input_shape=(*IMG_SIZE, 3),\n",
    "            num_classes=len(CLASSES),\n",
    "            freeze_base=config['freeze'],\n",
    "            trainable_layers=config['trainable']\n",
    "        )\n",
    "        \n",
    "        # Compilation\n",
    "        model.compile(\n",
    "            optimizer=optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy', 'precision', 'recall']\n",
    "        )\n",
    "        \n",
    "        # Stockage\n",
    "        transfer_models[config['name']] = model\n",
    "        \n",
    "        print(f\"✅ {config['name']} créé et compilé\")\n",
    "        print(f\"   📊 Paramètres entraînables: {model.count_params():,}\")\n",
    "        \n",
    "        # Affichage du résumé pour le premier modèle\n",
    "        if config['name'] == 'VGG16_frozen':\n",
    "            print(f\"\\n📋 Exemple d'architecture - {config['name']}:\")\n",
    "            model.summary()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur lors de la création de {config['name']}: {e}\")\n",
    "\n",
    "print(f\"\\n✅ {len(transfer_models)} modèles de transfer learning créés\")\n",
    "\n",
    "# Entraînement d'un modèle de démonstration (VGG16)\n",
    "if 'VGG16_frozen' in transfer_models:\n",
    "    print(\"\\n🚀 Entraînement de démonstration - VGG16 (epochs réduits)...\")\n",
    "    \n",
    "    # Callbacks pour la démo\n",
    "    demo_callbacks = setup_callbacks(\"VGG16_frozen_demo\", patience=5)\n",
    "    \n",
    "    # Entraînement avec moins d'époques pour la démo\n",
    "    demo_history = train_deep_model(\n",
    "        transfer_models['VGG16_frozen'],\n",
    "        \"VGG16_frozen\",\n",
    "        train_generator,\n",
    "        val_generator,\n",
    "        epochs=10,  # Réduit pour la démo\n",
    "        callbacks=demo_callbacks\n",
    "    )\n",
    "    \n",
    "    training_histories['VGG16_frozen'] = demo_history\n",
    "    \n",
    "    print(\"✅ Entraînement de démonstration terminé\")\n",
    "\n",
    "print(\"\\n💡 Note: Pour un entraînement complet, ajustez le nombre d'époques selon vos ressources\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c897ed1",
   "metadata": {},
   "source": [
    "## ⚡ Section 9: Fine-Tuning Pre-trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b71a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 9.1 STRATÉGIES DE FINE-TUNING\n",
    "# ===================================\n",
    "\n",
    "def fine_tune_model(base_model_name, pretrained_model_path=None, \n",
    "                   unfreeze_layers=20, fine_tune_lr=1e-5):\n",
    "    \"\"\"\n",
    "    Implémente le fine-tuning d'un modèle pré-entraîné\n",
    "    \n",
    "    Args:\n",
    "        base_model_name: Nom du modèle de base\n",
    "        pretrained_model_path: Chemin vers le modèle pré-entraîné (optionnel)\n",
    "        unfreeze_layers: Nombre de couches à dégeler pour le fine-tuning\n",
    "        fine_tune_lr: Learning rate réduit pour le fine-tuning\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"⚡ Fine-tuning de {base_model_name}\")\n",
    "    \n",
    "    # Si un modèle pré-entraîné existe, le charger\n",
    "    if pretrained_model_path and os.path.exists(pretrained_model_path):\n",
    "        print(f\"📂 Chargement du modèle pré-entraîné: {pretrained_model_path}\")\n",
    "        model = keras.models.load_model(pretrained_model_path)\n",
    "    else:\n",
    "        # Créer un nouveau modèle si pas de modèle pré-entraîné\n",
    "        print(\"🔧 Création d'un nouveau modèle pour le fine-tuning\")\n",
    "        model = create_transfer_learning_model(\n",
    "            base_model_name=base_model_name,\n",
    "            input_shape=(*IMG_SIZE, 3),\n",
    "            num_classes=len(CLASSES),\n",
    "            freeze_base=False,\n",
    "            trainable_layers=unfreeze_layers\n",
    "        )\n",
    "    \n",
    "    # Phase 1: Dégel progressif des couches\n",
    "    print(f\"🔓 Dégel des {unfreeze_layers} dernières couches du modèle de base\")\n",
    "    \n",
    "    # Identification du modèle de base (première couche)\n",
    "    base_model = model.layers[0]\n",
    "    \n",
    "    # Gel de toutes les couches d'abord\n",
    "    base_model.trainable = True\n",
    "    \n",
    "    # Gel des premières couches, dégel des dernières\n",
    "    total_layers = len(base_model.layers)\n",
    "    freeze_until = total_layers - unfreeze_layers\n",
    "    \n",
    "    for i, layer in enumerate(base_model.layers):\n",
    "        if i < freeze_until:\n",
    "            layer.trainable = False\n",
    "        else:\n",
    "            layer.trainable = True\n",
    "    \n",
    "    print(f\"   📊 Couches totales: {total_layers}\")\n",
    "    print(f\"   🔒 Couches gelées: {freeze_until}\")\n",
    "    print(f\"   🔓 Couches entraînables: {unfreeze_layers}\")\n",
    "    \n",
    "    # Recompilation avec learning rate réduit\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=fine_tune_lr),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy', 'precision', 'recall']\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Modèle recompilé avec learning rate: {fine_tune_lr}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Démonstration du fine-tuning sur VGG16\n",
    "print(\"⚡ Démonstration de Fine-Tuning - VGG16\")\n",
    "\n",
    "# Stratégies de fine-tuning à tester\n",
    "fine_tuning_strategies = [\n",
    "    {\n",
    "        'name': 'VGG16_fine_tuned_conservative',\n",
    "        'base': 'VGG16',\n",
    "        'unfreeze_layers': 10,\n",
    "        'lr': 1e-5,\n",
    "        'description': 'Fine-tuning conservateur - 10 dernières couches'\n",
    "    },\n",
    "    {\n",
    "        'name': 'VGG16_fine_tuned_aggressive', \n",
    "        'base': 'VGG16',\n",
    "        'unfreeze_layers': 20,\n",
    "        'lr': 5e-6,\n",
    "        'description': 'Fine-tuning agressif - 20 dernières couches'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Stockage des modèles fine-tunés\n",
    "fine_tuned_models = {}\n",
    "\n",
    "for strategy in fine_tuning_strategies:\n",
    "    print(f\"\\n🔄 {strategy['description']}\")\n",
    "    \n",
    "    try:\n",
    "        # Création du modèle fine-tuné\n",
    "        ft_model = fine_tune_model(\n",
    "            base_model_name=strategy['base'],\n",
    "            unfreeze_layers=strategy['unfreeze_layers'],\n",
    "            fine_tune_lr=strategy['lr']\n",
    "        )\n",
    "        \n",
    "        fine_tuned_models[strategy['name']] = ft_model\n",
    "        \n",
    "        # Affichage des informations sur l'entraînabilité\n",
    "        trainable_count = sum([1 for layer in ft_model.layers for sublayer in layer.layers if hasattr(sublayer, 'trainable') and sublayer.trainable])\n",
    "        total_count = sum([1 for layer in ft_model.layers for sublayer in layer.layers if hasattr(sublayer, 'trainable')])\n",
    "        \n",
    "        print(f\"   📊 Couches entraînables: {trainable_count}/{total_count}\")\n",
    "        print(f\"   📊 Paramètres entraînables: {ft_model.count_params():,}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur lors du fine-tuning de {strategy['name']}: {e}\")\n",
    "\n",
    "print(f\"\\n✅ {len(fine_tuned_models)} modèles fine-tunés créés\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22841f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 9.2 LEARNING RATE SCHEDULING AVANCÉ\n",
    "# ===================================\n",
    "\n",
    "def create_advanced_callbacks(model_name, strategy_type=\"fine_tuning\"):\n",
    "    \"\"\"\n",
    "    Crée des callbacks avancés pour le fine-tuning\n",
    "    \"\"\"\n",
    "    \n",
    "    model_save_dir = MODELS_DIR / f\"{model_name}_{strategy_type}\"\n",
    "    model_save_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Learning rate scheduler personnalisé\n",
    "    def lr_schedule(epoch, lr):\n",
    "        \"\"\"Planification du learning rate\"\"\"\n",
    "        if epoch < 5:\n",
    "            return lr\n",
    "        elif epoch < 15:\n",
    "            return lr * 0.9\n",
    "        else:\n",
    "            return lr * 0.95\n",
    "    \n",
    "    callbacks_list = [\n",
    "        # Early Stopping plus patient pour le fine-tuning\n",
    "        EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=15,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # Réduction automatique du learning rate\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.3,\n",
    "            patience=7,\n",
    "            min_lr=1e-8,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # Learning rate scheduler personnalisé\n",
    "        keras.callbacks.LearningRateScheduler(lr_schedule, verbose=1),\n",
    "        \n",
    "        # Sauvegarde des meilleurs modèles\n",
    "        ModelCheckpoint(\n",
    "            filepath=str(model_save_dir / \"best_model.h5\"),\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            save_weights_only=False,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # Sauvegarde des checkpoints\n",
    "        ModelCheckpoint(\n",
    "            filepath=str(model_save_dir / \"checkpoint_epoch_{epoch:02d}.h5\"),\n",
    "            save_freq='epoch',\n",
    "            save_weights_only=True,\n",
    "            verbose=0\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    return callbacks_list\n",
    "\n",
    "# Démonstration d'entraînement avec fine-tuning (version réduite)\n",
    "if 'VGG16_fine_tuned_conservative' in fine_tuned_models:\n",
    "    print(\"🚀 Démonstration de Fine-Tuning avec callbacks avancés...\")\n",
    "    \n",
    "    # Configuration des callbacks avancés\n",
    "    ft_callbacks = create_advanced_callbacks(\"VGG16\", \"fine_tuning_demo\")\n",
    "    \n",
    "    # Entraînement avec paramètres ajustés pour le fine-tuning\n",
    "    print(\"📊 Configuration pour Fine-Tuning:\")\n",
    "    print(\"   - Learning rate réduit\")\n",
    "    print(\"   - Patience augmentée\")\n",
    "    print(\"   - Callbacks avancés\")\n",
    "    print(\"   - Epochs réduits pour démo\")\n",
    "    \n",
    "    # Note: Dans un scénario réel, vous entraîneriez ici le modèle\n",
    "    print(\"\\n💡 Note: Dans un entraînement complet, vous exécuteriez:\")\n",
    "    print(\"   ft_history = train_deep_model(model, 'VGG16_fine_tuned', train_gen, val_gen, epochs=30)\")\n",
    "    \n",
    "    print(\"✅ Configuration de fine-tuning préparée\")\n",
    "\n",
    "print(\"\\n🔧 Pipeline de Fine-Tuning configuré avec succès!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4678b161",
   "metadata": {},
   "source": [
    "## 🤝 Section 10: Ensemble of Deep Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3c681b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 10.1 ENSEMBLE VOTING POUR DEEP LEARNING\n",
    "# ===================================\n",
    "\n",
    "class DeepLearningEnsemble:\n",
    "    \"\"\"\n",
    "    Classe pour créer des ensembles de modèles de deep learning\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, models_dict, ensemble_method='soft_voting'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            models_dict: Dictionnaire {nom: modèle} des modèles à ensembler\n",
    "            ensemble_method: 'soft_voting', 'hard_voting', ou 'stacking'\n",
    "        \"\"\"\n",
    "        self.models = models_dict\n",
    "        self.ensemble_method = ensemble_method\n",
    "        self.model_names = list(models_dict.keys())\n",
    "        \n",
    "    def predict_ensemble(self, X, return_individual=False):\n",
    "        \"\"\"\n",
    "        Fait des prédictions avec l'ensemble\n",
    "        \"\"\"\n",
    "        individual_predictions = {}\n",
    "        all_predictions = []\n",
    "        \n",
    "        print(f\"🔄 Prédiction avec {len(self.models)} modèles...\")\n",
    "        \n",
    "        # Prédictions individuelles\n",
    "        for name, model in self.models.items():\n",
    "            try:\n",
    "                pred = model.predict(X, verbose=0)\n",
    "                individual_predictions[name] = pred\n",
    "                all_predictions.append(pred)\n",
    "                print(f\"   ✅ {name}: {pred.shape}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ❌ Erreur avec {name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not all_predictions:\n",
    "            raise ValueError(\"Aucune prédiction réussie\")\n",
    "        \n",
    "        # Ensemble des prédictions\n",
    "        all_predictions = np.array(all_predictions)\n",
    "        \n",
    "        if self.ensemble_method == 'soft_voting':\n",
    "            # Moyenne des probabilités\n",
    "            ensemble_pred = np.mean(all_predictions, axis=0)\n",
    "        elif self.ensemble_method == 'hard_voting':\n",
    "            # Vote majoritaire\n",
    "            individual_classes = [np.argmax(pred, axis=1) for pred in all_predictions]\n",
    "            ensemble_classes = []\n",
    "            for i in range(len(X)):\n",
    "                votes = [pred[i] for pred in individual_classes]\n",
    "                ensemble_classes.append(max(set(votes), key=votes.count))\n",
    "            \n",
    "            # Conversion en format one-hot\n",
    "            ensemble_pred = to_categorical(ensemble_classes, num_classes=len(CLASSES))\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Méthode d'ensemble {self.ensemble_method} non supportée\")\n",
    "        \n",
    "        if return_individual:\n",
    "            return ensemble_pred, individual_predictions\n",
    "        return ensemble_pred\n",
    "    \n",
    "    def evaluate_ensemble(self, X, y_true, return_individual=False):\n",
    "        \"\"\"\n",
    "        Évalue les performances de l'ensemble\n",
    "        \"\"\"\n",
    "        print(f\"📊 Évaluation de l'ensemble ({self.ensemble_method})...\")\n",
    "        \n",
    "        # Prédictions\n",
    "        if return_individual:\n",
    "            ensemble_pred, individual_preds = self.predict_ensemble(X, return_individual=True)\n",
    "        else:\n",
    "            ensemble_pred = self.predict_ensemble(X)\n",
    "            individual_preds = None\n",
    "        \n",
    "        # Conversion en classes pour les métriques\n",
    "        y_true_classes = np.argmax(y_true, axis=1)\n",
    "        ensemble_classes = np.argmax(ensemble_pred, axis=1)\n",
    "        \n",
    "        # Métriques de l'ensemble\n",
    "        accuracy = accuracy_score(y_true_classes, ensemble_classes)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            y_true_classes, ensemble_classes, average='weighted'\n",
    "        )\n",
    "        \n",
    "        results = {\n",
    "            'ensemble_accuracy': accuracy,\n",
    "            'ensemble_precision': precision,\n",
    "            'ensemble_recall': recall,\n",
    "            'ensemble_f1': f1,\n",
    "            'ensemble_predictions': ensemble_pred\n",
    "        }\n",
    "        \n",
    "        # Évaluation individuelle si demandée\n",
    "        if return_individual and individual_preds:\n",
    "            individual_results = {}\n",
    "            for name, pred in individual_preds.items():\n",
    "                pred_classes = np.argmax(pred, axis=1)\n",
    "                ind_acc = accuracy_score(y_true_classes, pred_classes)\n",
    "                individual_results[name] = {\n",
    "                    'accuracy': ind_acc,\n",
    "                    'predictions': pred\n",
    "                }\n",
    "            results['individual_results'] = individual_results\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Démonstration d'ensemble avec les modèles disponibles\n",
    "print(\"🤝 Création d'ensembles de modèles Deep Learning...\")\n",
    "\n",
    "# Vérification des modèles disponibles pour l'ensemble\n",
    "available_models = {}\n",
    "\n",
    "# Ajout des modèles de transfer learning si disponibles\n",
    "for model_name, model in transfer_models.items():\n",
    "    available_models[model_name] = model\n",
    "\n",
    "# Ajout des modèles fine-tunés si disponibles  \n",
    "for model_name, model in fine_tuned_models.items():\n",
    "    available_models[model_name] = model\n",
    "\n",
    "print(f\"📊 Modèles disponibles pour l'ensemble: {list(available_models.keys())}\")\n",
    "\n",
    "if len(available_models) >= 2:\n",
    "    # Création d'ensembles avec différentes méthodes\n",
    "    ensemble_configs = [\n",
    "        {'method': 'soft_voting', 'description': 'Moyenne des probabilités'},\n",
    "        {'method': 'hard_voting', 'description': 'Vote majoritaire'}\n",
    "    ]\n",
    "    \n",
    "    ensembles = {}\n",
    "    \n",
    "    for config in ensemble_configs:\n",
    "        ensemble_name = f\"DL_Ensemble_{config['method']}\"\n",
    "        ensemble = DeepLearningEnsemble(\n",
    "            models_dict=available_models,\n",
    "            ensemble_method=config['method']\n",
    "        )\n",
    "        ensembles[ensemble_name] = ensemble\n",
    "        \n",
    "        print(f\"✅ {ensemble_name} créé: {config['description']}\")\n",
    "    \n",
    "    print(f\"\\n🎯 {len(ensembles)} ensembles de Deep Learning configurés\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️ Pas assez de modèles entraînés pour créer un ensemble\")\n",
    "    print(\"   Entraînez d'abord plusieurs modèles de transfer learning\")\n",
    "\n",
    "print(\"\\n💡 Note: L'évaluation complète des ensembles nécessite des modèles entièrement entraînés\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf8f766",
   "metadata": {},
   "source": [
    "## ⚙️ Section 11: Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b43bf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 11.1 OPTIMISATION DES HYPERPARAMÈTRES - MÉTHODES TRADITIONNELLES\n",
    "# ===================================\n",
    "\n",
    "print(\"⚙️ Optimisation des hyperparamètres pour les méthodes d'ensemble...\")\n",
    "\n",
    "# Configuration des espaces de recherche\n",
    "param_grids = {\n",
    "    'RandomForest': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 15, 20, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['sqrt', 'log2', None]\n",
    "    },\n",
    "    \n",
    "    'XGBoost': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'min_child_weight': [1, 3, 5],\n",
    "        'subsample': [0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "    } if 'xgb' in globals() else {},\n",
    "    \n",
    "    'GradientBoosting': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'subsample': [0.8, 0.9, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "def optimize_hyperparameters(model_class, param_grid, X_train, y_train, \n",
    "                           cv=3, scoring='accuracy', n_jobs=-1, verbose=True):\n",
    "    \"\"\"\n",
    "    Optimise les hyperparamètres avec GridSearchCV\n",
    "    \"\"\"\n",
    "    if not param_grid:\n",
    "        print(\"⚠️ Grille de paramètres vide - optimisation ignorée\")\n",
    "        return None, None\n",
    "    \n",
    "    print(f\"🔍 Optimisation avec {len(param_grid)} paramètres...\")\n",
    "    print(f\"   📊 Cross-validation: {cv} folds\")\n",
    "    print(f\"   📊 Métrique: {scoring}\")\n",
    "    \n",
    "    # Configuration de la recherche\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model_class,\n",
    "        param_grid=param_grid,\n",
    "        cv=cv,\n",
    "        scoring=scoring,\n",
    "        n_jobs=n_jobs,\n",
    "        return_train_score=True,\n",
    "        verbose=1 if verbose else 0\n",
    "    )\n",
    "    \n",
    "    # Exécution de la recherche\n",
    "    start_time = datetime.now()\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    optimization_time = datetime.now() - start_time\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"✅ Optimisation terminée en {optimization_time.total_seconds():.2f}s\")\n",
    "        print(f\"🎯 Meilleur score: {grid_search.best_score_:.4f}\")\n",
    "        print(f\"⚙️ Meilleurs paramètres: {grid_search.best_params_}\")\n",
    "    \n",
    "    return grid_search.best_estimator_, grid_search.cv_results_\n",
    "\n",
    "# Stockage des résultats d'optimisation\n",
    "optimization_results = {}\n",
    "\n",
    "# Optimisation pour Random Forest\n",
    "print(\"\\n🌳 Optimisation Random Forest...\")\n",
    "if 'RandomForest' in param_grids:\n",
    "    # Grille réduite pour la démo\n",
    "    rf_demo_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [10, 15],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'max_features': ['sqrt', None]\n",
    "    }\n",
    "    \n",
    "    best_rf, rf_cv_results = optimize_hyperparameters(\n",
    "        RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "        rf_demo_grid,\n",
    "        X_train_features_scaled,\n",
    "        y_train,\n",
    "        cv=3,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    if best_rf:\n",
    "        optimization_results['RandomForest'] = {\n",
    "            'best_model': best_rf,\n",
    "            'cv_results': rf_cv_results\n",
    "        }\n",
    "\n",
    "# Optimisation pour Gradient Boosting\n",
    "print(\"\\n🚀 Optimisation Gradient Boosting...\")\n",
    "gb_demo_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'learning_rate': [0.1, 0.2],\n",
    "    'max_depth': [3, 5]\n",
    "}\n",
    "\n",
    "best_gb, gb_cv_results = optimize_hyperparameters(\n",
    "    GradientBoostingClassifier(random_state=42),\n",
    "    gb_demo_grid,\n",
    "    X_train_features_scaled,\n",
    "    y_train,\n",
    "    cv=3,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "if best_gb:\n",
    "    optimization_results['GradientBoosting'] = {\n",
    "        'best_model': best_gb,\n",
    "        'cv_results': gb_cv_results\n",
    "    }\n",
    "\n",
    "print(f\"\\n✅ {len(optimization_results)} modèles optimisés\")\n",
    "\n",
    "# Évaluation des modèles optimisés\n",
    "print(\"\\n📊 Évaluation des modèles optimisés...\")\n",
    "for model_name, results in optimization_results.items():\n",
    "    model = results['best_model']\n",
    "    \n",
    "    # Évaluation sur validation\n",
    "    val_pred = model.predict(X_val_features_scaled)\n",
    "    val_accuracy = accuracy_score(y_val, val_pred)\n",
    "    \n",
    "    print(f\"🎯 {model_name} optimisé:\")\n",
    "    print(f\"   📊 Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    # Comparaison avec le modèle baseline\n",
    "    if model_name in baseline_results:\n",
    "        baseline_acc = baseline_results[model_name]['val_accuracy']\n",
    "        improvement = val_accuracy - baseline_acc\n",
    "        print(f\"   📈 Amélioration: {improvement:+.4f}\")\n",
    "    \n",
    "    # Stockage dans baseline_results pour comparaison\n",
    "    baseline_results[f\"{model_name}_optimized\"] = {\n",
    "        'model': model,\n",
    "        'val_accuracy': val_accuracy,\n",
    "        'y_val_pred': val_pred\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf697ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 11.2 RECHERCHE ALÉATOIRE ET ANALYSE DES HYPERPARAMÈTRES\n",
    "# ===================================\n",
    "\n",
    "def randomized_hyperparameter_search(model_class, param_distributions, \n",
    "                                    X_train, y_train, n_iter=20, cv=3):\n",
    "    \"\"\"\n",
    "    Recherche aléatoire d'hyperparamètres avec RandomizedSearchCV\n",
    "    \"\"\"\n",
    "    print(f\"🎲 Recherche aléatoire avec {n_iter} itérations...\")\n",
    "    \n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=model_class,\n",
    "        param_distributions=param_distributions,\n",
    "        n_iter=n_iter,\n",
    "        cv=cv,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        return_train_score=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    random_search.fit(X_train, y_train)\n",
    "    search_time = datetime.now() - start_time\n",
    "    \n",
    "    print(f\"✅ Recherche terminée en {search_time.total_seconds():.2f}s\")\n",
    "    print(f\"🎯 Meilleur score: {random_search.best_score_:.4f}\")\n",
    "    print(f\"⚙️ Meilleurs paramètres: {random_search.best_params_}\")\n",
    "    \n",
    "    return random_search.best_estimator_, random_search.cv_results_\n",
    "\n",
    "# Distributions pour la recherche aléatoire\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "rf_param_distributions = {\n",
    "    'n_estimators': randint(50, 300),\n",
    "    'max_depth': [5, 10, 15, 20, None],\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 10),\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Recherche aléatoire pour Random Forest\n",
    "print(\"\\n🎲 Recherche aléatoire - Random Forest...\")\n",
    "best_rf_random, rf_random_results = randomized_hyperparameter_search(\n",
    "    RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    rf_param_distributions,\n",
    "    X_train_features_scaled,\n",
    "    y_train,\n",
    "    n_iter=10,  # Réduit pour la démo\n",
    "    cv=3\n",
    ")\n",
    "\n",
    "# Analyse des résultats de recherche\n",
    "def analyze_hyperparameter_results(cv_results, param_name, model_name):\n",
    "    \"\"\"\n",
    "    Analyse l'impact d'un hyperparamètre sur les performances\n",
    "    \"\"\"\n",
    "    if f'param_{param_name}' not in cv_results:\n",
    "        print(f\"⚠️ Paramètre {param_name} non trouvé dans les résultats\")\n",
    "        return\n",
    "    \n",
    "    # Extraction des données\n",
    "    param_values = cv_results[f'param_{param_name}']\n",
    "    mean_scores = cv_results['mean_test_score']\n",
    "    \n",
    "    # Création du DataFrame pour l'analyse\n",
    "    df_analysis = pd.DataFrame({\n",
    "        'param_value': param_values,\n",
    "        'mean_score': mean_scores\n",
    "    })\n",
    "    \n",
    "    # Gestion des valeurs None\n",
    "    df_analysis = df_analysis[df_analysis['param_value'].notna()]\n",
    "    \n",
    "    if len(df_analysis) == 0:\n",
    "        print(f\"⚠️ Pas de données valides pour {param_name}\")\n",
    "        return\n",
    "    \n",
    "    # Visualisation\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    if df_analysis['param_value'].dtype == 'object':\n",
    "        # Paramètre catégoriel\n",
    "        df_grouped = df_analysis.groupby('param_value')['mean_score'].agg(['mean', 'std']).reset_index()\n",
    "        plt.bar(range(len(df_grouped)), df_grouped['mean'], \n",
    "                yerr=df_grouped['std'], alpha=0.7, capsize=5)\n",
    "        plt.xticks(range(len(df_grouped)), df_grouped['param_value'], rotation=45)\n",
    "    else:\n",
    "        # Paramètre numérique\n",
    "        plt.scatter(df_analysis['param_value'], df_analysis['mean_score'], alpha=0.6)\n",
    "        plt.xlabel(param_name)\n",
    "    \n",
    "    plt.title(f'Impact de {param_name} sur les performances - {model_name}')\n",
    "    plt.ylabel('Score de validation')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Analyse des résultats de Random Forest\n",
    "if rf_random_results:\n",
    "    print(\"\\n📊 Analyse des hyperparamètres - Random Forest...\")\n",
    "    \n",
    "    # Analyse de différents paramètres\n",
    "    important_params = ['n_estimators', 'max_depth', 'min_samples_split']\n",
    "    \n",
    "    for param in important_params:\n",
    "        analyze_hyperparameter_results(rf_random_results, param, 'Random Forest')\n",
    "\n",
    "# Comparaison des méthodes d'optimisation\n",
    "print(\"\\n📈 Comparaison des méthodes d'optimisation:\")\n",
    "\n",
    "comparison_data = []\n",
    "\n",
    "# Baseline\n",
    "for model_name in ['Random Forest', 'Gradient Boosting']:\n",
    "    if model_name in baseline_results:\n",
    "        comparison_data.append({\n",
    "            'Model': model_name,\n",
    "            'Method': 'Baseline',\n",
    "            'Accuracy': baseline_results[model_name]['val_accuracy']\n",
    "        })\n",
    "\n",
    "# Optimisés\n",
    "for model_name in ['RandomForest', 'GradientBoosting']:\n",
    "    if f\"{model_name}_optimized\" in baseline_results:\n",
    "        comparison_data.append({\n",
    "            'Model': model_name,\n",
    "            'Method': 'Grid Search',\n",
    "            'Accuracy': baseline_results[f\"{model_name}_optimized\"]['val_accuracy']\n",
    "        })\n",
    "\n",
    "# Random Search (Random Forest seulement)\n",
    "if best_rf_random:\n",
    "    rf_random_acc = accuracy_score(y_val, best_rf_random.predict(X_val_features_scaled))\n",
    "    comparison_data.append({\n",
    "        'Model': 'RandomForest',\n",
    "        'Method': 'Random Search',\n",
    "        'Accuracy': rf_random_acc\n",
    "    })\n",
    "\n",
    "if comparison_data:\n",
    "    df_optimization_comparison = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    # Visualisation\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=df_optimization_comparison, x='Model', y='Accuracy', hue='Method')\n",
    "    plt.title('Comparaison des Méthodes d\\'Optimisation')\n",
    "    plt.ylabel('Validation Accuracy')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"📊 Résultats de comparaison:\")\n",
    "    print(df_optimization_comparison)\n",
    "\n",
    "print(\"\\n✅ Optimisation des hyperparamètres terminée\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b28b1d",
   "metadata": {},
   "source": [
    "## 📊 Section 12: Model Evaluation and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ef334d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 12.1 MÉTRIQUES AVANCÉES ET MATRICES DE CONFUSION\n",
    "# ===================================\n",
    "\n",
    "def comprehensive_model_evaluation(y_true, y_pred, model_name, class_names):\n",
    "    \"\"\"\n",
    "    Évaluation complète d'un modèle avec métriques avancées\n",
    "    \"\"\"\n",
    "    print(f\"\\n📊 Évaluation complète - {model_name}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Métriques globales\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\"🎯 Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"🎯 Precision (weighted): {precision:.4f}\")\n",
    "    print(f\"🎯 Recall (weighted): {recall:.4f}\")\n",
    "    print(f\"🎯 F1-Score (weighted): {f1:.4f}\")\n",
    "    \n",
    "    # Métriques par classe\n",
    "    print(f\"\\n📋 Rapport de classification:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "    \n",
    "    # Matrice de confusion\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Visualisation de la matrice de confusion\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(f'Matrice de Confusion - {model_name}')\n",
    "    plt.xlabel('Prédictions')\n",
    "    plt.ylabel('Vraies étiquettes')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Métriques par classe détaillées\n",
    "    class_metrics = {}\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        class_precision = precision_recall_fscore_support(y_true, y_pred, labels=[i], average=None)[0][0]\n",
    "        class_recall = precision_recall_fscore_support(y_true, y_pred, labels=[i], average=None)[1][0]\n",
    "        class_f1 = precision_recall_fscore_support(y_true, y_pred, labels=[i], average=None)[2][0]\n",
    "        \n",
    "        class_metrics[class_name] = {\n",
    "            'precision': class_precision,\n",
    "            'recall': class_recall,\n",
    "            'f1_score': class_f1,\n",
    "            'support': support[i]\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'confusion_matrix': cm,\n",
    "        'class_metrics': class_metrics\n",
    "    }\n",
    "\n",
    "# Évaluation de tous les modèles disponibles\n",
    "print(\"📊 Évaluation complète de tous les modèles...\")\n",
    "\n",
    "evaluation_results = {}\n",
    "\n",
    "# Évaluation des modèles baseline\n",
    "for model_name, results in baseline_results.items():\n",
    "    if 'y_val_pred' in results:\n",
    "        eval_results = comprehensive_model_evaluation(\n",
    "            y_val, \n",
    "            results['y_val_pred'], \n",
    "            model_name, \n",
    "            CLASSES\n",
    "        )\n",
    "        evaluation_results[model_name] = eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c152e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 12.2 COMPARAISON GLOBALE DES MODÈLES\n",
    "# ===================================\n",
    "\n",
    "def create_models_comparison_dashboard():\n",
    "    \"\"\"\n",
    "    Crée un dashboard de comparaison des modèles\n",
    "    \"\"\"\n",
    "    if not evaluation_results:\n",
    "        print(\"⚠️ Aucun résultat d'évaluation disponible\")\n",
    "        return\n",
    "    \n",
    "    # Compilation des métriques\n",
    "    comparison_data = []\n",
    "    for model_name, results in evaluation_results.items():\n",
    "        comparison_data.append({\n",
    "            'Model': model_name,\n",
    "            'Accuracy': results['accuracy'],\n",
    "            'Precision': results['precision'],\n",
    "            'Recall': results['recall'],\n",
    "            'F1_Score': results['f1_score'],\n",
    "            'Type': get_model_type(model_name)\n",
    "        })\n",
    "    \n",
    "    df_comparison = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    # Tri par accuracy\n",
    "    df_comparison = df_comparison.sort_values('Accuracy', ascending=False)\n",
    "    \n",
    "    print(\"🏆 Classement des modèles par Accuracy:\")\n",
    "    print(df_comparison[['Model', 'Accuracy', 'F1_Score', 'Type']].to_string(index=False))\n",
    "    \n",
    "    # Visualisations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Dashboard de Comparaison des Modèles', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Accuracy par modèle\n",
    "    df_sorted = df_comparison.sort_values('Accuracy', ascending=True)\n",
    "    bars = axes[0, 0].barh(range(len(df_sorted)), df_sorted['Accuracy'], \n",
    "                          color=plt.cm.viridis(np.linspace(0, 1, len(df_sorted))))\n",
    "    axes[0, 0].set_yticks(range(len(df_sorted)))\n",
    "    axes[0, 0].set_yticklabels(df_sorted['Model'], fontsize=8)\n",
    "    axes[0, 0].set_xlabel('Accuracy')\n",
    "    axes[0, 0].set_title('Accuracy par Modèle')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Ajout des valeurs sur les barres\n",
    "    for i, bar in enumerate(bars):\n",
    "        width = bar.get_width()\n",
    "        axes[0, 0].text(width + 0.001, bar.get_y() + bar.get_height()/2, \n",
    "                       f'{width:.3f}', ha='left', va='center', fontsize=8)\n",
    "    \n",
    "    # 2. F1-Score vs Accuracy\n",
    "    scatter = axes[0, 1].scatter(df_comparison['Accuracy'], df_comparison['F1_Score'], \n",
    "                                c=df_comparison.index, cmap='viridis', s=100, alpha=0.7)\n",
    "    axes[0, 1].set_xlabel('Accuracy')\n",
    "    axes[0, 1].set_ylabel('F1-Score')\n",
    "    axes[0, 1].set_title('F1-Score vs Accuracy')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Ligne diagonale de référence\n",
    "    min_val = min(df_comparison['Accuracy'].min(), df_comparison['F1_Score'].min())\n",
    "    max_val = max(df_comparison['Accuracy'].max(), df_comparison['F1_Score'].max())\n",
    "    axes[0, 1].plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.5)\n",
    "    \n",
    "    # 3. Métriques par type de modèle\n",
    "    df_melted = df_comparison.melt(id_vars=['Model', 'Type'], \n",
    "                                  value_vars=['Accuracy', 'Precision', 'Recall', 'F1_Score'],\n",
    "                                  var_name='Metric', value_name='Score')\n",
    "    \n",
    "    sns.boxplot(data=df_melted, x='Type', y='Score', hue='Metric', ax=axes[1, 0])\n",
    "    axes[1, 0].set_title('Distribution des Métriques par Type')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    axes[1, 0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # 4. Heatmap des corrélations entre métriques\n",
    "    metrics_corr = df_comparison[['Accuracy', 'Precision', 'Recall', 'F1_Score']].corr()\n",
    "    sns.heatmap(metrics_corr, annot=True, cmap='coolwarm', center=0, \n",
    "                square=True, ax=axes[1, 1])\n",
    "    axes[1, 1].set_title('Corrélations entre Métriques')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return df_comparison\n",
    "\n",
    "# Création du dashboard\n",
    "print(\"📊 Création du dashboard de comparaison...\")\n",
    "comparison_df = create_models_comparison_dashboard()\n",
    "\n",
    "# Analyse des meilleures performances par catégorie\n",
    "if comparison_df is not None and not comparison_df.empty:\n",
    "    print(\"\\n🏆 Meilleurs modèles par catégorie:\")\n",
    "    \n",
    "    categories = ['Baseline', 'Bagging', 'Boosting']\n",
    "    for category in categories:\n",
    "        cat_models = comparison_df[comparison_df['Type'] == category]\n",
    "        if not cat_models.empty:\n",
    "            best_model = cat_models.loc[cat_models['Accuracy'].idxmax()]\n",
    "            print(f\"{category}: {best_model['Model']} (Accuracy: {best_model['Accuracy']:.4f})\")\n",
    "    \n",
    "    # Modèle global champion\n",
    "    best_overall = comparison_df.loc[comparison_df['Accuracy'].idxmax()]\n",
    "    print(f\"\\n👑 Champion global: {best_overall['Model']}\")\n",
    "    print(f\"   📊 Accuracy: {best_overall['Accuracy']:.4f}\")\n",
    "    print(f\"   📊 F1-Score: {best_overall['F1_Score']:.4f}\")\n",
    "    print(f\"   📊 Type: {best_overall['Type']}\")\n",
    "\n",
    "print(\"\\n✅ Évaluation comparative terminée\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19053bfd",
   "metadata": {},
   "source": [
    "## 🎨 Section 13: Prediction Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91181e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 13.1 VISUALISATION DES PRÉDICTIONS ET INTERPRÉTABILITÉ\n",
    "# ===================================\n",
    "\n",
    "def visualize_predictions_with_confidence(model, X_samples, y_true, sample_indices, \n",
    "                                        model_name, class_names, is_deep_learning=False):\n",
    "    \"\"\"\n",
    "    Visualise les prédictions avec scores de confiance\n",
    "    \"\"\"\n",
    "    n_samples = len(sample_indices)\n",
    "    fig, axes = plt.subplots(2, n_samples//2, figsize=(16, 8))\n",
    "    fig.suptitle(f'Prédictions et Confiance - {model_name}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    if is_deep_learning:\n",
    "        # Pour les modèles de deep learning\n",
    "        predictions_proba = model.predict(X_samples, verbose=0)\n",
    "        predictions = np.argmax(predictions_proba, axis=1)\n",
    "    else:\n",
    "        # Pour les modèles traditionnels avec features\n",
    "        X_features = extract_traditional_features(X_samples)\n",
    "        X_features_scaled = scaler.transform(X_features)\n",
    "        predictions_proba = model.predict_proba(X_features_scaled)\n",
    "        predictions = model.predict(X_features_scaled)\n",
    "    \n",
    "    for idx, sample_idx in enumerate(sample_indices):\n",
    "        row = idx // (n_samples//2)\n",
    "        col = idx % (n_samples//2)\n",
    "        \n",
    "        # Image\n",
    "        axes[row, col].imshow(X_samples[sample_idx])\n",
    "        \n",
    "        # Informations sur la prédiction\n",
    "        true_label = class_names[y_true[sample_idx]]\n",
    "        pred_label = class_names[predictions[sample_idx]]\n",
    "        confidence = np.max(predictions_proba[sample_idx])\n",
    "        \n",
    "        # Couleur selon la justesse de la prédiction\n",
    "        color = 'green' if true_label == pred_label else 'red'\n",
    "        \n",
    "        title = f'Vraie: {true_label}\\nPréd: {pred_label}\\nConf: {confidence:.3f}'\n",
    "        axes[row, col].set_title(title, color=color, fontsize=10)\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Graphique des scores de confiance par classe\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "    \n",
    "    for i, sample_idx in enumerate(sample_indices):\n",
    "        proba_scores = predictions_proba[sample_idx]\n",
    "        x_pos = np.arange(len(class_names)) + i * 0.1\n",
    "        \n",
    "        bars = ax.bar(x_pos, proba_scores, width=0.1, alpha=0.7, \n",
    "                     label=f'Échantillon {sample_idx+1}')\n",
    "        \n",
    "        # Highlight de la classe prédite\n",
    "        max_idx = np.argmax(proba_scores)\n",
    "        bars[max_idx].set_color('red')\n",
    "        bars[max_idx].set_alpha(1.0)\n",
    "    \n",
    "    ax.set_xlabel('Classes')\n",
    "    ax.set_ylabel('Score de Confiance')\n",
    "    ax.set_title(f'Scores de Confiance par Classe - {model_name}')\n",
    "    ax.set_xticks(np.arange(len(class_names)) + 0.2)\n",
    "    ax.set_xticklabels(class_names, rotation=45)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Démonstration avec le meilleur modèle traditionnel\n",
    "if baseline_results and X_val_processed is not None:\n",
    "    print(\"🎨 Visualisation des prédictions...\")\n",
    "    \n",
    "    # Sélection d'échantillons pour la visualisation\n",
    "    n_viz_samples = 6\n",
    "    sample_indices = np.random.choice(len(X_val_processed), n_viz_samples, replace=False)\n",
    "    \n",
    "    # Récupération du meilleur modèle traditionnel\n",
    "    best_traditional_model = None\n",
    "    best_traditional_name = \"\"\n",
    "    best_score = 0\n",
    "    \n",
    "    for model_name, results in baseline_results.items():\n",
    "        if 'val_accuracy' in results and results['val_accuracy'] > best_score:\n",
    "            best_score = results['val_accuracy']\n",
    "            best_traditional_model = results['model']\n",
    "            best_traditional_name = model_name\n",
    "    \n",
    "    if best_traditional_model:\n",
    "        print(f\"📊 Visualisation avec le meilleur modèle: {best_traditional_name}\")\n",
    "        \n",
    "        # Échantillons pour la visualisation\n",
    "        X_viz = X_val_processed[sample_indices]\n",
    "        y_viz = y_val[sample_indices]\n",
    "        \n",
    "        # Visualisation\n",
    "        visualize_predictions_with_confidence(\n",
    "            best_traditional_model,\n",
    "            X_viz,\n",
    "            y_viz,\n",
    "            range(len(sample_indices)),\n",
    "            best_traditional_name,\n",
    "            CLASSES,\n",
    "            is_deep_learning=False\n",
    "        )\n",
    "    \n",
    "    else:\n",
    "        print(\"⚠️ Aucun modèle traditionnel disponible pour la visualisation\")\n",
    "\n",
    "print(\"✅ Visualisations créées\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14da6041",
   "metadata": {},
   "source": [
    "## 🎯 Conclusion et Récapitulatif\n",
    "\n",
    "### 📊 **Résumé du Notebook**\n",
    "\n",
    "Ce notebook complet a démontré les techniques avancées de machine learning pour la classification d'images médicales COVID-19 :\n",
    "\n",
    "#### 🔧 **Techniques Implémentées :**\n",
    "\n",
    "1. **📦 Méthodes d'Ensemble - Bagging :**\n",
    "   - Random Forest optimisé\n",
    "   - Extra Trees (Extremely Randomized Trees)\n",
    "   - Bagging avec SVM et Logistic Regression\n",
    "   - Analyse d'importance des features\n",
    "\n",
    "2. **🚀 Méthodes d'Ensemble - Boosting :**\n",
    "   - AdaBoost avec arbres de décision\n",
    "   - Gradient Boosting Machine (GBM)\n",
    "   - XGBoost (si disponible)\n",
    "   - Comparaison des performances\n",
    "\n",
    "3. **🧠 Deep Learning :**\n",
    "   - CNN personnalisé from scratch\n",
    "   - Transfer Learning (VGG16, ResNet50, EfficientNetB0)\n",
    "   - Fine-Tuning avec stratégies avancées\n",
    "   - Ensemble de modèles de deep learning\n",
    "\n",
    "4. **⚙️ Optimisation :**\n",
    "   - Grid Search pour hyperparamètres\n",
    "   - Random Search alternatif\n",
    "   - Cross-validation stratifiée\n",
    "   - Learning rate scheduling\n",
    "\n",
    "5. **📊 Évaluation :**\n",
    "   - Métriques complètes (Accuracy, Precision, Recall, F1)\n",
    "   - Matrices de confusion détaillées\n",
    "   - Comparaisons visuelles\n",
    "   - Dashboard de performances\n",
    "\n",
    "#### 💡 **Points Clés Appris :**\n",
    "\n",
    "- **Bagging** : Réduit la variance, bon pour overfitting\n",
    "- **Boosting** : Réduit le biais, séquentiel et puissant\n",
    "- **Transfer Learning** : Efficace pour datasets médicaux limités\n",
    "- **Fine-Tuning** : Améliore les performances avec attention aux learning rates\n",
    "- **Ensemble Deep Learning** : Combine les forces de différents modèles\n",
    "\n",
    "#### 🛠️ **Recommandations pour la Production :**\n",
    "\n",
    "1. **Entraînement complet** : Utilisez plus d'époques (50-100) pour les modèles DL\n",
    "2. **Validation externe** : Testez sur un dataset externe pour validation\n",
    "3. **Monitoring** : Implémentez un suivi des performances en temps réel\n",
    "4. **Explicabilité** : Ajoutez des techniques comme LIME ou GRAD-CAM\n",
    "5. **Déploiement** : Considérez l'optimisation des modèles (quantization, pruning)\n",
    "\n",
    "### 🎯 **Prochaines Étapes Suggérées :**\n",
    "\n",
    "1. **Augmentation de données avancée** : Techniques spécifiques au médical\n",
    "2. **Architecture personnalisée** : CNN adapté aux radiographies\n",
    "3. **Méta-learning** : Apprentissage sur différents types d'images médicales\n",
    "4. **Federated Learning** : Entraînement distribué préservant la confidentialité\n",
    "5. **Validation clinique** : Collaboration avec professionnels de santé\n",
    "\n",
    "---\n",
    "\n",
    "**🏆 Félicitations ! Vous avez maintenant une base solide pour les techniques avancées de ML dans le domaine médical.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
