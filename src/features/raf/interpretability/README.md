# ğŸ” Module d'InterprÃ©tabilitÃ© - Framework RAF

Ce module fournit des outils d'interprÃ©tabilitÃ© pour les modÃ¨les de classification COVID-19, intÃ©grant **SHAP** et **GradCAM** dans le framework RAF.

## ğŸ“‹ FonctionnalitÃ©s

### ğŸ¯ GradCAM (Gradient-weighted Class Activation Mapping)
- Visualisation des zones importantes pour les prÃ©dictions CNN
- Support de GradCAM et GradCAM++
- Analyse quantitative des cartes d'activation
- Comparaison entre diffÃ©rentes couches

### ğŸ“Š SHAP (SHapley Additive exPlanations)
- Explications pour tous types de modÃ¨les (tree, linear, deep, kernel)
- Graphiques waterfall, summary et force plots
- Calcul d'importance des features
- Comparaison entre modÃ¨les

### âš–ï¸ Analyse Comparative
- Comparaison SHAP vs GradCAM
- Tableau de bord intÃ©grÃ©
- Rapports d'interprÃ©tabilitÃ©
- Analyse de cohÃ©rence

## ğŸš€ Installation

```bash
# Installation des dÃ©pendances
pip install -r requirements-interpretability.txt

# Ou installation manuelle des modules principaux
pip install shap tensorflow opencv-python scikit-learn matplotlib
```

## ğŸ“ Structure du Module

```
src/features/raf/interpretability/
â”œâ”€â”€ __init__.py                    # Exports principaux
â”œâ”€â”€ shap_explainer.py             # Module SHAP
â”œâ”€â”€ gradcam_explainer.py          # Module GradCAM
â””â”€â”€ utils.py                      # Utilitaires et analyseur intÃ©grÃ©
```

## ğŸ’» Utilisation Rapide

### Import du Module

```python
from src.features.raf.interpretability import (
    SHAPExplainer,
    GradCAMExplainer,
    InterpretabilityAnalyzer
)
```

### GradCAM pour CNN

```python
# Initialisation
gradcam_explainer = GradCAMExplainer(cnn_model)

# GÃ©nÃ©ration de la carte d'activation
results = gradcam_explainer.generate_gradcam(image)

# Visualisation
fig = gradcam_explainer.plot_gradcam(results, class_names=class_names)
```

### SHAP pour ModÃ¨les ML

```python
# Initialisation
shap_explainer = SHAPExplainer(ml_model, model_type='tree')

# Ajustement sur donnÃ©es d'arriÃ¨re-plan
shap_explainer.fit_explainer(X_background)

# Calcul des valeurs SHAP
shap_values = shap_explainer.explain(X_test)

# Visualisations
shap_explainer.plot_waterfall(0, feature_names=feature_names)
shap_explainer.plot_summary(feature_names=feature_names)
```

### Analyse Comparative

```python
# Analyseur intÃ©grÃ©
analyzer = InterpretabilityAnalyzer(
    cnn_model=cnn_model,
    ml_model=ml_model
)

# Analyse comparative
results = analyzer.compare_predictions(
    img=image,
    X_flat=image_flat,
    X_background=X_background,
    class_names=class_names
)

# Tableau de bord
dashboard = create_interpretability_dashboard(
    analyzer, image, image_flat, X_background, class_names
)
```

## ğŸ–¥ï¸ Interface Streamlit

Une interface web complÃ¨te est disponible :

```bash
streamlit run src/streamlit/pages/03_Interpretability.py
```

FonctionnalitÃ©s de l'interface :
- ğŸ–¼ï¸ **Analyse Simple** : Upload et analyse d'une image
- ğŸ“Š **Comparaison** : Comparaison entre modÃ¨les
- ğŸ“ˆ **Rapport Batch** : Analyse de plusieurs images
- ğŸ”§ **Utilitaires** : Configuration et tests

## ğŸ““ Notebook de DÃ©monstration

Un notebook complet est disponible pour apprendre et tester :

```
notebooks/Interpretability_SHAP_GradCAM_Demo.ipynb
```

Le notebook couvre :
- Configuration et imports
- DÃ©monstration GradCAM
- DÃ©monstration SHAP
- Analyse comparative
- Cas d'usage mÃ©dical
- GÃ©nÃ©ration de rapports

## ğŸ¥ Cas d'Usage MÃ©dical

### Diagnostic AssistÃ© par IA

```python
# Analyse d'une radiographie
gradcam_result = gradcam_explainer.generate_gradcam(xray_image)

predicted_class = gradcam_result['predicted_class']
confidence = gradcam_result['prediction_confidence']

# InterprÃ©tation mÃ©dicale
if predicted_class == COVID_CLASS:
    print("âš ï¸ ALERTE: Suspicion COVID-19 dÃ©tectÃ©e")
    print(f"Confiance: {confidence:.1%}")
    
# Zones d'attention pour le radiologue
features = extract_gradcam_features(gradcam_result['heatmap'])
regions = analyze_gradcam_regions(gradcam_result['heatmap'])
```

### Rapport MÃ©dical AutomatisÃ©

```python
# GÃ©nÃ©ration de rapport pour plusieurs patients
report_df = generate_interpretability_report(
    analyzer=analyzer,
    images=patient_images,
    X_background=reference_images,
    class_names=disease_classes,
    sample_names=patient_ids
)

# Export pour dossier mÃ©dical
report_df.to_csv('rapport_interpretabilite_patients.csv')
```

## ğŸ“Š API Principale

### Classes Principales

- **`SHAPExplainer`** : Explications SHAP pour tous modÃ¨les
- **`GradCAMExplainer`** : Cartes d'activation pour CNN
- **`InterpretabilityAnalyzer`** : Analyseur intÃ©grÃ© SHAP+GradCAM

### Fonctions Utilitaires

- **`explain_prediction()`** : Explication rapide d'une prÃ©diction
- **`generate_gradcam()`** : GÃ©nÃ©ration rapide de GradCAM
- **`create_interpretability_dashboard()`** : Tableau de bord complet
- **`generate_interpretability_report()`** : Rapport batch

### Fonctions de Visualisation

- **`visualize_shap_values()`** : Visualisations SHAP personnalisÃ©es
- **`visualize_gradcam_comparison()`** : Comparaison de mÃ©thodes GradCAM
- **`plot_interpretability_summary()`** : RÃ©sumÃ© visuel

## âš™ï¸ Configuration

### Types de ModÃ¨les SupportÃ©s

**SHAP :**
- `tree` : Random Forest, XGBoost, LightGBM
- `linear` : RÃ©gression linÃ©aire, SVM linÃ©aire
- `deep` : RÃ©seaux de neurones TensorFlow/Keras
- `kernel` : Tout modÃ¨le avec fonction predict

**GradCAM :**
- ModÃ¨les CNN TensorFlow/Keras
- Couches convolutionnelles 2D
- Classification multi-classe

### ParamÃ¨tres RecommandÃ©s

```python
# GradCAM
gradcam_params = {
    'alpha': 0.4,          # Transparence superposition
    'colormap': 'jet',     # Colormap heatmap
}

# SHAP
shap_params = {
    'max_evals': 100,      # Ã‰valuations pour kernel
    'model_type': 'auto',  # DÃ©tection automatique
}
```

## ğŸ”§ IntÃ©gration dans le Framework RAF

Le module s'intÃ¨gre naturellement dans RAF :

```python
# Import du framework complet
from src.features.raf import *

# Les modules d'interprÃ©tabilitÃ© sont automatiquement disponibles
analyzer = InterpretabilityAnalyzer(cnn_model, ml_model)
```

## ğŸ“š Ressources et RÃ©fÃ©rences

### Documentation
- [SHAP Documentation](https://shap.readthedocs.io/)
- [GradCAM Paper](https://arxiv.org/abs/1610.02391)
- [GradCAM++ Paper](https://arxiv.org/abs/1710.11063)

### Exemples d'Utilisation
- `notebooks/Interpretability_SHAP_GradCAM_Demo.ipynb`
- `src/streamlit/pages/03_Interpretability.py`

### Tests et Validation
- Tests unitaires avec donnÃ©es COVID-19
- Validation sur modÃ¨les rÃ©els
- Comparaison avec experts mÃ©dicaux

## ğŸš¨ Bonnes Pratiques MÃ©dicales

### âš ï¸ Avertissements
- **L'IA est un outil d'aide**, pas de remplacement du diagnostic mÃ©dical
- **Toujours valider** avec un expert mÃ©dical
- **DÃ©finir des seuils** de confiance appropriÃ©s
- **Documenter les dÃ©cisions** pour traÃ§abilitÃ©

### âœ… Recommandations
- Utiliser **SHAP ET GradCAM** pour plus de robustesse
- **Tester rÃ©guliÃ¨rement** sur de nouveaux cas
- **Former les utilisateurs** Ã  l'interprÃ©tation
- **Maintenir une supervision** mÃ©dicale

## ğŸ¤ Contribution

Pour contribuer au module d'interprÃ©tabilitÃ© :

1. **Fork** le repository
2. **CrÃ©er une branche** pour votre fonctionnalitÃ©
3. **Tester** avec le notebook de dÃ©monstration
4. **Documenter** vos ajouts
5. **Soumettre une pull request**

## ğŸ“„ Licence

Ce module fait partie du projet DS_COVID et suit la mÃªme licence.

---

**ğŸ” L'interprÃ©tabilitÃ© est essentielle pour l'IA mÃ©dicale de confiance !**