import streamlit as st
import sys
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import pickle
import joblib
from sklearn.metrics import (
    classification_report, confusion_matrix, roc_curve, auc,
    precision_recall_curve, average_precision_score
)
from sklearn.preprocessing import label_binarize
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Calcul correct du project_root
current_file = Path(__file__)  # pages/02_Model/2_Evaluation.py
current_dir = current_file.parent  # pages/02_Model/
streamlit_dir = current_dir.parent.parent  # streamlit/
src_dir = streamlit_dir.parent  # src/
project_root = src_dir.parent  # DS_COVID/

sys.path.append(str(project_root))

try:
    from src.features.Pipelines.Pipeline_Sklearn import PipelineManager
except ImportError:
    st.error("‚ùå Impossible d'importer PipelineManager. V√©rifiez le chemin d'acc√®s.")

st.title("üìä √âvaluation des Mod√®les")

st.markdown("""
Cette page permet d'√©valuer en d√©tail les mod√®les entra√Æn√©s et de comparer leurs performances
avec diff√©rentes m√©triques et visualisations.
""")

# Configuration sidebar
with st.sidebar:
    st.header("‚öôÔ∏è Configuration")
    
    # Mode d'√©valuation
    eval_mode = st.selectbox(
        "Mode d'√©valuation",
        ["Mod√®les en session", "Mod√®les sauvegard√©s", "Upload mod√®le"]
    )
    
    # Options d'affichage
    st.subheader("üìà Visualisations")
    show_confusion_matrix = st.checkbox("Matrice de confusion", True)
    show_roc_curves = st.checkbox("Courbes ROC", True)
    show_precision_recall = st.checkbox("Pr√©cision-Rappel", True)
    show_feature_importance = st.checkbox("Importance des features", False)

# Interface principale
tab1, tab2, tab3, tab4 = st.tabs(["üéØ S√©lection Mod√®les", "üìä M√©triques", "üìà Visualisations", "üìã Rapport"])

with tab1:
    st.header("S√©lection des Mod√®les √† √âvaluer")
    
    if eval_mode == "Mod√®les en session":
        if 'training_results' in st.session_state:
            results = st.session_state['training_results']
            
            st.success(f"‚úÖ {len(results)} mod√®le(s) disponible(s) en session")
            
            # S√©lection des mod√®les √† √©valuer
            model_names = list(results.keys())
            selected_models = st.multiselect(
                "Choisissez les mod√®les √† √©valuer:",
                options=model_names,
                default=model_names,
                help="S√©lectionnez un ou plusieurs mod√®les pour √©valuation d√©taill√©e"
            )
            
            if selected_models:
                # Afficher les informations de base
                st.subheader("üìã Informations des Mod√®les")
                
                for model_name in selected_models:
                    result = results[model_name]
                    
                    with st.expander(f"üîç {model_name}", expanded=False):
                        col1, col2, col3 = st.columns(3)
                        
                        with col1:
                            st.metric("Pr√©cision Test", f"{result.get('test_accuracy', 0):.4f}")
                            st.metric("F1-Score", f"{result.get('test_f1', 0):.4f}")
                        
                        with col2:
                            st.metric("Score CV", f"{result.get('cv_mean', 0):.4f}")
                            st.metric("√âcart-type CV", f"{result.get('cv_std', 0):.4f}")
                        
                        with col3:
                            training_time = result.get('training_time')
                            if training_time:
                                st.metric("Temps entra√Ænement", f"{training_time.total_seconds():.1f}s")
                        
                        # Param√®tres du mod√®le
                        if 'best_params' in result:
                            st.write("üîß **Meilleurs param√®tres:**")
                            st.json(result['best_params'])
        else:
            st.warning("""
            ‚ö†Ô∏è Aucun mod√®le en session.
            
            Veuillez d'abord entra√Æner des mod√®les dans la page **Entra√Ænement**.
            """)
            selected_models = []
    
    elif eval_mode == "Mod√®les sauvegard√©s":
        st.info("üöß Chargement de mod√®les sauvegard√©s non encore impl√©ment√©")
        selected_models = []
    
    else:  # Upload mod√®le
        st.info("üöß Upload de mod√®les externes non encore impl√©ment√©")
        selected_models = []

with tab2:
    st.header("M√©triques D√©taill√©es")
    
    if eval_mode == "Mod√®les en session" and 'training_results' in st.session_state and selected_models:
        results = st.session_state['training_results']
        
        # Tableau comparatif d√©taill√©
        st.subheader("üìä Comparaison D√©taill√©e")
        
        metrics_data = []
        for model_name in selected_models:
            result = results[model_name]
            
            # R√©cup√©rer les m√©triques d√©taill√©es si disponibles
            test_report = result.get('test_classification_report', {})
            
            row = {
                'Mod√®le': model_name,
                'Pr√©cision Test': result.get('test_accuracy', 0),
                'F1-Score Macro': result.get('test_f1', 0),
                'Score CV Moyen': result.get('cv_mean', 0),
                'Score CV Std': result.get('cv_std', 0),
            }
            
            # Ajouter les m√©triques par classe si disponibles
            if isinstance(test_report, dict) and 'macro avg' in test_report:
                row.update({
                    'Pr√©cision Macro': test_report['macro avg']['precision'],
                    'Rappel Macro': test_report['macro avg']['recall'],
                    'Support Total': test_report['macro avg']['support']
                })
            
            metrics_data.append(row)
        
        metrics_df = pd.DataFrame(metrics_data)
        st.dataframe(metrics_df, use_container_width=True)
        
        # M√©triques individuelles pour chaque mod√®le
        st.subheader("üìã Rapports de Classification")
        
        for model_name in selected_models:
            result = results[model_name]
            
            with st.expander(f"üìà Rapport d√©taill√© - {model_name}"):
                col1, col2 = st.columns([2, 1])
                
                with col1:
                    # Rapport de classification
                    test_report = result.get('test_classification_report')
                    if isinstance(test_report, dict):
                        # Convertir en DataFrame pour affichage
                        report_df = pd.DataFrame(test_report).transpose()
                        
                        # Nettoyer et formater
                        if 'support' in report_df.columns:
                            report_df['support'] = report_df['support'].fillna(0).astype(int)
                        
                        # Arrondir les valeurs num√©riques
                        numeric_cols = ['precision', 'recall', 'f1-score']
                        for col in numeric_cols:
                            if col in report_df.columns:
                                report_df[col] = report_df[col].round(4)
                        
                        st.dataframe(report_df, use_container_width=True)
                    
                    elif isinstance(test_report, str):
                        st.code(test_report)
                
                with col2:
                    # M√©triques principales en cards
                    st.metric("üéØ Pr√©cision", f"{result.get('test_accuracy', 0):.4f}")
                    st.metric("üìä F1-Score", f"{result.get('test_f1', 0):.4f}")
                    
                    if 'best_score' in result:
                        st.metric("üèÜ Best Grid Score", f"{result['best_score']:.4f}")
        
        # Graphique radar de comparaison
        if len(selected_models) > 1:
            st.subheader("üï∏Ô∏è Comparaison Radar")
            
            # Pr√©parer les donn√©es pour le graphique radar
            radar_metrics = ['test_accuracy', 'test_f1', 'cv_mean']
            radar_labels = ['Pr√©cision Test', 'F1-Score', 'Score CV']
            
            fig = go.Figure()
            
            for model_name in selected_models:
                result = results[model_name]
                values = [result.get(metric, 0) for metric in radar_metrics]
                values.append(values[0])  # Fermer le radar
                
                fig.add_trace(go.Scatterpolar(
                    r=values,
                    theta=radar_labels + [radar_labels[0]],
                    fill='toself',
                    name=model_name
                ))
            
            fig.update_layout(
                polar=dict(
                    radialaxis=dict(visible=True, range=[0, 1])
                ),
                showlegend=True,
                title="Comparaison des M√©triques Principales"
            )
            
            st.plotly_chart(fig, use_container_width=True)
    
    else:
        st.info("‚ÑπÔ∏è S√©lectionnez des mod√®les pour voir les m√©triques d√©taill√©es.")

with tab3:
    st.header("Visualisations Avanc√©es")
    
    if eval_mode == "Mod√®les en session" and 'training_results' in st.session_state and selected_models:
        results = st.session_state['training_results']
        
        # Matrices de confusion
        if show_confusion_matrix:
            st.subheader("üî• Matrices de Confusion")
            
            # Organiser en colonnes selon le nombre de mod√®les
            if len(selected_models) == 1:
                cols = [st.container()]
            elif len(selected_models) == 2:
                cols = st.columns(2)
            else:
                cols = st.columns(min(3, len(selected_models)))
            
            for i, model_name in enumerate(selected_models):
                result = results[model_name]
                
                with cols[i % len(cols)]:
                    st.write(f"**{model_name}**")
                    
                    if 'confusion_matrix' in result:
                        cm = result['confusion_matrix']
                        
                        # Cr√©er une heatmap interactive avec Plotly
                        fig = px.imshow(
                            cm,
                            labels=dict(x="Pr√©diction", y="R√©alit√©", color="Nombre"),
                            x=[f"Classe {i}" for i in range(cm.shape[1])],
                            y=[f"Classe {i}" for i in range(cm.shape[0])],
                            color_continuous_scale="Blues",
                            title=f"Matrice - {model_name}"
                        )
                        
                        # Ajouter les valeurs dans les cellules
                        for i in range(cm.shape[0]):
                            for j in range(cm.shape[1]):
                                fig.add_annotation(
                                    x=j, y=i,
                                    text=str(cm[i, j]),
                                    showarrow=False,
                                    font=dict(color="white" if cm[i, j] > cm.max()/2 else "black")
                                )
                        
                        st.plotly_chart(fig, use_container_width=True)
                    else:
                        st.info("Matrice de confusion non disponible")
        
        # Comparaison des temps d'entra√Ænement
        st.subheader("‚è±Ô∏è Temps d'Entra√Ænement")
        
        timing_data = []
        for model_name in selected_models:
            result = results[model_name]
            training_time = result.get('training_time')
            if training_time:
                timing_data.append({
                    'Mod√®le': model_name,
                    'Temps (secondes)': training_time.total_seconds()
                })
        
        if timing_data:
            timing_df = pd.DataFrame(timing_data)
            
            fig = px.bar(
                timing_df,
                x='Mod√®le',
                y='Temps (secondes)',
                title="Comparaison des Temps d'Entra√Ænement",
                color='Temps (secondes)',
                color_continuous_scale="viridis"
            )
            
            st.plotly_chart(fig, use_container_width=True)
        
        # Distribution des scores CV
        st.subheader("üìà Distribution des Scores de Validation Crois√©e")
        
        cv_data = []
        for model_name in selected_models:
            result = results[model_name]
            if 'cv_scores' in result:
                for score in result['cv_scores']:
                    cv_data.append({
                        'Mod√®le': model_name,
                        'Score CV': score
                    })
        
        if cv_data:
            cv_df = pd.DataFrame(cv_data)
            
            fig = px.box(
                cv_df,
                x='Mod√®le',
                y='Score CV',
                title="Distribution des Scores de Validation Crois√©e",
                points="all"
            )
            
            st.plotly_chart(fig, use_container_width=True)
    
    else:
        st.info("‚ÑπÔ∏è S√©lectionnez des mod√®les pour voir les visualisations.")

with tab4:
    st.header("Rapport d'√âvaluation Complet")
    
    if eval_mode == "Mod√®les en session" and 'training_results' in st.session_state and selected_models:
        results = st.session_state['training_results']
        comparison_data = st.session_state.get('comparison_data', {})
        
        # G√©n√©ration du rapport
        st.subheader("üìÑ Rapport Automatique")
        
        report_sections = []
        
        # Section 1: R√©sum√© ex√©cutif
        report_sections.append("## üéØ R√©sum√© Ex√©cutif")
        
        best_model = max(selected_models, key=lambda x: results[x].get('test_accuracy', 0))
        best_accuracy = results[best_model].get('test_accuracy', 0)
        
        report_sections.append(f"""
**√âvaluation de {len(selected_models)} mod√®le(s) de classification COVID-19**

- **Meilleur mod√®le**: {best_model}
- **Pr√©cision maximale**: {best_accuracy:.4f}
- **Date d'√©valuation**: {pd.Timestamp.now().strftime('%d/%m/%Y %H:%M')}
        """)
        
        # Section 2: Configuration exp√©rimentale
        if comparison_data:
            data_info = comparison_data.get('data_info', {})
            report_sections.append("## ‚öôÔ∏è Configuration Exp√©rimentale")
            report_sections.append(f"""
**Param√®tres des donn√©es:**
- Nombre d'√©chantillons: {data_info.get('n_samples', 'N/A')}
- Nombre de features: {data_info.get('n_features', 'N/A')}
- Nombre de classes: {data_info.get('n_classes', 'N/A')}
- Taille du jeu de test: {data_info.get('test_size', 'N/A')}
            """)
        
        # Section 3: R√©sultats par mod√®le
        report_sections.append("## üìä R√©sultats D√©taill√©s")
        
        for model_name in selected_models:
            result = results[model_name]
            
            report_sections.append(f"### {model_name}")
            
            metrics_text = f"""
- **Pr√©cision Test**: {result.get('test_accuracy', 0):.4f}
- **F1-Score**: {result.get('test_f1', 0):.4f}
- **Score CV Moyen**: {result.get('cv_mean', 0):.4f} ¬± {result.get('cv_std', 0):.4f}
"""
            
            if 'training_time' in result:
                metrics_text += f"- **Temps d'entra√Ænement**: {result['training_time'].total_seconds():.1f}s\n"
            
            if 'best_params' in result:
                metrics_text += f"- **Param√®tres optimaux**: {result['best_params']}\n"
            
            report_sections.append(metrics_text)
        
        # Section 4: Recommandations
        report_sections.append("## üí° Recommandations")
        
        # Analyser les r√©sultats pour donner des recommandations
        accuracies = [results[name].get('test_accuracy', 0) for name in selected_models]
        times = [results[name].get('training_time', pd.Timedelta(0)).total_seconds() 
                for name in selected_models]
        
        recommendations = []
        
        if max(accuracies) - min(accuracies) < 0.05:
            recommendations.append("- Les performances sont similaires entre mod√®les. Privil√©gier le plus rapide.")
        
        fastest_model = selected_models[times.index(min(times))]
        recommendations.append(f"- **Mod√®le le plus rapide**: {fastest_model}")
        
        if len(selected_models) > 1:
            recommendations.append("- Consid√©rer un ensemble (ensemble) des meilleurs mod√®les.")
        
        report_sections.extend(recommendations)
        
        # Afficher le rapport
        full_report = "\n\n".join(report_sections)
        st.markdown(full_report)
        
        # Bouton de t√©l√©chargement du rapport
        st.download_button(
            label="üì• T√©l√©charger le Rapport (Markdown)",
            data=full_report,
            file_name=f"rapport_evaluation_covid_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.md",
            mime="text/markdown"
        )
        
        # Export des donn√©es
        st.subheader("üíæ Export des Donn√©es")
        
        col1, col2 = st.columns(2)
        
        with col1:
            # CSV des m√©triques
            if st.button("üìä Exporter M√©triques (CSV)"):
                metrics_export = []
                for model_name in selected_models:
                    result = results[model_name]
                    metrics_export.append({
                        'model': model_name,
                        'test_accuracy': result.get('test_accuracy', 0),
                        'test_f1': result.get('test_f1', 0),
                        'cv_mean': result.get('cv_mean', 0),
                        'cv_std': result.get('cv_std', 0),
                        'training_time_s': result.get('training_time', pd.Timedelta(0)).total_seconds()
                    })
                
                export_df = pd.DataFrame(metrics_export)
                csv_data = export_df.to_csv(index=False)
                
                st.download_button(
                    label="‚¨áÔ∏è T√©l√©charger CSV",
                    data=csv_data,
                    file_name=f"metrics_covid_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.csv",
                    mime="text/csv"
                )
        
        with col2:
            # JSON des r√©sultats complets
            if st.button("üóÉÔ∏è Exporter R√©sultats (JSON)"):
                # Pr√©parer les donn√©es pour JSON (s√©rialisation)
                json_export = {}
                for model_name in selected_models:
                    result = results[model_name].copy()
                    
                    # Convertir les objets non-s√©rialisables
                    if 'training_time' in result:
                        result['training_time'] = result['training_time'].total_seconds()
                    
                    if 'confusion_matrix' in result:
                        result['confusion_matrix'] = result['confusion_matrix'].tolist()
                    
                    json_export[model_name] = result
                
                import json
                json_data = json.dumps(json_export, indent=2, default=str)
                
                st.download_button(
                    label="‚¨áÔ∏è T√©l√©charger JSON",
                    data=json_data,
                    file_name=f"results_covid_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.json",
                    mime="application/json"
                )
    
    else:
        st.info("""
        ‚ÑπÔ∏è **G√©n√©ration de rapport**
        
        S√©lectionnez des mod√®les √©valu√©s pour g√©n√©rer un rapport automatique complet.
        
        Le rapport inclura:
        - R√©sum√© ex√©cutif avec le meilleur mod√®le
        - Configuration exp√©rimentale
        - M√©triques d√©taill√©es par mod√®le
        - Recommandations personnalis√©es
        - Options d'export des donn√©es
        """)


